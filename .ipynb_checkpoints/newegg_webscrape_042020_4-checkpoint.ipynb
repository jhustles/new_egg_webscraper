{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping NewEgg.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "#import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "#from selenium import webdriver\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "# conn = 'mongodb://localhost:27017'\n",
    "# client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database\n",
    "# db = client.newegg_laptops_db\n",
    "# # Define collection\n",
    "# collection = db.apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Splinter and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Splinter inputs\n",
    "\n",
    "executable_path = {'executable_path': './chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up url that you want to scrape\n",
    "#url = 'https://www.newegg.com/p/pl?N=100006740&page=1&order=RELEASE'\n",
    "url = 'https://www.newegg.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splinter uses browser to go directly to the url\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Input\n",
    "\n",
    "#query_input = input(\"What products would you like to scrape?\")\n",
    "query_input = \"Laptops\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the search bar, and type in \"laptops\"\n",
    "search_box = browser.find_by_id(\"SearchBox2020\").type(query_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the Search button!\n",
    "\n",
    "submit = browser.find_by_css(\".fa-search\").first.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on the first element which is laptops / notebooks on the side menu far target by class\n",
    "# Optional - can expand this out later for other options\n",
    "\n",
    "browser.find_by_css(\".filter-box-label\").first.click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on \"Sold By: Newegg\" - later add options for each type (later code by xpath for each option)\n",
    "\n",
    "browser.find_by_css(\".form-radiobox-title\").first.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newegg_page_scraper(containers, turn_page):\n",
    "    \n",
    "    images = []\n",
    "    product_brands = []\n",
    "    product_models = []\n",
    "    product_links = []\n",
    "    item_numbers = []\n",
    "    promotions = []\n",
    "    prices = []\n",
    "    shipping = []\n",
    "    \n",
    "    \n",
    "    for con in containers:\n",
    "        page_counter = turn_page\n",
    "        \n",
    "        \n",
    "        image = con.a.img[\"src\"]\n",
    "        images.append(image)\n",
    "        \n",
    "        brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "        product_brands.append(brand_name)\n",
    "        #print(brand_name)\n",
    "\n",
    "        prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "        product_models.append(prd_title)\n",
    "        \n",
    "        product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "        product_links.append(product_link)\n",
    "        \n",
    "        item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('?')[1]\n",
    "        item_numbers.append(item_num)\n",
    "        \n",
    "        current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "        promotions.append(current_promo)\n",
    "\n",
    "        price = con.find_all('li', class_=\"price-current\")[0].text.split()[0]\n",
    "        prices.append(price)\n",
    "\n",
    "        shipping = con.find_all('li', class_='price-ship')[0].text.strip()\n",
    "        shipping.append(shipping)     \n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "    'item_number': item_numbers,\n",
    "    'brand': product_brands,\n",
    "    'model_listing': product_models,\n",
    "    'price': laptop_prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': laptop_shipping,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "\n",
    "    return df.to_csv(f'./processing/{current_date}_{query_input}_{page_counter}_scraped.csv')\n",
    "    \n",
    "#newegg_page_scraper(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(total_results_pages):\n",
    "    \n",
    "    path = f'./output\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    concatenate_pages = []\n",
    "\n",
    "    for page in scraped_pages:\n",
    "        df = pd.read_csv(page, index_col=0, header=0)\n",
    "        concatenate_pages.append(df)\n",
    "\n",
    "    compiled_data = pd.concat(concatenate_pages, axis=0, ignore_index=True)\n",
    "    compiled_data.to_csv(f\"./finished_outputs/{current_date}_{query_input}_scraped_{total_results_pages}_pages_.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Egg WebScraper Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract total page numbers from results\n",
    "\n",
    "# Setup current date - for later use\n",
    "current_date = str(datetime.datetime.now()).replace(':','.').replace(' ','_')[:-7]\n",
    "\n",
    "# Use Splinter to grab the current url, to setup request to pull URL\n",
    "current_url = browser.url\n",
    "\n",
    "# Use Request.get() to pull the current url\n",
    "response = requests.get(current_url)\n",
    "#response\n",
    "\n",
    "# Use BeautifulSoup to grab all the HTML using the lxml parser\n",
    "current_page_soup = soup(response.text, 'html.parser')\n",
    "\n",
    "# Use BeautifulSoup to extract the total results page number\n",
    "results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "# Find and extract total pages + and add 1 to ensure proper length of total pages\n",
    "total_results_pages = int(re.split(\"/\", results_pages)[1]) + 2 # need to add 2 b/c 'range(inclusive, exclusive)'\n",
    "\n",
    "# This is \"NEXT PAGE BUTTON CLICK\" - This loops thru the total amount of pages by clicking the next page button\n",
    "\n",
    "for turn_page in range(1, total_results_pages):\n",
    "    \n",
    "    # set the current url as the target page (aiming the boomerang)\n",
    "    target_url = browser.url\n",
    "    \n",
    "    # Use Request.get() - (throw the boomerang at the target, retrieve the info, & return\n",
    "    response_target = requests.get(target_url)\n",
    "    #response\n",
    "    \n",
    "    # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "    target_page_soup = soup(response_target.text, 'html.parser')\n",
    "    \n",
    "\n",
    "\n",
    "    containers = target_page_soup.find_all(\"div\", class_=\"item-container\") \n",
    "    newegg_page_scraper(containers, turn_page)\n",
    "    time.sleep(4)\n",
    "    browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "    \n",
    "browser.quit()\n",
    "concatenate(total_results_pages)\n",
    "\n",
    "print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{query_input}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "print('Thank you and hope you found this useful!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Scraper And Exports Out to CSV. Later can figure out how to get it out to PyMongo / Mongo Db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct webdriver this is the html page to scrape\n",
    "# splinter .html will set the 'source of the page content' \n",
    "\n",
    "current_url = browser.url\n",
    "# Use Request.get() to pull the current url\n",
    "response = requests.get(current_url)\n",
    "# Use beautiful soup to parse the text using html.parser\n",
    "page_soup = soup(response.text, 'html.parser')\n",
    "\n",
    "# Capture all containers on the page to set up for looping thru later\n",
    "containers = page_soup.find_all(\"div\", class_=\"item-container\") \n",
    "\n",
    "con = containers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"item-img\" href=\"https://www.newegg.com/p/N82E16834233304?Item=N82E16834233304\">\n",
       "<div class=\"item-badges\">\n",
       "</div>\n",
       "<img alt='GIGABYTE AERO 15 Classic-SA-U74ADP Gaming Laptop - 15.6\" 4K/UHD IPS - Intel Core i7-9750H 2.60 GHz - NVIDIA GeForce GTX 1660 Ti - 16 GB Memory 512 GB SSD - Windows 10 Pro 64-bit' src=\"//c1.neweggimages.com/NeweggImage/ProductImageCompressAll300/34-233-304-S01.jpg\" title='GIGABYTE AERO 15 Classic-SA-U74ADP Gaming Laptop - 15.6\" 4K/UHD IPS - Intel Core i7-9750H 2.60 GHz - NVIDIA GeForce GTX 1660 Ti - 16 GB Memory 512 GB SSD - Windows 10 Pro 64-bit'/>\n",
       "</a>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.contents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Item=N82E16834233304'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.a.img[\"src\"]\n",
    "con.find_all('a', class_=\"item-title\")[0]['href'].split('?')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the loop to extract: brand, Title of the product, price-dollar, price-cents,\n",
    "# shipping\n",
    "\n",
    "\n",
    "#def newegg_page_scraper(containers, turn_page):\n",
    "def newegg_page_scraper(containers):\n",
    "    item_numbers = []\n",
    "    laptop_brands = []\n",
    "    laptop_models = []\n",
    "    product_links = []\n",
    "    laptop_prices = []\n",
    "    laptop_shipping = []\n",
    "    laptop_images = []\n",
    "    promotions = []\n",
    "    \n",
    "    for con in containers:\n",
    "\n",
    "        page_counter = turn_page\n",
    "        \n",
    "        item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('?')[1]\n",
    "        item_numbers.append(item_num)\n",
    "        \n",
    "        brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "        laptop_brands.append(brand_name)\n",
    "        #print(brand_name)\n",
    "\n",
    "        prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "        laptop_models.append(prd_title)\n",
    "        \n",
    "        product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "        product_links.append(product_link)\n",
    "        \n",
    "        price = con.find_all('li', class_=\"price-current\")[0].text.split()[0]\n",
    "        laptop_prices.append(price)\n",
    "\n",
    "        shipping = con.find_all('li', class_='price-ship')[0].text.strip()\n",
    "        laptop_shipping.append(shipping)\n",
    "        \n",
    "        image = con.a.img[\"src\"]\n",
    "        laptop_image.append(image)\n",
    "        \n",
    "        # add this and the list up top\n",
    "        current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "        promotions.append(current_promo)\n",
    "        \n",
    "        #if statements for the price-was and for sale ending - optional\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "    'brand': laptop_brands,\n",
    "    'model_listing': laptop_models,\n",
    "    'price': laptop_prices,\n",
    "    'shipping': laptop_shipping\n",
    "    })\n",
    "\n",
    "    return df.to_csv(f'./output/newgg_laptops_{page_counter}.csv')\n",
    "    \n",
    "newegg_page_scraper(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
