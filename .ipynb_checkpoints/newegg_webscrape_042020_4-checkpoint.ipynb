{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping NewEgg.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "# conn = 'mongodb://localhost:27017'\n",
    "# client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database\n",
    "# db = client.newegg_laptops_db\n",
    "# # Define collection\n",
    "# collection = db.apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Splinter inputs\n",
    "\n",
    "executable_path = {'executable_path': './chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up url that you want to scrape\n",
    "#url = 'https://www.newegg.com/p/pl?N=100006740&page=1&order=RELEASE'\n",
    "url = 'https://www.newegg.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splinter uses browser to go directly to the url\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Input\n",
    "\n",
    "#query_input = input(\"What products would you like to scrape?\")\n",
    "query_input = \"Laptops\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the search bar, and type in \"laptops\"\n",
    "search_box = browser.find_by_id(\"SearchBox2020\").type(query_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the Search button!\n",
    "\n",
    "submit = browser.find_by_css(\".fa-search\").first.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on the first element which is laptops / notebooks on the side menu far target by class\n",
    "# Optional - can expand this out later for other options\n",
    "\n",
    "browser.find_by_css(\".filter-box-label\").first.click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on \"Sold By: Newegg\" - later add options for each type (later code by xpath for each option)\n",
    "\n",
    "browser.find_by_css(\".form-radiobox-title\").first.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newegg_page_scraper(containers, turn_page):\n",
    "\n",
    "    laptop_brands = []\n",
    "    laptop_models = []\n",
    "    laptop_prices = []\n",
    "    laptop_shipping = []\n",
    "    \n",
    "    \n",
    "    for con in containers:\n",
    "        #\n",
    "        #page_counter = re.split(\"/\", results_pages)[0]\n",
    "        page_counter = turn_page\n",
    "        \n",
    "        brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "        laptop_brands.append(brand_name)\n",
    "        #print(brand_name)\n",
    "\n",
    "        prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "        laptop_models.append(prd_title)\n",
    "\n",
    "        price = con.find_all('li', class_=\"price-current\")[0].text.split()[0]\n",
    "        laptop_prices.append(price)\n",
    "\n",
    "        shipping = con.find_all('li', class_='price-ship')[0].text.strip()\n",
    "        laptop_shipping.append(shipping)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "    'brand': laptop_brands,\n",
    "    'model_listing': laptop_models,\n",
    "    'price': laptop_prices,\n",
    "    'shipping': laptop_shipping\n",
    "    })\n",
    "\n",
    "    #df.to_csv(f'./output/newgg_laptop_{page_counter}.csv')\n",
    "    \n",
    "    #return f\"Completed scraping laptop section page number: {page_counter} . \"\n",
    "    return df.to_csv(f'./output/newgg_laptops_{page_counter}.csv')\n",
    "    \n",
    "#newegg_page_scraper(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract total page numbers from results\n",
    "\n",
    "# Use Splinter to grab the current url, to setup request to pull URL\n",
    "current_url = browser.url\n",
    "\n",
    "# Use Request.get() to pull the current url\n",
    "response = requests.get(current_url)\n",
    "#response\n",
    "\n",
    "# Use BeautifulSoup to grab all the HTML using the lxml parser\n",
    "current_page_soup = soup(response.text, 'html.parser')\n",
    "\n",
    "# Use BeautifulSoup to extract the total results page number\n",
    "results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "# Find and extract total pages + and add 1 to ensure proper length of total pages\n",
    "total_results_pages = int(re.split(\"/\", results_pages)[1]) + 2 # need to add 2 b/c 'range(inclusive, exclusive)'\n",
    "\n",
    "# This is \"NEXT PAGE BUTTON CLICK\" - This loops thru the total amount of pages by clicking the next page button\n",
    "\n",
    "for turn_page in range(1, total_results_pages):\n",
    "    \n",
    "    target_url = browser.url\n",
    "    # Use Request.get() to pull the current url\n",
    "    response_target = requests.get(target_url)\n",
    "    #response\n",
    "    # Use BeautifulSoup to grab all the HTML using the lxml parser\n",
    "    target_page_soup = soup(response_target.text, 'html.parser')\n",
    "    \n",
    "    #curr_pg_of_total_pgs = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "    #print(curr_pg_of_total_pgs)\n",
    "    \n",
    "    #current_page = re.split(\"/\", curr_pg_of_total_pgs)[0]\n",
    "    #print(current_page)\n",
    "\n",
    "    containers = target_page_soup.find_all(\"div\", class_=\"item-container\") \n",
    "    newegg_page_scraper(containers, turn_page)\n",
    "    time.sleep(4)\n",
    "    browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "    \n",
    "    \n",
    "#browser.quit()\n",
    "\n",
    "print('WebScraping Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#browser.find_by_text(\"2\").click()\n",
    "#browser.find_by_css(\".list-tool-pagination-text\").first.has_class('list-tool-pagination-text')#.first.click()\n",
    "#browser.find_by_css(\".btn-group-cell\").has_class('btn-group-cell')\n",
    "\n",
    "#browser.find_by_css(\".btn-group-cell\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Scraper And Exports Out to CSV. Later can figure out how to get it out to PyMongo / Mongo Db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct webdriver this is the html page to scrape\n",
    "# splinter .html will set the 'source of the page content' \n",
    "html = browser.html\n",
    "# Use beautiful soup to parse the text using html.parser\n",
    "page_soup = soup(html, 'html.parser')\n",
    "\n",
    "# Capture all containers on the page to set up for looping thru later\n",
    "containers = page_soup.find_all(\"div\", class_=\"item-container\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the loop to extract: brand, Title of the product, price-dollar, price-cents,\n",
    "# shipping\n",
    "\n",
    "\n",
    "def newegg_page_scraper(containers):\n",
    "\n",
    "    laptop_brands = []\n",
    "    laptop_models = []\n",
    "    laptop_prices = []\n",
    "    laptop_shipping = []\n",
    "\n",
    "    for con in containers:\n",
    "        page_counter = 0\n",
    "        page_counter += 1\n",
    "        brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "        laptop_brands.append(brand_name)\n",
    "        #print(brand_name)\n",
    "\n",
    "        prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "        laptop_models.append(prd_title)\n",
    "\n",
    "        price = con.find_all('li', class_=\"price-current\")[0].text.split()[0]\n",
    "        laptop_prices.append(price)\n",
    "\n",
    "        shipping = contain.find_all('li', class_='price-ship')[0].text.strip()\n",
    "        laptop_shipping.append(shipping)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "    'brand': laptop_brands,\n",
    "    'model_listing': laptop_models,\n",
    "    'price': laptop_prices,\n",
    "    'shipping': laptop_shipping\n",
    "    })\n",
    "\n",
    "    df.to_csv(f'./output/newgg_laptop_page_{page_counter}.csv')\n",
    "    \n",
    "    return f\"Completed scraping laptop section page number: {page_counter} . \"\n",
    "    \n",
    "newegg_page_scraper(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
