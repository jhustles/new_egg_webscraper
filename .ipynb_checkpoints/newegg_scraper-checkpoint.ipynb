{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NewEgg.Com WebScraping Program For Laptops - Beta v1.0\n",
    "\n",
    "###  - April 2020\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from re import search\n",
    "from splinter import Browser\n",
    "from playsound import playsound\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder to self.\n",
    "#import this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions & Classes Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return date throughout the program.\n",
    "\n",
    "def return_dt():\n",
    "    \n",
    "    global current_date\n",
    "    \n",
    "    current_date = str(datetime.datetime.now()).replace(':','.').replace(' ','_')[:-7]\n",
    "    \n",
    "    return current_date\n",
    "\n",
    "#return_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main NewEgg WebScraper function - outputs are csv file and Laptop objects.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def newegg_page_scraper(containers, turn_page):\n",
    "    \n",
    "    page_nums = []\n",
    "    \n",
    "    general_category = []\n",
    "    \n",
    "    product_categories = []\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    product_brands = []\n",
    "    \n",
    "    product_models = []\n",
    "    \n",
    "    product_links = []\n",
    "    \n",
    "    item_numbers = []\n",
    "    \n",
    "    promotions = []\n",
    "    \n",
    "    prices = []\n",
    "    \n",
    "    shipping_terms = []\n",
    "    \n",
    "    # Set gen_category as a global variable to make it accessible throughout the program, and to avoid an error.\n",
    "    \n",
    "    global gen_category\n",
    "    \n",
    "    \"\"\" \n",
    "    Loop through all the containers on the HTML, and scrap the following content into the following lists\n",
    "    \n",
    "    \"\"\"\n",
    "    for con in containers:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            page_counter = turn_page\n",
    "            page_nums.append(int(turn_page))\n",
    "            \n",
    "            gen_category = target_page_soup.find_all('div', class_=\"nav-x-body-top-bar fix\")[0].text.split('\\n')[5]\n",
    "            general_category.append(gen_category)\n",
    "            \n",
    "            prod_category = target_page_soup.find_all('h1', class_=\"page-title-text\")[0].text\n",
    "            product_categories.append(prod_category)\n",
    "            \n",
    "            image = con.a.img[\"src\"]\n",
    "            images.append(image)\n",
    "\n",
    "            prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "            product_models.append(prd_title)\n",
    "\n",
    "            product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "            product_links.append(product_link)\n",
    "            \n",
    "            shipping = con.find_all('li', class_='price-ship')[0].text.strip().split()[0]\n",
    "            \n",
    "            if shipping != \"Free\":\n",
    "                shipping = shipping.replace('$', '')\n",
    "                shipping_terms.append(shipping)\n",
    "                \n",
    "            else:\n",
    "                shipping = 0.00\n",
    "                shipping_terms.append(shipping)\n",
    "\n",
    "            brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "            product_brands.append(brand_name)\n",
    "\n",
    "        except (IndexError, ValueError) as e:\n",
    "            \n",
    "            # If there are no item_brand container, take the Brand from product details.\n",
    "            product_brands.append(con.find_all('a', class_=\"item-title\")[0].text.split()[0])\n",
    "\n",
    "        try:\n",
    "            current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "            promotions.append(current_promo)\n",
    "            \n",
    "        except:\n",
    "            promotions.append('null')\n",
    "\n",
    "        try:\n",
    "            price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "            prices.append(price)\n",
    "            \n",
    "        except:\n",
    "            price = 'null / out of stock'\n",
    "            prices.append(price)\n",
    "\n",
    "        try:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "            item_numbers.append(item_num)\n",
    "            \n",
    "        except (IndexError) as e:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]\n",
    "            item_numbers.append(item_num)    \n",
    "    \n",
    "    # Convert all of the lists into a dataframe\n",
    "    df = pd.DataFrame({\n",
    "    'item_number': item_numbers,\n",
    "    'general_category': general_category,\n",
    "    'product_category': product_categories,\n",
    "    'brand': product_brands,\n",
    "    'model_specifications': product_models,\n",
    "    'price': prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': shipping_terms,\n",
    "    'page_number': page_nums,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "    \n",
    "    # Rearrange the dataframe columns into the following order.\n",
    "    df = df[['item_number', 'general_category','product_category', 'page_number' ,'brand','model_specifications' ,'current_promotions' ,'price' ,'shipping' ,'product_links','image_link']]\n",
    "    \n",
    "    # Convert the dataframe into a dictionary.\n",
    "    global scraped_dict\n",
    "    scraped_dict = df.to_dict('records')\n",
    "    \n",
    "    # Grab the subcategory \"Laptop/Notebooks\" and eliminate any special characters that may cause errors.\n",
    "    global pdt_category\n",
    "    pdt_category = df['product_category'].unique()[0]\n",
    "    \n",
    "    # Eliminate special characters in a string if it exists.\n",
    "    pdt_category = ''.join(e for e in pdt_category if e.isalnum())\n",
    "    \n",
    "    \"\"\" Count the number of items scraped by getting the length of a all the models for sale.\n",
    "        This parameter is always available for each item-container in the HTML\n",
    "    \"\"\"\n",
    "\n",
    "    global items_scraped\n",
    "    items_scraped = len(df['model_specifications'])\n",
    "\n",
    "    \"\"\"\n",
    "    Save the results into a csv file using Pandas\n",
    "    \"\"\"\n",
    "    df.to_csv(f'./processing/{current_date}_{pdt_category}_{items_scraped}_scraped_page{turn_page}.csv')\n",
    "    \n",
    "    # Return these variables as they will be used.\n",
    "    return scraped_dict, items_scraped, pdt_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the total results pages.\n",
    "\n",
    "def results_pages(target_page_soup):\n",
    "    \n",
    "    # Use BeautifulSoup to extract the total results page number.\n",
    "    results_pages = target_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "    #print(results_pages)\n",
    "    \n",
    "    # Find and extract total pages + and add 1 to ensure proper length of total pages.\n",
    "    global total_results_pages\n",
    "    total_results_pages = int(re.split(\"/\", results_pages)[1])\n",
    "    \n",
    "    return total_results_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build a function to concatenate all pages that were scraped and saved in the processing folder.\n",
    "Save the final output (1 csv file) all the results\n",
    "    \n",
    "\"\"\"\n",
    "def concatenate(total_results_pages):\n",
    "    \n",
    "    path = f'./processing\\\\'\n",
    "    \n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    concatenate_pages = []\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for page in scraped_pages:\n",
    "        \n",
    "        df = pd.read_csv(page, index_col=0, header=0)\n",
    "        \n",
    "        concatenate_pages.append(df)\n",
    "\n",
    "    compiled_data = pd.concat(concatenate_pages, axis=0, ignore_index=True)\n",
    "    \n",
    "    total_items_scraped = len(compiled_data['brand'])\n",
    "    \n",
    "    concatenated_output = compiled_data.to_csv(f\"./finished_outputs/{current_date}_{total_items_scraped}_scraped_{total_results_pages}_pages_.csv\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Built a function to clear out the entire processing files folder to avoid clutter.\n",
    "Or the user can keep the processing files (page by page) for their own analysis.\n",
    "\n",
    "\"\"\"\n",
    "def clean_processing_fldr():\n",
    "    \n",
    "    path = f'./processing\\\\'\n",
    "    \n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    if len(scraped_pages) < 1:\n",
    "        \n",
    "        print(\"There are no files in the folder to clear. \\n\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(f\"Clearing out a total of {len(scraped_pages)} scraped pages in the processing folder... \\n\")\n",
    "        \n",
    "        clear_processing_files = []\n",
    "        \n",
    "        for page in scraped_pages:\n",
    "            \n",
    "            os.remove(page)\n",
    "        \n",
    "    print('Clearing of \"Processing\" folder complete. \\n')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse over function to go thru hover over product links on the page to emulate humans.\n",
    "\n",
    "def random_a_tag_mouse_over3():\n",
    "    \n",
    "    x = random.randint(6, 10)\n",
    "    \n",
    "    def rdm_slp_6_10(x):\n",
    "\n",
    "        time.sleep(x)\n",
    "\n",
    "        print(f\"Mimic Humans - Sleeping for {x} seconds. \")\n",
    "\n",
    "        return x    \n",
    "\n",
    "    working_try_atags = []\n",
    "    \n",
    "    finally_atags = []\n",
    "\n",
    "    working_atags = []\n",
    "\n",
    "    not_working_atags = []\n",
    "\n",
    "    try_counter = 0\n",
    "\n",
    "    finally_counter = 0\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Mouse over to header of the page \"Laptops\"\n",
    "    \n",
    "    browser.find_by_tag(\"h1\").mouse_over()\n",
    "    \n",
    "    number_of_a_tags = len(browser.find_by_tag(\"a\"))\n",
    "    \n",
    "    # My observation has taught me that most of the actual laptop clickable links on the grid are in the <a> range 2000 to 2100.\n",
    "    if number_of_a_tags > 1900:\n",
    "        \n",
    "        print(f\"Found {number_of_a_tags} <a> tags when parsing html... \")\n",
    "        \n",
    "        random_90_percent_plug = (random.randint(90, 94)/100.00)\n",
    "        \n",
    "        start_a_tag = int(round((number_of_a_tags * random_90_percent_plug)))\n",
    "        \n",
    "        end_a_tag = int(round((number_of_a_tags * .96)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # After proving you're human, clickable <a>'s reduced 300, so adjusting mouse_over for that scenario.\n",
    "\n",
    "        print(f\"Found {number_of_a_tags} <a> tags when parsing html... \")\n",
    "        \n",
    "        random_40_percent_plug = (random.randint(40, 44)/100.00)\n",
    "        \n",
    "        start_a_tag = int(round((number_of_a_tags * random_40_percent_plug)))\n",
    "        \n",
    "        end_a_tag = int(round((number_of_a_tags * .46)))\n",
    "\n",
    "    step = random.randint(13, 23)\n",
    "\n",
    "    for i in range(start_a_tag, end_a_tag, step):\n",
    "\n",
    "        try: # try this as normal part of the program - SHORT\n",
    "\n",
    "            rdm_slp_6_10(x)\n",
    "\n",
    "            browser.find_by_tag(\"a\")[i+2].mouse_over()\n",
    "\n",
    "            time.sleep(3)\n",
    "\n",
    "        except: # Execute this when there is an exception\n",
    "\n",
    "            print(\"EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \\n\")\n",
    "\n",
    "            break\n",
    "\n",
    "        else: # execute this only if no exceptions are raised\n",
    "\n",
    "            working_try_atags.append(i+2)\n",
    "\n",
    "            working_atags.append(i+2)\n",
    "\n",
    "            try_counter += 1\n",
    "\n",
    "            print(f\"<a> number = {i+2} | Current Attempts (Try Count): {try_counter} \\n\")\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for Google's reCaptcha \"are you human?\" test and alerts the user.\n",
    "\n",
    "def g_recaptcha_check():\n",
    "    \n",
    "    if browser.is_element_present_by_id('g-recaptcha') == True:\n",
    "        \n",
    "        for sound in range(0, 2):\n",
    "            \n",
    "            playsound('./sounds/user_alert.wav')\n",
    "        \n",
    "        print(\"recaptcha - Check Alert! \\n\")\n",
    "        \n",
    "        continue_scrape = input(\"Newegg system suspects you are a bot. \\n Complete the recaptcha test to prove you're not a bot. After, enter in any key and press ENTER to continue the scrape. \\n\")\n",
    "        \n",
    "        print(\"Continuing with scrape... \\n\")\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created an are you human backend test bc Newegg would send bogus \"are you human\" html when \"requesting\" for html.\n",
    "\n",
    "def are_you_human_backend(target_page_soup):\n",
    "    \n",
    "    if target_page_soup.find_all(\"title\")[0].text == 'Are you a human?':\n",
    "        \n",
    "        playsound('./sounds/user_alert.wav')\n",
    "\n",
    "        print(\"Newegg suspects you're a bot on the backend. Automatically will refresh the page 3 times, and target new URL. \\n\")\n",
    "        \n",
    "        print(\"Refreshing three times in 12 seconds. Please wait... \\n\")\n",
    "        \n",
    "        for i in range(0, 2):\n",
    "            \n",
    "            browser.reload()\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        browser.back()\n",
    "        \n",
    "        time.sleep(4)\n",
    "        \n",
    "        browser.forward()\n",
    "        \n",
    "        time.sleep(3)\n",
    "            \n",
    "        print(\"Targeting new url... \")\n",
    "            \n",
    "        # After user passes test, target the new url, and return updated target_page_soup.\n",
    "        \n",
    "        target_url = browser.url\n",
    "\n",
    "        response_target = requests.get(target_url)\n",
    "\n",
    "        target_page_soup = soup(response_target.text, 'html.parser')\n",
    "        \n",
    "        # Print so the user can see if we can proceed \n",
    "        \n",
    "        print(\"#\"* 60)\n",
    "        \n",
    "        print(target_page_soup)\n",
    "        \n",
    "        print(\"#\"* 60)\n",
    "        \n",
    "        break_pedal = input(\"Break pedal - enter anything to continue... \")\n",
    "        \n",
    "        # Recursion to run the test again.\n",
    "        are_you_human_backend(target_page_soup)\n",
    "        \n",
    "        if break_pedal == 'y':\n",
    "\n",
    "            # Recursion.\n",
    "            are_you_human_backend(target_page_soup)\n",
    "\n",
    "        else:\n",
    "\n",
    "            target_url = browser.url\n",
    "\n",
    "            response_target = requests.get(target_url)\n",
    "\n",
    "            target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "            return target_page_soup\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(\"Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \\n\")\n",
    "        \n",
    "        # Otherwise, return the target_page_soup that was passed in.\n",
    "        \n",
    "        return target_page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_xpath_top_bottom():\n",
    "    \n",
    "    x = random.randint(3, 8)\n",
    "    \n",
    "    def rdm_slp_3_8(x):\n",
    "\n",
    "        time.sleep(x)\n",
    "\n",
    "        print(f\"Slept for {x} seconds. \\n\")\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Check if there are working links on the screen, otherwise alert the user.\n",
    "    \n",
    "    if (browser.is_element_present_by_tag('h1')) == True:\n",
    "        \n",
    "        print(\"(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"(Check 1 - ERROR - Random Xpath Top Bottom) Header is NOT present on page. \\n\")\n",
    "        \n",
    "        for s in range(0, 1):\n",
    "            \n",
    "            playsound('./sounds/user_alert.wav')\n",
    "        \n",
    "        red_light = input(\"Program could not detect a clickable links to hover over, and click. Please use your mouse to refresh the page, and enter 'y' to continue the scrape. \\n\")\n",
    "        \n",
    "    if (browser.is_element_present_by_tag(\"a\")) == True:\n",
    "        \n",
    "        print(\"(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \\n\")\n",
    "        \n",
    "    else:\n",
    "        # If there isn't, pause the program. Have user click somewhere on the screen.\n",
    "        \n",
    "        for s in range(0, 1):\n",
    "            \n",
    "            playsound('./sounds/user_alert.wav')\n",
    "        \n",
    "        red_light = input(\"Program could not detect a clickable links to hover over, and click. Please use your mouse to refresh the page, and enter 'y' to continue the scrape. \\n\")\n",
    "    \n",
    "    # There are clickable links, then 'flip the coin' to choose top or bottom button\n",
    "    \n",
    "    coin_toss_top_bottom = random.randint(0,1)\n",
    "    \n",
    "    next_page_button_results = []\n",
    "    \n",
    "    # If the coin toss is even, mouse_over and click the top page link.\n",
    "    \n",
    "    if (coin_toss_top_bottom == 0):\n",
    "        \n",
    "        print('Heads - Clicking \"Next Page\" Top Button. \\n')\n",
    "        \n",
    "        rdm_slp_3_8(x)\n",
    "        \n",
    "        print(f\"Mimic human behavior by randomly sleeping for {x}. \\n\")\n",
    "        \n",
    "        browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').mouse_over()\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "        \n",
    "        next_page_button_results.append(coin_toss_top_bottom)\n",
    "        \n",
    "        print('Heads - SUCCESSFUL \"Next Page\" Top Button. \\n')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('Tails - Clicking \"Next Page\" Bottom Button. \\n')\n",
    "        \n",
    "        rdm_slp_3_8(x)\n",
    "        \n",
    "        print(f\"Mimic human behavior by randomly sleeping for {x}. \\n\")\n",
    "        \n",
    "        browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[4]/div/div/div[11]/button').mouse_over()\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[4]/div/div/div[11]/button').click()\n",
    "        \n",
    "        next_page_button_results.append(coin_toss_top_bottom)\n",
    "        \n",
    "        print('Tails - SUCCESSFUL \"Next Page\" Bottom Button. \\n')\n",
    "        \n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOriginally modeled out parent/child inheritance object structure.\\n\\nAfter careful research, I found it much easier to export the Pandas Dataframe of the results to a dictionary,\\n\\nand then into a class object, which I will elaborate more down below.\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This class takes in the dictionary from the webscraper function, and will be used in a list comprehension\n",
    "\n",
    "to produce class \"objects\"\n",
    "\n",
    "\"\"\"\n",
    "class Laptops:\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    def __init__(self, **entries):\n",
    "        \n",
    "        self.__dict__.update(entries)\n",
    "        \n",
    "    def count(self):\n",
    "        print(f\"Total Laptops scraped: {Laptops.counter}\")\n",
    "\n",
    "\"\"\"\n",
    "Originally modeled out parent/child inheritance object structure.\n",
    "\n",
    "After careful research, I found it much easier to export the Pandas Dataframe of the results to a dictionary,\n",
    "\n",
    "and then into a class object.\n",
    "\n",
    "\"\"\"\n",
    "# class Product_catalog:\n",
    "    \n",
    "#     all_prod_count = 0\n",
    "    \n",
    "#     def __init__(self, general_category): # computer systems\n",
    "#         self.general_category = general_category\n",
    "        \n",
    "#         Product_catalog.all_prod_count += 1\n",
    "        \n",
    "#     def count_prod(self):\n",
    "#         return int(self.all_prod_count)\n",
    "#         #return '{}'.format(self.general_category)\n",
    "\n",
    "# Sub_category was later changed to Laptops due to the scope of this project.\n",
    "# class Sub_category(Product_catalog): # laptops/notebooks, gaming\n",
    "    \n",
    "#     sub_category_ct = 0\n",
    "    \n",
    "#     def __init__(self, general_category, sub_categ, item_num, brand, price, img_link, prod_link, model_specifications, current_promotions):\n",
    "#         super().__init__(general_category)\n",
    "#         Sub_category.sub_category_ct += 1\n",
    "        \n",
    "#         self.sub_categ = sub_categ\n",
    "#         self.item_num = item_num\n",
    "#         self.brand = brand\n",
    "#         self.price = price\n",
    "#         self.img_link = img_link\n",
    "#         self.prod_link = prod_link\n",
    "#         self.model_specifications = model_specifications\n",
    "#         self.current_promotions = current_promotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program Logic\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NewEgg.Com Laptop WebScraper Beta v1.0 ===\n",
      "============================================================\n",
      "Scope: This project is a beta and is only built to scrape the laptop section of NewEgg.com due to limited time. \n",
      "\n",
      "Instructions: \n",
      "\n",
      "Current Date And Time: 2020-04-25_12.25.42 \n",
      "\n",
      "(1) Go to www.newegg.com, go to the laptop section, select your requirements (e.g. brand, screensize, and specifications - SSD size, processor brand and etc...) \n",
      "(2) Copy and paste the url from your exact search when prompted \n",
      "(3) After the webscraping is successful, you will have an option to concatenate all of the pages you scraped together into one csv file\n",
      "(4) Lastly, you will have an option to clear out the processing folder (data scraped by each page)\n",
      "(5) If you have any issues or errors, \"PRESS CTRL + C\" to quit the program in the terminal \n",
      "(6) You may run the program in the background as the program will make an alert noise to flag when Newegg suspects there is a bot, and will pause the scrape until you finish proving you are human. \n",
      "(7) Disclaimer: Newegg may ban you for a 24 - 48 hours for webscraping their data, then you may resume. \n",
      " Also, please consider executing during the day, with tons of web traffic to their site in your respective area. \n",
      "\n",
      "Happy Scraping!\n",
      "Please copy and paste your laptop query that you want to webscrape, and press enter: \n",
      "https://www.newegg.com/p/pl?N=100006740%20601307583%20601107729%20601274231%20600136700%20601313977%20600337010%20600440394\n",
      "<Response [200]> \n",
      "\n",
      "Beginning webscraping and activity log below... \n",
      "============================================================\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Scraping Current Page: 1 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 1 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2160 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "<a> number = 2011 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "<a> number = 2025 | Current Attempts (Try Count): 2 \n",
      "\n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "<a> number = 2039 | Current Attempts (Try Count): 3 \n",
      "\n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 1) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 8 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 8. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 2 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 2 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2155 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 2) | SLEEPING FOR 5 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 3 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 3. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 3 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 3 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2153 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 10 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 3) | SLEEPING FOR 3 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 5 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 5. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 4 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 4 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2156 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 9 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 4) | SLEEPING FOR 3 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 8 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 8. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 5 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 5 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2152 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 9 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 5) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 3 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 3. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 6 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 6 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2151 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 2002 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 6) | SLEEPING FOR 3 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 7 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 7. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 7 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 7 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2154 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 7) | SLEEPING FOR 3 SECONDS THEN will click next page. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 7 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 7. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 8 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 8 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2152 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 9 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 8) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 6 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 6. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 9 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 9 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2153 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 7 seconds. \n",
      "<a> number = 2004 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Mimic Humans - Sleeping for 7 seconds. \n",
      "<a> number = 2020 | Current Attempts (Try Count): 2 \n",
      "\n",
      "Mimic Humans - Sleeping for 7 seconds. \n",
      "<a> number = 2036 | Current Attempts (Try Count): 3 \n",
      "\n",
      "Mimic Humans - Sleeping for 7 seconds. \n",
      "<a> number = 2052 | Current Attempts (Try Count): 4 \n",
      "\n",
      "Mimic Humans - Sleeping for 7 seconds. \n",
      "<a> number = 2068 | Current Attempts (Try Count): 5 \n",
      "\n",
      "Current Page: 9) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 8 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 8. \n",
      "\n",
      " (EXCEPTION) Current Page: 9) | SLEEPING FOR 5 SECONDS - Will click next page, if applicable. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 6 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 6. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 10 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 10 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2158 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "<a> number = 1987 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 10) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 7 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 7. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 11 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 11 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2152 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 7 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 11) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 3 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 3. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 12 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 12 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2152 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 10 seconds. \n",
      "<a> number = 1982 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Mimic Humans - Sleeping for 10 seconds. \n",
      "<a> number = 2002 | Current Attempts (Try Count): 2 \n",
      "\n",
      "Mimic Humans - Sleeping for 10 seconds. \n",
      "<a> number = 2022 | Current Attempts (Try Count): 3 \n",
      "\n",
      "Mimic Humans - Sleeping for 10 seconds. \n",
      "<a> number = 2042 | Current Attempts (Try Count): 4 \n",
      "\n",
      "Mimic Humans - Sleeping for 10 seconds. \n",
      "<a> number = 2062 | Current Attempts (Try Count): 5 \n",
      "\n",
      "Current Page: 12) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 5 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 5. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 13 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 13 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2153 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "<a> number = 1983 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "<a> number = 2006 | Current Attempts (Try Count): 2 \n",
      "\n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 13) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 4 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 4. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 14 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 14 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2153 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 14) | SLEEPING FOR 5 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 4 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 4. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 15 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 15 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2152 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 7 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 15) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 4 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 4. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 16 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 16 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2152 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 16) | SLEEPING FOR 3 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 5 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 5. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 17 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 17 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2152 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 7 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 17) | SLEEPING FOR 5 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 8 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 8. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 18 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 18 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2155 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 2006 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 2021 | Current Attempts (Try Count): 2 \n",
      "\n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 18) | SLEEPING FOR 5 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 7 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 7. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 19 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 19 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 2152 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 1960 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 1974 | Current Attempts (Try Count): 2 \n",
      "\n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 1988 | Current Attempts (Try Count): 3 \n",
      "\n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 2002 | Current Attempts (Try Count): 4 \n",
      "\n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 2016 | Current Attempts (Try Count): 5 \n",
      "\n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 2030 | Current Attempts (Try Count): 6 \n",
      "\n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 2044 | Current Attempts (Try Count): 7 \n",
      "\n",
      "Mimic Humans - Sleeping for 6 seconds. \n",
      "<a> number = 2058 | Current Attempts (Try Count): 8 \n",
      "\n",
      "Current Page: 19) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 8 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 8. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 20 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 20 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 297 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 9 seconds. \n",
      "<a> number = 127 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Current Page: 20) | SLEEPING FOR 5 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 5 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 5. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 21 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 21 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 297 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "<a> number = 133 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Current Page: 21) | SLEEPING FOR 3 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Heads - Clicking \"Next Page\" Top Button. \n",
      "\n",
      "Slept for 3 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 3. \n",
      "\n",
      "Heads - SUCCESSFUL \"Next Page\" Top Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 22 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 22 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 297 <a> tags when parsing html... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mimic Humans - Sleeping for 9 seconds. \n",
      "<a> number = 127 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Current Page: 22) | SLEEPING FOR 3 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 4 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 4. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Beginning mouse over activity... \n",
      "\n",
      "Passed the 'Are you human?' check when requesting and parsing the html. Continuing with scrape ... \n",
      "\n",
      "Scraping Current Page: 23 \n",
      "\n",
      "Creating laptop objects for this page... \n",
      "\n",
      "Finished creating Laptop objects for page 23 ... \n",
      "\n",
      "Adding 36 to laptop catalog... \n",
      "\n",
      "Found 297 <a> tags when parsing html... \n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "<a> number = 121 | Current Attempts (Try Count): 1 \n",
      "\n",
      "Mimic Humans - Sleeping for 8 seconds. \n",
      "EXCEPTION raised during mouse over. Going to break loop and proceed with moving to the next page. \n",
      "\n",
      "Current Page: 23) | SLEEPING FOR 4 SECONDS THEN will click next page. \n",
      "\n",
      "(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \n",
      "\n",
      "(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "\n",
      "Tails - Clicking \"Next Page\" Bottom Button. \n",
      "\n",
      "Slept for 3 seconds. \n",
      "\n",
      "Mimic human behavior by randomly sleeping for 3. \n",
      "\n",
      "Tails - SUCCESSFUL \"Next Page\" Bottom Button. \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "All 24 pages have been saved in the \"processing\" folder (1 page = csv files). Would you like for us concatenate all the files into one? Enter \"y\", if so. Otherwise, enter anykey to exit the program. \n",
      "y\n",
      "WebScraping Complete! All 24 have been scraped and saved as 2020-04-25_12.25.42_LaptopsNotebooks_scraped_24_pages_.csv in the \"finished_outputs\" folder \n",
      "\n",
      "The \"processing\" folder has 24 csv files of each page that was scraped. Would you like to clear the files? Enter \"y\", if so. Otherwise, enter anykey to exit the program. \n",
      "n\n",
      "Thank you checking out my project, and hope you found this useful! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Welcome to the program message!\n",
    "\"\"\"\n",
    "print(\"=== NewEgg.Com Laptop WebScraper Beta v1.0 ===\")\n",
    "\n",
    "print(\"==\"*30)\n",
    "\n",
    "print('Scope: This project is a beta and is only built to scrape the laptop section of NewEgg.com due to limited time. \\n')\n",
    "\n",
    "print(\"Instructions: \\n\")\n",
    "\n",
    "return_dt()\n",
    "\n",
    "print(f'Current Date And Time: {current_date} \\n')\n",
    "\n",
    "print(\"(1) Go to www.newegg.com, go to the laptop section, select your requirements (e.g. brand, screensize, and specifications - SSD size, processor brand and etc...) \")\n",
    "\n",
    "print(\"(2) Copy and paste the url from your exact search when prompted \")\n",
    "\n",
    "print('(3) After the webscraping is successful, you will have an option to concatenate all of the pages you scraped together into one csv file')\n",
    "\n",
    "print('(4) Lastly, you will have an option to clear out the processing folder (data scraped by each page)')\n",
    "\n",
    "print('(5) If you have any issues or errors, \"PRESS CTRL + C\" to quit the program in the terminal ')\n",
    "\n",
    "print('(6) You may run the program in the background as the program will make an alert noise to flag when Newegg suspects there is a bot, and will pause the scrape until you finish proving you are human. ')\n",
    "\n",
    "print('(7) Disclaimer: Newegg may ban you for a 24 - 48 hours for webscraping their data, then you may resume. \\n Also, please consider executing during the day, with tons of web traffic to their site in your respective area. \\n')\n",
    "\n",
    "print('Happy Scraping!')\n",
    "\n",
    "# Set up Splinter requirements.\n",
    "\n",
    "executable_path = {'executable_path': './chromedriver.exe'}\n",
    "\n",
    "# Ask user to input in the laptop query link they would like to scrape.\n",
    "\n",
    "url = input(\"Please copy and paste your laptop query that you want to webscrape, and press enter: \\n\")\n",
    "\n",
    "browser = Browser('chrome', **executable_path, headless=False, incognito=True)\n",
    "\n",
    "browser.visit(url)\n",
    "\n",
    "current_url = browser.url\n",
    "\n",
    "# Allocating loading time.\n",
    "\n",
    "time.sleep(3)\n",
    "        \n",
    "response = requests.get(current_url)\n",
    "\n",
    "print(f\"{response} \\n\")\n",
    "\n",
    "target_page_soup = soup(response.text, 'html.parser')\n",
    "\n",
    "# Run the results_pages function to gather the total pages to be scraped.\n",
    "results_pages(target_page_soup)\n",
    "\n",
    "\"\"\"\n",
    "This is the loop that performs the page by page scraping of data / results\n",
    "of the user's query.\n",
    "\"\"\"\n",
    "# List set up for where class Laptop objects will be stored.\n",
    "\n",
    "print(\"Beginning webscraping and activity log below... \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "product_catalog = []\n",
    "\n",
    "for turn_page in range(1, total_results_pages+1):\n",
    "    \n",
    "    \"\"\"\n",
    "    If \"reCAPTCHA\" pops up, pause the program using an input. This allows the user to continue\n",
    "    to scrape after they're done completing the quiz by inputting any value.\n",
    "    \"\"\"\n",
    "    # Allocating loading time.\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Check if the site believes we are a bot, if so alert the user.\n",
    "    \n",
    "    g_recaptcha_check()\n",
    "\n",
    "    print(f\"Beginning mouse over activity... \\n\")\n",
    "    \n",
    "    # Set up \"containers\" to be passed into main scraping function. \n",
    "    \n",
    "    if turn_page == 1:\n",
    "        \n",
    "        containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        target_url = browser.url\n",
    "\n",
    "        # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor.\n",
    "        \n",
    "        response_target = requests.get(target_url)\n",
    "        \n",
    "        # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "        \n",
    "        target_page_soup = soup(response_target.text, 'html.parser')\n",
    "        \n",
    "        # Pass in target_page_soup to scan on the background (usually 10 pages in) if the html has text \"Are you human?\"\n",
    "        # If yes, the browser will refresh twice, and return a new target_page_soup that should have the scrapable items we want\n",
    "        \n",
    "        are_you_human_backend(target_page_soup)\n",
    "\n",
    "        containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "    \n",
    "    print(f\"Scraping Current Page: {turn_page} \\n\")\n",
    "    \n",
    "    # Execute webscraper function. Output is a csv file in the processing folder and dictionary.\n",
    "    \n",
    "    try:\n",
    "        newegg_page_scraper(containers, turn_page)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print(\"\")\n",
    "    \n",
    "    print(\"Creating laptop objects for this page... \\n\")\n",
    "    \n",
    "    # Create instances of class objects of the laptops/notebooks using a list comprehension.\n",
    "    \n",
    "    objects = [Laptops(**prod_obj) for prod_obj in scraped_dict]\n",
    "    \n",
    "    print(f\"Finished creating Laptop objects for page {turn_page} ... \\n\")\n",
    "    \n",
    "    # Append all of the objects to the main product_catalog list (List of List of Objects).\n",
    "    \n",
    "    print(f\"Adding {len(objects)} to laptop catalog... \\n\")\n",
    "    \n",
    "    product_catalog.append(objects)\n",
    "    \n",
    "    random_a_tag_mouse_over3()\n",
    "\n",
    "    #print(\"Will scrape pages, but will need to randomly sleep for max 35 seconds to emulate human behavior. \\n\")\n",
    "    \n",
    "    if turn_page == total_results_pages:\n",
    "        \n",
    "        print(f\"Completed scraping {turn_page} / {total_results_pages} pages. \\n \")\n",
    "        \n",
    "        # Exit the broswer once complete webscraping.\n",
    "        \n",
    "        browser.quit()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            y = random.randint(3, 5)\n",
    "            \n",
    "            print(f\"Current Page: {turn_page}) | SLEEPING FOR {y} SECONDS THEN will click next page. \\n\")\n",
    "            \n",
    "            time.sleep(y)\n",
    "            \n",
    "            random_xpath_top_bottom()\n",
    "\n",
    "        except:\n",
    "            z = random.randint(3, 5)\n",
    "            \n",
    "            print(f\" (EXCEPTION) Current Page: {turn_page}) | SLEEPING FOR {z} SECONDS - Will click next page, if applicable. \\n\")\n",
    "            \n",
    "            time.sleep(z)\n",
    "            \n",
    "            random_xpath_top_bottom()\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "    print(\"\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\")\n",
    "\n",
    "# Prompt the user if they would like to concatenate all of the pages into one csv file\n",
    "\n",
    "concat_y_n = input(f'All {total_results_pages} pages have been saved in the \"processing\" folder (1 page = csv files). Would you like for us concatenate all the files into one? Enter \"y\", if so. Otherwise, enter anykey to exit the program. \\n')\n",
    "\n",
    "if concat_y_n == 'y':\n",
    "    \n",
    "    concatenate(total_results_pages)\n",
    "    \n",
    "    print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{pdt_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder \\n')\n",
    "\n",
    "# Prompt the user to if they would like to clear out processing folder function here - as delete everything to prevent clutter\n",
    "\n",
    "clear_processing_y_n = input(f'The \"processing\" folder has {total_results_pages} csv files of each page that was scraped. Would you like to clear the files? Enter \"y\", if so. Otherwise, enter anykey to exit the program. \\n')\n",
    "\n",
    "if clear_processing_y_n == 'y':\n",
    "    \n",
    "    clean_processing_fldr()\n",
    "\n",
    "print('Thank you checking out my project, and hope you found this useful! \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
