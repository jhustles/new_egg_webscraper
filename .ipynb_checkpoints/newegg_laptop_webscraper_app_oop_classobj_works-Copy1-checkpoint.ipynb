{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from re import search\n",
    "import copy # might not need this\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import random\n",
    "#import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dt():\n",
    "    global current_date\n",
    "    current_date = str(datetime.datetime.now()).replace(':','.').replace(' ','_')[:-7]\n",
    "    return current_date\n",
    "#return_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of class objects:\n",
    "\n",
    "product_catalog = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this below to pull single items and test parts of the scraper and the head on off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " executable_path = {'executable_path': './chromedriver.exe'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " url = input(\"Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2)\n",
    "\n",
    "# Function to ask users if they want to watch the Bot (headless = False) work OR not (headless = True)\n",
    "# Lastly, will take you directly to the webpage that was inputted\n",
    "head = ''\n",
    "browser =''\n",
    "def head_on_off(executable_path):\n",
    "    # Have moved two preset variables, head and browser that are both \" = '' \"\n",
    "    # assigning these as global variables enable us to reference them outside and inside the function\n",
    "    global head\n",
    "    global browser\n",
    "    # options creates a bound to an answer\n",
    "    options = [1, 2]\n",
    "    #executable_path = {'executable_path': './chromedriver.exe'}\n",
    "    # for all cases where users input in a value that is not valid\n",
    "    while head not in options:\n",
    "        head = int(input('Do you want the desktop watch the bot work? Enter a number: 1 - YES | 2 - NO . Your Answer: '))\n",
    "        if head not in options:\n",
    "            print(\"That was not a valid answer. Please try again. \")\n",
    "    # For cases where users enter in valid options:\n",
    "    if head == options[0]:\n",
    "        print('Head is activated. Please view only the new automated Google Chrome web browser. ')\n",
    "        print('Do not make any adjustments to this automated window while the program runs, as it may produce errors or undesired outputs. ')\n",
    "        browser = Browser('chrome', **executable_path, headless=False)\n",
    "    if head == options[1]:\n",
    "        print('Headless mode activated. No web browser will pop up. Please proceeed. ')\n",
    "        browser = Browser('chrome', **executable_path, headless=True)\n",
    "    # visit the target site\n",
    "    browser.visit(url)\n",
    "    global current_url\n",
    "    current_url = browser.url\n",
    "    #print(current_url)\n",
    "    return current_url\n",
    "\n",
    "#head_on_off(executable_path)\n",
    "#time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3)\n",
    "\n",
    "# Use Splinter to grab the current url, to setup request to pull URL\n",
    "#current_url = browser.url #+ '&Page=' #+ str(turn_page)\n",
    "\n",
    "# Use Request.get() to pull the current url\n",
    "current_url = browser.url\n",
    "print(current_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(current_url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4)\n",
    "\n",
    "# Use BeautifulSoup to grab all the HTML using the htmlparser\n",
    "current_page_soup = soup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_page_soup.find_all('a', class_=\"item-title\")[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_soup.find_all('div', class_=\"nav-x-body-top-bar fix\")[0].text.split('\\n')[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_soup.find_all('h1', class_=\"page-title-text\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_page_soup.find_all(\"a\", class_=\"item-title\")[0].text\n",
    "#current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "\n",
    "#example = current_page_soup.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "\n",
    "try:\n",
    "    example1 = current_page_soup.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "    print(\"example 1\")\n",
    "    print(example1)\n",
    "except (IndexError) as e:\n",
    "    example2 = current_page_soup.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]\n",
    "    print(\"example 2\")\n",
    "    print(example2)\n",
    "#bool(example.split('?'))\n",
    "\n",
    "# if bool(example.split('?')) == True:\n",
    "#     #example.split('?')\n",
    "    \n",
    "# if re.search('?', example) == True:\n",
    "#     print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.split('?')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_soup.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 5) Are there scrappable items-contrainers on the page? List first, last and count, also how many pages\n",
    "\n",
    "def scrappable_y_n(current_page_soup):\n",
    "    global containers\n",
    "    containers = current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "    \n",
    "    # print first and last objects so users can understand what the output will be\n",
    "    print(\"Preview: expect these scrapped off this page, and for every other total results pages, if there's more than one: \")\n",
    "    print(\"=\"*35)\n",
    "    # max items should be 36\n",
    "    counter = 0\n",
    "    for con in containers:\n",
    "        try:\n",
    "            counter += 1\n",
    "            product_details = con.find_all(\"a\", class_=\"item-title\")[0].text\n",
    "            product_price = con.find_all(\"li\", class_=\"price-current\")[0].text.split()[0]\n",
    "            print(f'{counter}) {product_details} | Price: {product_price}')\n",
    "            print(\"-\"*35)\n",
    "            \n",
    "        except (IndexError) as e:\n",
    "            print(f\"{counter}) This item was not scrappable. Skipped. \")\n",
    "            print(\"-\"*35)\n",
    "            \n",
    "    print(\"=\"*60)\n",
    "    if counter == 0:\n",
    "        print(\"Unable to scrap this link. \")\n",
    "    else:\n",
    "        print(f\"{len(containers)} Scrappable Objects on the page. \")\n",
    "    #return current_page_soup\n",
    "#scrappable_y_n(current_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic classes here and then have the function create product objects AND export out to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product_catalog:\n",
    "    \n",
    "    all_prod_count = 0\n",
    "    \n",
    "    def __init__(self, general_category): # computer systems\n",
    "        self.general_category = general_category\n",
    "        \n",
    "        Product_catalog.all_prod_count += 1\n",
    "        \n",
    "    def count_prod(self):\n",
    "        return int(self.all_prod_count)\n",
    "        #return '{}'.format(self.general_category)\n",
    "        \n",
    "class Sub_category(Product_catalog): # laptops/notebooks, gaming\n",
    "    \n",
    "    sub_category_ct = 0\n",
    "    \n",
    "    def __init__(self, general_category, sub_categ, item_num, brand, price, img_link, prod_link, model_specifications, current_promotions):\n",
    "        super().__init__(general_category)\n",
    "        Sub_category.sub_category_ct += 1\n",
    "        \n",
    "        self.sub_categ = sub_categ\n",
    "        self.item_num = item_num\n",
    "        self.brand = brand\n",
    "        self.price = price\n",
    "        self.img_link = img_link\n",
    "        self.prod_link = prod_link\n",
    "        self.model_specifications = model_specifications\n",
    "        self.current_promotions = current_promotions\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CREATING OBJECTS AND IT WORKS\n",
    "\n",
    "Sub_category(\n",
    "\"Computer_Systems\",\n",
    "\"Laptops/Notebooks\",\n",
    "'Item=9SIA7AB8D73120',\n",
    "\"HP\",\n",
    "1449.00, \"//c1.neweggimages.com/ProductImageCompressAll300/A7AB_1_201811092013621813.jpg\",\n",
    "'https://www.newegg.com/p/1TS-000D-032G0?Item=9SIA7AB8D73120',\n",
    "                      \n",
    "'HP EliteBook 840 G5 Premium School and Business Laptop (Intel 8th Gen i7-8550U Quad-Core, 16GB RAM, 256GB PCIe SSD, 14\" FHD 1920x1080 Sure View Display, Thunderbolt3, NFC, Fingerprint, Win 10 Pro)',   \n",
    "'Free Expedited Shipping')\n",
    "\n",
    "\n",
    "Sub_category(\n",
    "\"Computer_Systems\",\n",
    "\"Laptops/Notebooks\",\n",
    "'Item=0000000000000',\n",
    "\"HP\",\n",
    "1449.00, \"//c1.neweggimages.com/ProductImageCompressAll300/A7AB_1_201811092013621813.jpg\",\n",
    "'https://www.newegg.com/p/1TS-000D-032G0?Item=AKAKAKAKAKA',\n",
    "                      \n",
    "'HP EliteBook 840 G5 Premium School and Business Laptop (Intel 8th Gen i7-8550U Quad-Core, 16GB RAM, 256GB PCIe SSD, 14\" FHD 1920x1080 Sure View Display, Thunderbolt3, NFC, Fingerprint, Win 10 Pro)',   \n",
    "'Free Expedited Shipping')\n",
    "\n",
    "#Product_catalog.all_prod_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning purposes\n",
    "Sub_category.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Product_catalog.__dict__\n",
    "print(Product_catalog.count_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to pass in the Sub_category for it to know what to count\n",
    "Product_catalog.count_prod(Sub_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lptp_1.count_prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub_category.sub_category_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keys_list = list(lptp_1.__dict__.keys())\n",
    "keys_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lptp_1.__dict__.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values_list = list(lptp_1.__dict__.values())\n",
    "dict_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    'keys': keys_list,\n",
    "    'values': dict_values_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# handy trick - to see all data from the object.\n",
    "#print(prod_1.__dict__)\n",
    "#prod_1\n",
    "\n",
    "lptp_1 = sub_category(\"Computer_Systems\", Product_catalog.all_prod_count,\"Laptops/Notebooks\", sub_category.sub_cat_ct, \"HP\", 1449.00, \"//c1.neweggimages.com/ProductImageCompressAll300/A7AB_1_201811092013621813.jpg\", 'https://www.newegg.com/p/1TS-000D-032G0?Item=9SIA7AB8D73120',\n",
    "'HP EliteBook 840 G5 Premium School and Business Laptop (Intel 8th Gen i7-8550U Quad-Core, 16GB RAM, 256GB PCIe SSD, 14\" FHD 1920x1080 Sure View Display, Thunderbolt3, NFC, Fingerprint, Win 10 Pro)', 'Free Expedited Shipping')\n",
    "lptp_1.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_2.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKIP THIS ONE - NEXT ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reenabled item_number\n",
    "def newegg_page_scraper(containers, turn_page): #before: (containers, turn_page)\n",
    "    images = []\n",
    "    product_brands = []\n",
    "    product_models = []\n",
    "    product_links = []\n",
    "    item_numbers = []\n",
    "    product_categories = []\n",
    "    promotions = []\n",
    "    prices = []\n",
    "    shipping_terms = []\n",
    "    page_nums = []\n",
    "\n",
    "\n",
    "    for con in containers:\n",
    "        try:\n",
    "            page_counter = turn_page\n",
    "            page_nums.append(int(turn_page))\n",
    "            \n",
    "            image = con.a.img[\"src\"]\n",
    "            #print(image)\n",
    "            images.append(image)\n",
    "\n",
    "            prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "            product_models.append(prd_title)\n",
    "\n",
    "            product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "            product_links.append(product_link)\n",
    "            \n",
    "            shipping = con.find_all('li', class_='price-ship')[0].text.strip().split()[0]\n",
    "            if shipping != \"Free\":\n",
    "                shipping = shipping.replace('$', '')\n",
    "                shipping_terms.append(shipping)\n",
    "            else:\n",
    "                shipping = 0.00\n",
    "                shipping_terms.append(shipping)\n",
    "\n",
    "            brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "            product_brands.append(brand_name)\n",
    "\n",
    "        except (IndexError, ValueError) as e:\n",
    "            # if there's no item_brand container, take the Brand from product details\n",
    "            product_brands.append(con.find_all('a', class_=\"item-title\")[0].text.split()[0])\n",
    "            #print(f\"{e} block 1\")\n",
    "\n",
    "        try:\n",
    "            current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "            promotions.append(current_promo)\n",
    "        except:\n",
    "            promotions.append('null')\n",
    "            #print(f\"{e} block 2\")\n",
    "        try:\n",
    "            price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "            prices.append(price)\n",
    "        except (IndexError, ValueError) as e:\n",
    "            prices.append('null')\n",
    "            #print(f\"{e} block 3\")\n",
    "        \n",
    "        try:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "            item_numbers.append(item_num)\n",
    "        except (IndexError) as e:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]\n",
    "            item_numbers.append(item_num)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "    'item_number': item_numbers,\n",
    "    'brand': product_brands,\n",
    "    'model_specifications': product_models,\n",
    "    'price': prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': shipping_terms,\n",
    "    'page_number': page_nums,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "    df['general_category'] = current_page_soup.find_all('div', class_=\"nav-x-body-top-bar fix\")[0].text.split('\\n')[5]\n",
    "    df['product_category'] = current_page_soup.find_all('h1', class_=\"page-title-text\")[0].text\n",
    "    # rearrange columns\n",
    "    df = df[['item_number', 'general_category','product_category', 'page_number' ,'brand','model_specifications' ,'current_promotions' ,'price' ,'shipping' ,'product_links','image_link']]\n",
    "    global product_category\n",
    "    product_category = df['product_category'].unique()[0]\n",
    "    # eliminate special characters in a string if it exists\n",
    "    product_category = ''.join(e for e in product_category if e.isalnum())\n",
    "    \n",
    "    #return_list.append(product_category)\n",
    "    global items_scraped\n",
    "    items_scraped = len(df['model_specifications'])\n",
    "\n",
    "    \n",
    "    df.to_csv(f'./processing/{current_date}_{product_category}_{items_scraped}_scraped_page{turn_page}.csv')\n",
    "    return items_scraped, product_category\n",
    "    \n",
    "#df.head()\n",
    "#newegg_page_scraper(containers, turn_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(containers[1].find_all('a', class_=\"item-brand\")[0].img[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST VERSION FOR CLASSES #############################\n",
    "def newegg_page_scraper(containers, turn_page): #before: (containers, turn_page)\n",
    "    page_nums = []\n",
    "    general_category = []\n",
    "    product_categories = []\n",
    "    images = []\n",
    "    product_brands = []\n",
    "    product_models = []\n",
    "    product_links = []\n",
    "    item_numbers = []\n",
    "    promotions = []\n",
    "    prices = []\n",
    "    shipping_terms = []\n",
    "    \n",
    "    global gen_category\n",
    "    \n",
    "    for con in containers:\n",
    "        try:\n",
    "            page_counter = turn_page\n",
    "            page_nums.append(int(turn_page))\n",
    "            \n",
    "            gen_category = target_page_soup.find_all('div', class_=\"nav-x-body-top-bar fix\")[0].text.split('\\n')[5]\n",
    "            general_category.append(gen_category)\n",
    "            \n",
    "            prod_category = target_page_soup.find_all('h1', class_=\"page-title-text\")[0].text\n",
    "            product_categories.append(prod_category)\n",
    "            \n",
    "            prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "            product_models.append(prd_title)\n",
    "\n",
    "            product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "            product_links.append(product_link)\n",
    "            \n",
    "            shipping = con.find_all('li', class_='price-ship')[0].text.strip().split()[0]\n",
    "            if shipping != \"Free\":\n",
    "                shipping = shipping.replace('$', '')\n",
    "                shipping_terms.append(shipping)\n",
    "            else:\n",
    "                shipping = 0.00\n",
    "                shipping_terms.append(shipping)\n",
    "\n",
    "            brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "            product_brands.append(brand_name)\n",
    "\n",
    "        except (IndexError, TypeError, ValueError) as e:\n",
    "            # if there's no item_brand container, take the Brand from product details\n",
    "            product_brands.append(con.find_all('a', class_=\"item-title\")[0].text.split()[0])\n",
    "        finally:\n",
    "            product_brands.append('no brand data available / unable to scrape - refer to screenshots')\n",
    "\n",
    "        try:\n",
    "            current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "            promotions.append(current_promo)\n",
    "        except (IndexError, TypeError, ValueError) as e:\n",
    "            promotions.append('no promotion / unable to scrape - refer to screenshots')\n",
    "            \n",
    "        try:\n",
    "            image = con.a.img[\"src\"]\n",
    "            images.append(image)\n",
    "        except (IndexError, TypeError, ValueError) as e:\n",
    "            images.append(\"unable to scrape image / not available - refer to screenshots\")\n",
    "            \n",
    "        try:\n",
    "            price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "            prices.append(price)\n",
    "        except (IndexError, TypeError, ValueError) as e:\n",
    "            prices.append('no promotion / unable to scrape - refer to screenshots')\n",
    "            #print(f\"{e} block 3\")\n",
    "        \n",
    "        try:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "            item_numbers.append(item_num)\n",
    "        except (IndexError, TypeError, ValueError) as e:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]\n",
    "            item_numbers.append(item_num)    \n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "    'item_number': item_numbers,\n",
    "    'general_category': general_category,\n",
    "    'product_category': product_categories,\n",
    "    'brand': product_brands,\n",
    "    'model_specifications': product_models,\n",
    "    'price': prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': shipping_terms,\n",
    "    'page_number': page_nums,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "    \n",
    "    df = df[['item_number', 'general_category','product_category', 'page_number' ,'brand','model_specifications' ,'current_promotions' ,'price' ,'shipping' ,'product_links','image_link']]\n",
    "    global scraped_dict\n",
    "    scraped_dict = df.to_dict('records')\n",
    "    \n",
    "    global pdt_category\n",
    "    pdt_category = df['product_category'].unique()[0]\n",
    "    # eliminate special characters in a string if it exists\n",
    "    pdt_category = ''.join(e for e in pdt_category if e.isalnum())\n",
    "    \n",
    "    global items_scraped\n",
    "    items_scraped = len(df['model_specifications'])\n",
    "    \n",
    "    df.to_csv(f'./processing/{current_date}_{pdt_category}_{items_scraped}_scraped_page{turn_page}.csv')\n",
    "    return scraped_dict, items_scraped, pdt_category\n",
    "    \n",
    "#df.head()\n",
    "#newegg_page_scraper(containers, turn_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "\n",
    "# create a function to return results pages, if exists, otherwise just scrape one page\n",
    "def results_pages(current_page_soup):\n",
    "    # Use BeautifulSoup to extract the total results page number\n",
    "    results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "    #print(results_pages)\n",
    "    # Find and extract total pages + and add 1 to ensure proper length of total pages\n",
    "    global total_results_pages\n",
    "    total_results_pages = int(re.split(\"/\", results_pages)[1]) + 2 # need to add 2 b/c 'range(inclusive, exclusive)'\n",
    "    #========================================= need to remember to +2, and remove -30\n",
    "    #print(total_results_pages)\n",
    "    \n",
    "    return total_results_pages\n",
    "#results_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working\n",
    "def concatenate(total_results_pages):\n",
    "    \n",
    "    path = f'./processing\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    concatenate_pages = []\n",
    "    counter = 0\n",
    "    for page in scraped_pages:\n",
    "        df = pd.read_csv(page, index_col=0, header=0)\n",
    "        concatenate_pages.append(df)\n",
    "\n",
    "    compiled_data = pd.concat(concatenate_pages, axis=0, ignore_index=True)\n",
    "    total_items_scraped = len(compiled_data['brand']) # can replace this counter by creating class objects everytime it scrapes\n",
    "    concatenated_output = compiled_data.to_csv(f\"./finished_outputs/{current_date}_{total_items_scraped}_scraped_{total_results_pages}_pages_.csv\")\n",
    "    return concatenated_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_results_pages = 4\n",
    "# concatenate(total_results_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTRUCT CLASSES HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis is working\n",
    "def clean_processing_fldr():\n",
    "    # delete all files in the 'processing folder'\n",
    "    path = f'./processing\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    if len(scraped_pages) < 1:\n",
    "        print(\"There are no files in the folder to clear. \")\n",
    "    else:\n",
    "        print(f\"Clearing out a total of {len(scraped_pages)} scraped pages in the processing folder... \")\n",
    "        clear_processing_files = []\n",
    "        for page in scraped_pages:\n",
    "            os.remove(page)\n",
    "        \n",
    "    print('Clearing of \"Processing\" folder complete. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# probably won't use these but good practice of fundamentals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# webscrape first page, then run page turner, then scraper for every page thereafter\n",
    "# \n",
    "\n",
    "path = f'./finished_outputs\\\\'\n",
    "finished_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "dict = {}\n",
    "counter = 0\n",
    "for file in finished_files:\n",
    "    counter += 1\n",
    "    key = str(counter)\n",
    "    file = file[19:]\n",
    "    dict[key] = file\n",
    "    print(str(counter) + \") \" + file )\n",
    "    \n",
    "select = input(\"Which file would you like to read? Enter in the number. \")\n",
    "\n",
    "\n",
    "\n",
    "#with open(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not plan on using this... Save for later\n",
    "\n",
    "results = []\n",
    "for num in range(counter):\n",
    "    results.append(num)\n",
    "\n",
    "correct_selection = False\n",
    "while correct_selection == False:\n",
    "    \n",
    "    select = input(\"Which file would you like to read? Enter in the number. \")\n",
    "    \n",
    "    if int(select) not in results:\n",
    "        print(\"That was not a valid selection\")\n",
    "        correct_selection = False\n",
    "    else:\n",
    "        print(\"Processing now. \")\n",
    "        correct_selection = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold off on using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning lesson is you can't call a function within a function\n",
    "\n",
    "def page_turner(total_results_pages):\n",
    "    # This is \"NEXT PAGE BUTTON CLICK\" - This loops thru the total amount of pages by clicking the next page button\n",
    "    for turn_page in range(1, total_results_pages):\n",
    "        # set the current url as the target page (aiming the boomerang)\n",
    "        target_url = browser.url\n",
    "\n",
    "        # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "        response_target = requests.get(target_url)\n",
    "        #response\n",
    "\n",
    "        # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "        target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "        # Use BeautifulSoup to extract the total results page number\n",
    "        #results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "        \n",
    "        results_pages = target_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "        #=========================================================\n",
    "        containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "        \n",
    "        newegg_page_scraper(containers, turn_page)\n",
    "        \n",
    "        #for i in range(total_results_pages):\n",
    "        x = random.randint(3, 25)\n",
    "        print(f\"{turn_page}) | SLEEPING FOR SECONDS: {x} \")\n",
    "        time.sleep(x)\n",
    "            \n",
    "        browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "\n",
    "    browser.quit()\n",
    "# concatenate(total_results_pages)\n",
    "# clean_processing_fldr()\n",
    "# # clear out processing folder function here - as delete everything to prevent clutter\n",
    "\n",
    "\n",
    "# print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{product_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "# print('Thank you and hope you found this useful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume below - just skip the top function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sub_category:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NewEgg.Com WebScraper Beta ===\n",
      "============================================================\n",
      "Date: 2020-04-15_20.51.40\n",
      "\n",
      "Instructions:\n",
      "(1) Go to www.newegg.com, search for your laptop requirements (e.g. brand and specifications) \n",
      "(2) Copy and paste the url from your exact search \n",
      "(3) Activate or Disable the \"Head View\", webscraper bots point of view \n",
      "(4) Check the \"final_output folder when the webscraper bot is done scraping \"\n",
      "\n",
      "Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: https://www.newegg.com/p/pl?N=100006740%20600004343%20601296066%20600136700%20601286795\n",
      "Do you want the desktop watch the bot work? Enter a number: 1 - YES | 2 - NO . Your Answer: 1\n",
      "Head is activated. Please view only the new automated Google Chrome web browser. \n",
      "Do not make any adjustments to this automated window while the program runs, as it may produce errors or undesired outputs. \n",
      "Preview: expect these scrapped off this page, and for every other total results pages, if there's more than one: \n",
      "===================================\n",
      "1) HP EliteBook 840 G5 Premium School and Business Laptop (Intel 8th Gen i7-8550U Quad-Core, 16GB RAM, 256GB PCIe SSD, 14\" FHD 1920x1080 Sure View Display, Thunderbolt3, NFC, Fingerprint, Win 10 Pro) | Price: $1,449.00\n",
      "-----------------------------------\n",
      "2) Lenovo ThinkBook 14s Notebook, 14\" FHD Display, Intel Core i7-8565U Upto 4.6GHz, 16GB RAM, 256GB NVMe SSD, AMD Radeon 540X, HDMI, Wi-Fi, Bluetooth, Windows 10 Pro | Price: $997.99\n",
      "-----------------------------------\n",
      "3) Lenovo ThinkPad E490 Home and Business Laptop (Intel i7-8565U 4-Core, 16GB RAM, 256GB  SATA SSD, 14\" Full HD (1920x1080), Intel UHD 620, Fingerprint, Wifi, Bluetooth, Webcam, 2xUSB 3.0, Win 10 Pro) | Price: $1,258.80\n",
      "-----------------------------------\n",
      "4) 2018 Lenovo ThinkPad X1 Carbon (6th Gen) - Windows 10 Pro - Intel Quad Core i7-8550U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" FHD IPS (1920x1080) Display, Fingerprint Reader, Black | Price: $1,599.99\n",
      "-----------------------------------\n",
      "5) Dell Inspiron 14\" Full HD 2-in-1 Touchscreen Notebook, 8th Generation Intel Core i7-8565U,16GB DDR4 RAM,256GB SSD, Intel UHD Graphics 620,Wifi 802.11AC,Bluetooth,HD Webcam,HDMI,USB, Windows 10 Home | Price: $899.00\n",
      "-----------------------------------\n",
      "6) Lenovo ThinkPad T470 2020 Flagship Laptop, 14\" FHD IPS Touchscreen, Intel Core i7-6500U 2.5GHz, 16GB DDR4 256GB SSD, Thunderbolt Fingerprint Backlit KB WiFi HDMI Win 10 Pro | Price: $943.00\n",
      "-----------------------------------\n",
      "7) Newest Lenovo ThinkBook 14s 14\" FHD IPS Laptop PC, Intel Quad-Core i7-8565U upto 4.6GHz, 16GB RAM, 256GB SSD, Backlit Keyboard, Fingerprint Reader, AMD Radeon 540X, USB-C, WIFI, HDMI, Windows 10 Pro | Price: $892.99\n",
      "-----------------------------------\n",
      "8) LG Gram 14\" Full HD IPS MIL-Spec Notebook Computer, Intel Core i7-8565U 1.8GHz, 16GB RAM, 256GB SSD, Windows 10 Home, Dark Silver | Price: $1,196.99\n",
      "-----------------------------------\n",
      "9) DELL Laptop Latitude 7490 (R5VYY) Intel Core i7 8th Gen 8650U (1.90 GHz) 16 GB Memory 256 GB SSD Intel HD Graphics 620 14.1\" Windows 10 Pro 64-bit English, French, Spanish | Price: $3,564.99\n",
      "-----------------------------------\n",
      "10) HP ProBook 640 G4 Notebook, 14\" FHD Display, Intel Core i7-8650U Upto 4.2GHz, 16GB RAM, 256GB NVMe SSD, HDMI, VGA, DisplayPort via USB-C, Card Reader, Wi-Fi, Bluetooth, Windows 10 Pro (6JR72US) | Price: $1,399.99\n",
      "-----------------------------------\n",
      "11) Lenovo ThinkPad X1 Carbon Notebook, 14\" FHD Display, Intel Core i7-8650U Upto 4.2GHz, 16GB RAM, 256GB NVMe SSD, HDMI, Thunderbolt, Wi-Fi, Bluetooth, Windows 10 Pro | Price: $1,569.99\n",
      "-----------------------------------\n",
      "12) 2020 HP Probook 440 G6 14\" FHD Full HD (1920x1080) Business Laptop (Intel Quad-Core i7-8565U, 16GB DDR4 RAM, 256GB PCIe NVMe M.2 SSD) Backlit, Type-C, RJ-45, HDMI, Windows 10 Pro 64-bit | Price: $1,099.99\n",
      "-----------------------------------\n",
      "13) Lenovo ThinkPad T490s 20NX0032US 14\" Touchscreen Notebook - 1920 x 1080 - Core i7 i7-8665U - 16 GB RAM - 256 GB SSD - Black | Price: $2,086.81\n",
      "-----------------------------------\n",
      "14) Lenovo Flex 6, 14-Inch IPS Touchscreen, 2-in-1 Laptop (Intel Core i7-8550U 1.8GHz, NVIDIA GeForce MX130 Graphics, 16GB DDR4 RAM, 256GB SSD), 81EM000DUS Tablet Notebook PC Computer | Price: $1,057.00\n",
      "-----------------------------------\n",
      "15) DELL 14\" I7 7600U 16GB 256GB | Price: $1,999.11\n",
      "-----------------------------------\n",
      "16) HP ProBook 440 G6 Notebook, 14\" FHD Display, Intel Core i7-8565U Upto 4.6GHz, 16GB RAM, 256GB NVMe SSD, HDMI, Card Reader, Wi-Fi, Bluetooth, Windows 10 Pro | Price: $1,399.99\n",
      "-----------------------------------\n",
      "17) HP Laptop EliteBook 1040 G4 3NU56UT#ABA Intel Core i7 7th Gen 2.9 GHz 16 GB Memory 256 GB SSD Intel HD Graphics 630 14.0\" Touchscreen Windows 10 Pro 64-bit | Price: $2,186.99\n",
      "-----------------------------------\n",
      "18) 2018 Lenovo ThinkPad X1 Carbon (6th Gen) - Windows 10 Pro - Intel Quad Core i7-8550U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" WQHD IPS (2560x1440) Display, Fingerprint Reader, 4G LTE WWAN | Price: $1,999.99\n",
      "-----------------------------------\n",
      "19) Lenovo ThinkPad T470s Windows 10 Pro Laptop - Intel Core i7-7500U, 16GB RAM, 256GB PCIe NVMe SSD, 14\" IPS WQHD (2560x1440) Matte Display, Fingerprint Reader, Smart Card Reader, Black Color | Price: $1,704.99\n",
      "-----------------------------------\n",
      "20) 2017 Lenovo ThinkPad X1 Carbon 4G LTE (5th Gen) - Windows 10 Pro - Intel Core i7-7500U, 256GB SSD, 16GB RAM, 14\" WQHD IPS (2560x1440) Display, Fingerprint Reader, (Classic Black) | Price: $1,869.99\n",
      "-----------------------------------\n",
      "21) 2017 Lenovo ThinkPad X1 Carbon 4G LTE (5th Gen) - Windows 7 Pro - Intel Core i7-7500U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" WQHD IPS (2560x1440) Display, Fingerprint Reader, (Classic Black) | Price: $1,869.99\n",
      "-----------------------------------\n",
      "22) 2017 Lenovo ThinkPad X1 Carbon 4G LTE (5th Gen) - Windows 7 Pro - Intel Core i7-7500U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" FHD IPS (1920x1080) Display, Fingerprint Reader, (Classic Black) | Price: $1,759.99\n",
      "-----------------------------------\n",
      "23) 2018 Lenovo ThinkPad T480s Windows 10 Pro Laptop - Intel Core i7-8650U, 16GB RAM, 256GB PCIe NVMe SSD, 14\" IPS WQHD (2560x1440) Matte Display, Fingerprint Reader, Black Color | Price: $2,309.99\n",
      "-----------------------------------\n",
      "24) 2018 Lenovo ThinkPad X1 Carbon (6th Gen) - Windows 10 Pro - Intel Quad Core i7-8650U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" FHD IPS (1920x1080) Display, Fingerprint Reader, Black | Price: $1,649.97\n",
      "-----------------------------------\n",
      "25) 2018 Lenovo ThinkPad T480s Windows 10 Pro Laptop - Intel Core i7-8650U, 16GB RAM, 256GB SSD, 14\" IPS FHD (1920x1080) Matte Display, Fingerprint Reader,  Black Color | Price: $1,759.99\n",
      "-----------------------------------\n",
      "26) 2018 HDR Lenovo ThinkPad X1 Carbon (6th Gen) - Windows 10 Pro - Intel Quad Core i7-8650U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" HDR WQHD IPS (2560x1440) Non-Touch Display, Fingerprint Reader, Black | Price: $1,999.97\n",
      "-----------------------------------\n",
      "27) 2018 Lenovo ThinkPad T480s Windows 10 Pro Laptop -  i7-8650U, 16GB RAM, 256GB SSD, 14\" IPS FHD 1920x1080 Touch Display w/ NVIDIA MX150 Graphics, Smart Card Reader, 4G LTE WWAN,  Black | Price: $1,924.99\n",
      "-----------------------------------\n",
      "28) 2017 Lenovo ThinkPad X1 Carbon 4G LTE (5th Gen) - Windows 10 Pro - Intel Core i7-7600U, 256GB SSD, 16GB RAM, 14\" WQHD IPS (2560x1440) Display, Fingerprint Reader, (Classic Black) | Price: $1,979.99\n",
      "-----------------------------------\n",
      "29) 2018 Lenovo ThinkPad T480s Windows 10 Pro Laptop - Intel Core i7-8650U, 16GB RAM, 256GB PCIe NVMe SSD, 14\" IPS FHD (1920x1080) Matte Display, Fingerprint Reader, Black Color | Price: $1,759.99\n",
      "-----------------------------------\n",
      "30) 2018 HDR Lenovo ThinkPad X1 Carbon (6th Gen) - Windows 10 Pro - Intel Quad Core i7-8550U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" HDR WQHD IPS (2560x1440) Non-Touch Display, Fingerprint Reader, Black | Price: $1,799.99\n",
      "-----------------------------------\n",
      "31) 2018 Lenovo ThinkPad X1 Carbon Touch (6th Gen) - Windows 10 Pro - Intel Quad Core i7-8650U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" FHD IPS (1920x1080) Touchscreen, Fingerprint Reader, 4G LTE WWAN | Price: $2,249.99\n",
      "-----------------------------------\n",
      "32) 2018 Lenovo ThinkPad T480s Windows 10 Pro Laptop -  i7-8650U, 16GB RAM, 256GB PCIe NVMe SSD, 14\" IPS FHD (1920x1080) Touch Display w/ NVIDIA MX150 Graphics, Smart Card ReaderBlack | Price: $1,814.99\n",
      "-----------------------------------\n",
      "33) 2018 Lenovo ThinkPad T480s Windows 10 Pro Laptop -  i7-8650U, 16GB RAM, 256GB PCIe NVMe SSD, 14\" IPS FHD 1920x1080 Touch Display w/ NVIDIA MX150 Graphics, Smart Card Reader, 4G LTE WWAN, Black | Price: $1,924.99\n",
      "-----------------------------------\n",
      "34) 2017 Lenovo ThinkPad X1 Carbon 4G LTE (5th Gen) - Windows 10 Pro - Intel Core i7-7500U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" FHD IPS (1920x1080) Display, Fingerprint Reader, (Classic Black) | Price: $1,759.99\n",
      "-----------------------------------\n",
      "35) HDR Lenovo ThinkPad X1 Carbon (7th Gen) - Windows 10 Pro - Intel Quad Core i7-8665U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" HDR UHD 4K IPS (3840x2160) Non-Touch Display, Fingerprint Reader, Black | Price: $2,199.99\n",
      "-----------------------------------\n",
      "36) 2017 Lenovo ThinkPad X1 Carbon 4G LTE (5th Gen) - Windows 7 Pro - Intel Core i7-7600U, 256GB NVMe-PCIe SSD, 16GB RAM, 14\" WQHD IPS (2560x1440) Display, Fingerprint Reader, (Classic Black) | Price: $1,979.99\n",
      "-----------------------------------\n",
      "============================================================\n",
      "36 Scrappable Objects on the page. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Break Pedal: Answer any robot queries by NewEgg. Enter \"y\" when you are ready to proceed. y\n",
      "Proceeding with webscrape... \n",
      "1) | SLEEPING FOR 7 SECONDS \n",
      "2) | SLEEPING FOR 25 SECONDS \n",
      "3) | SLEEPING FOR 11 SECONDS \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fab2a5d56687>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mcontainers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_page_soup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"item-container\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mnewegg_page_scraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontainers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mturn_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mobjects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSub_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscraped_dict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-0d556ca5cd3e>\u001b[0m in \u001b[0;36mnewegg_page_scraper\u001b[1;34m(containers, turn_page)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;34m'page_number'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpage_nums\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;34m'product_links'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mproduct_links\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;34m'image_link'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     })\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[1;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m   7354\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7356\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7358\u001b[0m     \u001b[1;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   7400\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7401\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7402\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'arrays must all be same length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7404\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "scrape_again = True\n",
    "while scrape_again == True:\n",
    "    return_dt()\n",
    "    print(\"=== NewEgg.Com WebScraper Beta ===\")\n",
    "    print(\"==\"*30)\n",
    "    print(f\"Date: {current_date}\")\n",
    "    print(\"\")\n",
    "    print(\"Instructions:\")\n",
    "    print(\"(1) Go to www.newegg.com, search for your laptop requirements (e.g. brand and specifications) \")\n",
    "    print(\"(2) Copy and paste the url from your exact search \")\n",
    "    print('(3) Activate or Disable the \"Head View\", webscraper bots point of view ')\n",
    "    print('(4) Check the \"final_output folder when the webscraper bot is done scraping \"')\n",
    "    print(\"\")\n",
    "\n",
    "    executable_path = {'executable_path': './chromedriver.exe'}\n",
    "    url = input(\"Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: \")\n",
    "    head = ''\n",
    "    browser =''\n",
    "    head_on_off(executable_path)\n",
    "    response = requests.get(current_url)\n",
    "    #response\n",
    "\n",
    "    current_page_soup = soup(response.text, 'html.parser')\n",
    "    current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "    scrappable_y_n(current_page_soup)\n",
    "    ######################################\n",
    "    # # Are there any pop ups / safe to proceed?\n",
    "    safe_proceed_y_n = input(f'The Break Pedal: Answer any robot queries by NewEgg. Enter \"y\" when you are ready to proceed. ')\n",
    "    if safe_proceed_y_n == 'y':\n",
    "        print(f'Proceeding with webscrape... ')\n",
    "    else:\n",
    "        print(\"Quitting browser. You will need to press ctrl + c to quit, and then restart the program to try again. \")\n",
    "        browser.quit()\n",
    "    # ################\n",
    "\n",
    "\n",
    "    #newegg_page_scraper(containers)\n",
    "\n",
    "    # will need to UNCOMMENT AFTER\n",
    "    results_pages(current_page_soup)\n",
    "\n",
    "    #page_turner(total_results_pages)\n",
    "\n",
    "    #total_results_pages = 5\n",
    "    for turn_page in range(1, total_results_pages):\n",
    "        # set the current url as the target page (aiming the boomerang)\n",
    "        target_url = browser.url\n",
    "\n",
    "        # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "        response_target = requests.get(target_url)\n",
    "        #response\n",
    "\n",
    "        # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "        target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "        # Use BeautifulSoup to extract the total results page number\n",
    "        #results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "\n",
    "        results_pages = target_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "        #=========================================================\n",
    "        containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "\n",
    "        newegg_page_scraper(containers, turn_page)\n",
    "        screenshot(f\"./screenshots/{page_turn_page}\", suffix=\".png\", full=True)\n",
    "        objects = [Sub_category(**i) for i in scraped_dict]\n",
    "        product_catalog.append(objects)\n",
    "\n",
    "        x = random.randint(3, 25)\n",
    "        print(\"Emulating Human Behavior\")\n",
    "        print(f\"{turn_page}) | SLEEPING FOR {x} SECONDS \")\n",
    "        time.sleep(x)\n",
    "        \n",
    "        browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "\n",
    "    ##################################################################\n",
    "    concat_y_n = input(f'All {total_results_pages} pages have been saved in the \"processing\" folder (1 page = csv files). Would you like for us concatenate all the files into one? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "    if concat_y_n == 'y':\n",
    "        concatenate(total_results_pages)\n",
    "        print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{pdt_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "    # temporarily changed product_category to \"pdt_category\"\n",
    "\n",
    "\n",
    "    # clear out processing folder function here - as delete everything to prevent clutter\n",
    "    clear_processing_y_n = input(f'The \"processing\" folder has {total_results_pages} csv files of each page that was scraped. Would you like to clear the files? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "    if clear_processing_y_n == 'y':\n",
    "        clean_processing_fldr()\n",
    "\n",
    "\n",
    "    scrape_again = input(\"Would you like to scrape again? Enter in 'y' or 'n'. \")\n",
    "    if scrape_again == 'y':\n",
    "        scrape_again = True\n",
    "    scrape_again = False\n",
    "    print('Thank you! Hope you found this useful. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_catalog[1][0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-dd6f897a10d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtarget_page_soup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"src\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "target_page_soup.a.img[\"src\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
