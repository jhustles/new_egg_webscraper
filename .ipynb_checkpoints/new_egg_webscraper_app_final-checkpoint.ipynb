{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NewEgg.Com WebScraping Program For Laptops - Beta\n",
    "\n",
    "###  - April 2020\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from re import search\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions & Classes Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function to return data, which will be used throughout the program\n",
    "def return_dt():\n",
    "    global current_date\n",
    "    current_date = str(datetime.datetime.now()).replace(':','.').replace(' ','_')[:-7]\n",
    "    return current_date\n",
    "#return_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created a function to ask users if they want to watch the Bot (headless = False) work OR not (headless = True)\n",
    "# Lastly, will take you directly to the webpage that was inputted\n",
    "\"\"\"\n",
    "#head = ''\n",
    "#browser =''\n",
    "def head_on_off(executable_path):\n",
    "    # Have moved two preset variables, head and browser that are both \" = '' \"\n",
    "    # assigning these as global variables enable us to reference them outside and inside the function\n",
    "    global head\n",
    "    global browser\n",
    "    # options creates a bound to an answer\n",
    "    options = [1, 2]\n",
    "    #executable_path = {'executable_path': './chromedriver.exe'}\n",
    "    # for all cases where users input in a value that is not valid\n",
    "    while head not in options:\n",
    "        head = int(input('Do you want the desktop watch the bot work? Enter a number: 1 - YES | 2 - NO . Your Answer: '))\n",
    "        if head not in options:\n",
    "            print(\"That was not a valid answer. Please try again. \")\n",
    "    # For cases where users enter in valid options:\n",
    "    if head == options[0]:\n",
    "        print('Head is activated. Please view only the new automated Google Chrome web browser. ')\n",
    "        print('Do not make any adjustments to this automated window while the program runs, as it may produce errors or undesired outputs. Only time you should touch the automated window when the recaptcha ask you to prove you are a human. ')\n",
    "        browser = Browser('chrome', **executable_path, headless=False, incognito=True)\n",
    "    if head == options[1]:\n",
    "        print('Headless mode activated. No web browser will pop up. Please proceeed. ')\n",
    "        browser = Browser('chrome', **executable_path, headless=True)\n",
    "    # visit the target site\n",
    "    browser.visit(url)\n",
    "#     global current_url\n",
    "    current_url = browser.url\n",
    "    #print(current_url)\n",
    "    \n",
    "    return current_url\n",
    "\n",
    "#head_on_off(executable_path)\n",
    "#time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 5) Previews to the user of the scrappable items-contrainers on the page\n",
    "\n",
    "\"\"\"\n",
    "def scrappable_y_n(current_page_soup):\n",
    "    global containers\n",
    "    containers = current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "    \n",
    "    # print first and last objects so users can understand what the output will be\n",
    "    print(\"Preview: expect these scrapped off this page, and for every other total results pages, if there's more than one: \")\n",
    "    print(\"=\"*35)\n",
    "    # max items should be 36\n",
    "    counter = 0\n",
    "    for con in containers:\n",
    "        try:\n",
    "            counter += 1\n",
    "            product_details = con.find_all(\"a\", class_=\"item-title\")[0].text\n",
    "            product_price = con.find_all(\"li\", class_=\"price-current\")[0].text.split()[0]\n",
    "            print(f'{counter}) {product_details} | Price: {product_price}')\n",
    "            print(\"-\"*35)\n",
    "            \n",
    "        except (IndexError) as e:\n",
    "            print(f\"{counter}) This item was not scrappable. Skipped. \")\n",
    "            print(\"-\"*35)\n",
    "            \n",
    "    print(\"=\"*60)\n",
    "    if counter == 0:\n",
    "        print(\"Unable to scrap this link. \")\n",
    "    else:\n",
    "        print(f\"{len(containers)} Scrappable Objects on the page. \")\n",
    "    #return current_page_soup\n",
    "#scrappable_y_n(current_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOriginally modeled out parent/child inheritance object structure.\\n\\nAfter careful research, I found it much easier to export the Pandas Dataframe of the results to a dictionary, and then into a class object, which I will elaborate more down below.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Originally modeled out parent/child inheritance object structure.\n",
    "\n",
    "After careful research, I found it much easier to export the Pandas Dataframe of the results to a dictionary, and then into a class object, which I will elaborate more down below.\n",
    "\"\"\"\n",
    "# class Product_catalog:\n",
    "    \n",
    "#     all_prod_count = 0\n",
    "    \n",
    "#     def __init__(self, general_category): # computer systems\n",
    "#         self.general_category = general_category\n",
    "        \n",
    "#         Product_catalog.all_prod_count += 1\n",
    "        \n",
    "#     def count_prod(self):\n",
    "#         return int(self.all_prod_count)\n",
    "#         #return '{}'.format(self.general_category)\n",
    "        \n",
    "# class Sub_category(Product_catalog): # laptops/notebooks, gaming\n",
    "    \n",
    "#     sub_category_ct = 0\n",
    "    \n",
    "#     def __init__(self, general_category, sub_categ, item_num, brand, price, img_link, prod_link, model_specifications, current_promotions):\n",
    "#         super().__init__(general_category)\n",
    "#         Sub_category.sub_category_ct += 1\n",
    "        \n",
    "#         self.sub_categ = sub_categ\n",
    "#         self.item_num = item_num\n",
    "#         self.brand = brand\n",
    "#         self.price = price\n",
    "#         self.img_link = img_link\n",
    "#         self.prod_link = prod_link\n",
    "#         self.model_specifications = model_specifications\n",
    "#         self.current_promotions = current_promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main NewEgg WebScraper function.\n",
    "\n",
    "\"\"\"\n",
    "def newegg_page_scraper(containers, turn_page): #before: (containers, turn_page)\n",
    "    page_nums = []\n",
    "    general_category = []\n",
    "    product_categories = []\n",
    "    images = []\n",
    "    product_brands = []\n",
    "    product_models = []\n",
    "    product_links = []\n",
    "    item_numbers = []\n",
    "    promotions = []\n",
    "    prices = []\n",
    "    shipping_terms = []\n",
    "    \n",
    "    # Put this to avoid error that was being generated\n",
    "    global gen_category\n",
    "    \n",
    "    \"\"\" \n",
    "    Loop through all the containers on the HTML, and scrap the following content into the following lists\n",
    "    \n",
    "    \"\"\"\n",
    "    for con in containers:\n",
    "        try:\n",
    "            page_counter = turn_page\n",
    "            page_nums.append(int(turn_page))\n",
    "            \n",
    "            gen_category = target_page_soup.find_all('div', class_=\"nav-x-body-top-bar fix\")[0].text.split('\\n')[5]\n",
    "            general_category.append(gen_category)\n",
    "            \n",
    "            prod_category = target_page_soup.find_all('h1', class_=\"page-title-text\")[0].text\n",
    "            product_categories.append(prod_category)\n",
    "            \n",
    "            image = con.a.img[\"src\"]\n",
    "            #print(image)\n",
    "            images.append(image)\n",
    "\n",
    "            prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "            product_models.append(prd_title)\n",
    "\n",
    "            product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "            product_links.append(product_link)\n",
    "            \n",
    "            shipping = con.find_all('li', class_='price-ship')[0].text.strip().split()[0]\n",
    "            if shipping != \"Free\":\n",
    "                shipping = shipping.replace('$', '')\n",
    "                shipping_terms.append(shipping)\n",
    "            else:\n",
    "                shipping = 0.00\n",
    "                shipping_terms.append(shipping)\n",
    "\n",
    "            brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "            product_brands.append(brand_name)\n",
    "\n",
    "        except (IndexError, ValueError) as e:\n",
    "            # if there's no item_brand container, take the Brand from product details\n",
    "            product_brands.append(con.find_all('a', class_=\"item-title\")[0].text.split()[0])\n",
    "            #print(f\"{e} block 1\")\n",
    "\n",
    "        try:\n",
    "            current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "            promotions.append(current_promo)\n",
    "        except:\n",
    "            promotions.append('null')\n",
    "            #print(f\"{e} block 2\")\n",
    "        try:\n",
    "            price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "            prices.append(price)\n",
    "        except:\n",
    "            price = 'null / out of stock'\n",
    "            prices.append(price)\n",
    "            #print(f\"{e} block 3\")\n",
    "        \n",
    "        try:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "            item_numbers.append(item_num)\n",
    "        except (IndexError) as e:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]\n",
    "            item_numbers.append(item_num)    \n",
    "    \n",
    "    # Convert all of the lists into a dataframe\n",
    "    df = pd.DataFrame({\n",
    "    'item_number': item_numbers,\n",
    "    'general_category': general_category,\n",
    "    'product_category': product_categories,\n",
    "    'brand': product_brands,\n",
    "    'model_specifications': product_models,\n",
    "    'price': prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': shipping_terms,\n",
    "    'page_number': page_nums,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "    \n",
    "    # Rearrange the dataframe columns into the following order\n",
    "    df = df[['item_number', 'general_category','product_category', 'page_number' ,'brand','model_specifications' ,'current_promotions' ,'price' ,'shipping' ,'product_links','image_link']]\n",
    "    \n",
    "    # Convert the dataframe into a dictionary\n",
    "    global scraped_dict\n",
    "    scraped_dict = df.to_dict('records')\n",
    "    \n",
    "    # Grab the subcategory \"Laptop/Notebooks\" and eliminate any special characters that may cause errors\n",
    "    global pdt_category\n",
    "    pdt_category = df['product_category'].unique()[0]\n",
    "    # eliminate special characters in a string if it exists\n",
    "    pdt_category = ''.join(e for e in pdt_category if e.isalnum())\n",
    "    \n",
    "    \"\"\" Count the number of items scraped by getting the length of a all the models for sale.\n",
    "        This parameter is always available for each item-container in the HTML\n",
    "    \"\"\"\n",
    "    global items_scraped\n",
    "    items_scraped = len(df['model_specifications'])\n",
    "    \n",
    "    \"\"\"\n",
    "    Save the results into a csv file using Pandas\n",
    "    \"\"\"\n",
    "    df.to_csv(f'./processing/{current_date}_{pdt_category}_{items_scraped}_scraped_page{turn_page}.csv')\n",
    "    \n",
    "    # Return these variables as they will be used.\n",
    "    return scraped_dict, items_scraped, pdt_category\n",
    "    \n",
    "#df.head()\n",
    "#newegg_page_scraper(containers, turn_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to return the total results pages, if exists, otherwise just scrape one page\n",
    "\n",
    "def results_pages(current_page_soup):\n",
    "    # Use BeautifulSoup to extract the total results page number\n",
    "    results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "    #print(results_pages)\n",
    "    # Find and extract total pages + and add 1 to ensure proper length of total pages\n",
    "    global total_results_pages\n",
    "    total_results_pages = int(re.split(\"/\", results_pages)[1]) + 1 # need to add 1 b/c 'range(inclusive, exclusive)'\n",
    "    #========================================= need to remember to +2, and remove -30\n",
    "    #print(total_results_pages)\n",
    "    \n",
    "    return total_results_pages\n",
    "#results_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build a function to concatenate all pages that were scraped and saved in the processing folder.\n",
    "Save the final output (1 csv file) all the results\n",
    "    \n",
    "\"\"\"\n",
    "def concatenate(total_results_pages):\n",
    "    \n",
    "    path = f'./processing\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    concatenate_pages = []\n",
    "    counter = 0\n",
    "    for page in scraped_pages:\n",
    "        df = pd.read_csv(page, index_col=0, header=0)\n",
    "        concatenate_pages.append(df)\n",
    "\n",
    "    compiled_data = pd.concat(concatenate_pages, axis=0, ignore_index=True)\n",
    "    total_items_scraped = len(compiled_data['brand']) # can replace this counter by creating class objects everytime it scrapes\n",
    "    concatenated_output = compiled_data.to_csv(f\"./finished_outputs/{current_date}_{total_items_scraped}_scraped_{total_results_pages}_pages_.csv\")\n",
    "    return concatenated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Built a function to clear out the entire processing files folder to avoid clutter.\n",
    "Or the user can keep the processing files (page by page) for their own analysis.\n",
    "\n",
    "\"\"\"\n",
    "def clean_processing_fldr():\n",
    "    # delete all files in the 'processing folder'\n",
    "    path = f'./processing\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    if len(scraped_pages) < 1:\n",
    "        print(\"There are no files in the folder to clear. \")\n",
    "    else:\n",
    "        print(f\"Clearing out a total of {len(scraped_pages)} scraped pages in the processing folder... \")\n",
    "        clear_processing_files = []\n",
    "        for page in scraped_pages:\n",
    "            os.remove(page)\n",
    "        \n",
    "    print('Clearing of \"Processing\" folder complete. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdm_slp_4_20():\n",
    "    x = random.randint(4, 20)\n",
    "    time.sleep(x)\n",
    "    print(f\"Slept for {x} seconds. \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This class takes in the dictionary from the webscraper function, and will be used in a list comprehension to produce class \"objects\" in the \"product_catalog\"\n",
    "\n",
    "\"\"\"\n",
    "class Laptops:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program Logic\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NewEgg.Com WebScraper Beta ===\n",
      "============================================================\n",
      "Scope: This project is a beta and is only built to scrape the laptop section of NewEgg.com due to limited time. \n",
      "\n",
      "Instructions:\n",
      "\n",
      "Current Date And Time: 2020-04-16_21.38.17\n",
      "\n",
      "(1) Go to www.newegg.com, go to the laptop section, select your requirements (e.g. brand, screensize, and specifications - SSD size, processor brand and etc...) \n",
      "(2) Copy and paste the url from your exact search when prompted \n",
      "(3) Activate or Disable the \"Head View\", webscraper bots point of view \n",
      "(4) After the webscraping is successful, you will have an option to concatenate all of the pages you scraped together into one csv file\n",
      "(5) Lastly, you will have an option to clear out the processing folder (data scraped by each page)\n",
      "(6) If you have any issues or errors, \"PRESS CTRL + C\" to quit the program in the terminal \n",
      "(7) Disclaimer: Newegg may ban you for a 24 - 48 hours for webscraping their data, then you may resume \n",
      "\n",
      "Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: https://www.newegg.com/p/pl?N=100167749%20600136700\n",
      "Do you want the desktop watch the bot work? Enter a number: 1 - YES | 2 - NO . Your Answer: 1\n",
      "Head is activated. Please view only the new automated Google Chrome web browser. \n",
      "Do not make any adjustments to this automated window while the program runs, as it may produce errors or undesired outputs. Only time you should touch the automated window when the recaptcha ask you to prove you are a human. \n",
      "The Break Pedal: Answer any robot queries by NewEgg. Enter \"y\" when you are ready to proceed. y\n",
      "Proceeding with webscrape... \n",
      "Preview: expect these scrapped off this page, and for every other total results pages, if there's more than one: \n",
      "===================================\n",
      "1) SAMSUNG Notebook 9 Pen NP950SBE-X01US Intel Core i7 8th Gen 8565U (1.80 GHz) 16 GB Memory 512 GB SSD NVIDIA GeForce MX150 15.0\" Convertible 2-in-1 Laptop Windows 10 Home 64-bit | Price: $1,799.99\n",
      "-----------------------------------\n",
      "2) ASUS ZenBook Flip 14 Ultra Slim Convertible Laptop 14\" Full HD WideView Touch, 8th-Gen Intel Core i7-8565U Processor, 16 GB LPDDR3, 512 GB NVMe PCIe SSD, GeForce MX150, Windows 10 - UX461FN-DH74T | Price: $1,149.99\n",
      "-----------------------------------\n",
      "3) Microsoft Surface Pro 7 - 12.3\" Touch-Screen - Intel Core i5 - 16 GB Memory - 256 GB Solid State Drive (Latest Model) - Platinum | Price: $1,229.00\n",
      "-----------------------------------\n",
      "4) Microsoft Surface Pro 7 - 12.3\" Touch-Screen - Intel Core i7 - 16 GB Memory - 256 GB Solid State Drive (Latest Model) - Matte Black | Price: $1,299.00\n",
      "-----------------------------------\n",
      "5) Microsoft Surface Pro 7 - 12.3\" Touch-Screen - Intel Core i7 - 16 GB Memory - 512 GB Solid State Drive (Latest Model) - Matte Black | Price: $1,699.00\n",
      "-----------------------------------\n",
      "6) Microsoft Surface Pro 4 TH4-00001 Intel Core i7 6th Gen 6650U (2.20 GHz) 16 GB Memory 512 GB SSD 12.3\" Touchscreen 2736 x 1824 Tablet Windows 10 Pro 64-Bit | Price: $1,029.99\n",
      "-----------------------------------\n",
      "7) Microsoft Surface Pro 7 - 12.3\" Touch-Screen - Intel Core i7 - 16 GB Memory - 1 TB Solid State Drive (Latest Model) - Platinum | Price: $2,079.99\n",
      "-----------------------------------\n",
      "8) Microsoft Surface Pro 4 TH2-00001 Intel Core i7 6th Gen 6650U (2.20 GHz) 16 GB Memory 256 GB SSD 12.3\" Touchscreen 2736 x 1824 Tablet Windows 10 Pro 64-Bit | Price: $779.00\n",
      "-----------------------------------\n",
      "9) Microsoft Surface Pro 7 - 12.3\" Touch-Screen - Intel Core i7 - 16 GB Memory - 512 GB Solid State Drive (Latest Model) - Platinum | Price: $1,699.00\n",
      "-----------------------------------\n",
      "10) Microsoft Surface Book CR7-00001 Intel Core i7 6th Gen 6600U (2.60 GHz) 16 GB Memory 512 GB SSD NVIDIA GeForce graphics 13.5\" Touchscreen 3000 x 2000 2-in-1 Laptop Windows 10 Pro 64-Bit | Price: $1,399.99\n",
      "-----------------------------------\n",
      "11) Dell Inspiron 2-in-1 5481 TouchScreen Notebook, 14\" HD Display, Intel Core i3-8145U Upto 3.90GHz, 16GB RAM, 512GB SSD, Wi-Fi, Bluetooth, Card Reader, HDMI, Windows 10 Pro | Price: $569.99\n",
      "-----------------------------------\n",
      "12) Lenovo Yoga710 2-in-1 15.6 Convertible Touchscreen Full HD (1920 x 1080) Laptop,Intel Core i5-7200U ,16GB RAM,256GB SSD,Backlit Keyboard,HDMI, Bluetooth,Fingerprint Reader,Windows 10 Aluminum Chassis | Price: $739.00\n",
      "-----------------------------------\n",
      "13) Lenovo IdeaPad Flex 5 15 15.6” Full HD FHD (1920x1080) IPS Touchscreen 2-in-1 Laptop (Intel Quad-Core i7-8565U, 16GB DDR4 RAM, 512GB PCIE M.2 SSD, NVIDIA MX230), Fingerprint, Backlit, Windows 10 Home | Price: $899.99\n",
      "-----------------------------------\n",
      "14) Microsoft Surface Pro 2017 Edition FKH-00001 Intel Core i7 7th Gen 16 GB Memory 512 GB SSD 12.3\" Touchscreen 2736 x 1824 Tablet Windows 10 Pro 64-Bit | Price: $1,399.95\n",
      "-----------------------------------\n",
      "15) Lenovo IdeaPad Flex 5 15 15.6” Full HD FHD (1920x1080) IPS Touchscreen 2-in-1 Laptop (Intel Quad-Core i7-8565U, 16GB DDR4 RAM, 512GB PCIE SSD+1TB HDD, NVIDIA MX230), Fingerprint, Backlit, Windows 10 | Price: $949.99\n",
      "-----------------------------------\n",
      "16) Lenovo IdeaPad Flex 5 15 15.6” Full HD FHD (1920x1080) IPS Touchscreen 2-in-1 Laptop (Intel Quad-Core i7-8565U, 16GB DDR4 RAM, 1TB SATA M.2 SSD, NVIDIA MX230), Fingerprint, Backlit, Windows 10 Home | Price: $949.99\n",
      "-----------------------------------\n",
      "17) Microsoft Surface Pro 2017 Edition FKK-00001 Intel Core i7 7th Gen 16 GB Memory 1 TB SSD 12.3\" Touchscreen 2736 x 1824 Tablet Windows 10 Pro 64-Bit | Price: $1,399.99\n",
      "-----------------------------------\n",
      "18) Dell Inspiron 2-in-1 5481 TouchScreen Notebook, 14\" HD Display, Intel Core i3-8145U Upto 3.90GHz, 16GB RAM, 256GB SSD, Wi-Fi, Bluetooth, Card Reader, HDMI, Windows 10 Pro | Price: $539.99\n",
      "-----------------------------------\n",
      "19) HP Pavilion X360 14\" FHD Edge-to-Edge Glass Touchscreen 2-in-1 Laptop, 8th Gen Intel Core i5-8250U,16GB DDR4 RAM, 1TB SSD,Intel® UHD Graphics 620,Wifi-AC,BlueTooth 4.2,B&O Audio,Windows 10 Pro | Price: $799.00\n",
      "-----------------------------------\n",
      "20) Microsoft Surface Book 2 FVH-00001 Intel Core i7 8th Gen 8650U (1.90 GHz) 16 GB Memory 1 TB PCIe SSD NVIDIA GeForce GTX 1060 15.0\" Touchscreen 3240 x 2160 Detachable 2-in-1 Laptop Windows 10 Pro | Price: $2,999.00\n",
      "-----------------------------------\n",
      "21) ASUS VivoBook Filp 14 2-in-1 Touchscreen Laptop, 14\" FHD NanoEdge, Intel Core i5 Quad-core up to 3.40 GHz, 16GB RAM, 512GB SSD, Backlit, USB-C, FP Reader, WebCam, Wi-Fi 5, BT, Win 10 | Price: $852.49\n",
      "-----------------------------------\n",
      "22) ASUS VivoBook Filp 14 2-in-1 Touchscreen Laptop, 14\" FHD NanoEdge, Intel Core i5 Quad-core up to 3.40 GHz, 16GB RAM, 1TB SSD, Backlit, USB-C, FP Reader, WebCam, Wi-Fi 5, BT, Win 10 | Price: $899.79\n",
      "-----------------------------------\n",
      "23) Microsoft Surface Pro i7 16GB 512GB device + Microsoft Signature Type Cover Platinum bundle | Price: $1,599.95\n",
      "-----------------------------------\n",
      "24) HP Pavilion X360 14\" FHD Edge-to-Edge Glass Touchscreen 2-in-1 Laptop, 8th Gen Intel Core i5-8250U,16GB DDR4 RAM, 256GB SSD,Intel® UHD Graphics 620,Wifi-AC,BlueTooth 4.2,B&O Audio,Windows 10 Pro | Price: $699.00\n",
      "-----------------------------------\n",
      "25) Microsoft Surface Book 2 FUX-00001 Intel Core i7 8th Gen 8650U (1.90 GHz) 16 GB Memory 512 GB PCIe SSD NVIDIA GeForce GTX 1060 15.0\" Touchscreen 3240 x 2160 Detachable 2-in-1 Laptop Windows 10 Pro | Price: $2,599.00\n",
      "-----------------------------------\n",
      "26) Dell Inspiron 7000 2-in-1 17.3\" Powerful Touch-Screen  IPS FHD Laptop (1920 x 1080), 8th Intel Core i7-8565U, 16GB DDR4,  16GB Intel Optane SSD memory + 1TB HDD, NVIDIA GeForce MX150, Windows 10 Home | Price: $1,999.99\n",
      "-----------------------------------\n",
      "27) ASUS VivoBook Filp 14 2-in-1 Touchscreen Laptop, 14\" FHD NanoEdge, Intel Core i5 Quad-core up to 3.40 GHz, 16GB RAM, 128GB SSD, Backlit, USB-C, FP Reader, WebCam, Wi-Fi 5, BT, Win 10 | Price: $732.32\n",
      "-----------------------------------\n",
      "28) Lenovo Flex 5 2-in-1, 14\" FHD Touch Display, Intel Core i7-8550U Upto 4.0GHz, 16GB RAM, 2TB NVMe SSD + 1TB HDD, HDMI, Card Reader, Wi-Fi, Bluetooth, Windows 10 Pro | Price: $1,501.49\n",
      "-----------------------------------\n",
      "29) Microsoft Surface Book 2 HNR-00001 Intel Core i7 8th Gen 8650U (1.90 GHz) 16 GB Memory 256 GB PCIe SSD NVIDIA GeForce GTX 1060 15.0\" Touchscreen 3240 x 2160 Detachable 2-in-1 Laptop Windows 10 Pro | Price: $2,219.00\n",
      "-----------------------------------\n",
      "30) Microsoft Surface Pro 6 KJV-00016 Intel Core i7 8th Gen 8650U (1.90 GHz) 16 GB Memory 512 GB SSD Intel UHD Graphics 620 12.3\" Touchscreen 2736 x 1824 Detachable 2-in-1 Laptop Windows 10 Home 64-Bit | Price: $1,399.00\n",
      "-----------------------------------\n",
      "31) Microsoft Surface Pro 6 KJV-00001 Intel Core i7 8th Gen 8650U (1.90 GHz) 16 GB Memory 512 GB SSD Intel UHD Graphics 620 12.3\" Touchscreen 2736 x 1824 Detachable 2-in-1 Laptop Windows 10 Home 64-bit | Price: $1,199.99\n",
      "-----------------------------------\n",
      "32) 2019 Lenovo Yoga 730 2-in-1 15.6\" FHD IPS Touchscreen Thin & Lightweight Laptop, Intel Quad Core i5-8265U upto 3.9GHz, 16GB RAM, 512GB SSD, Backlit Keyboard, Fingerprint Reader, Windows 10, Blue | Price: $895.99\n",
      "-----------------------------------\n",
      "33) Microsoft Surface Pro 6 KJW-00001 Intel Core i7 8th Gen 8650U (1.90 GHz) 16 GB Memory 1 TB SSD Intel UHD Graphics 620 12.3\" Touchscreen 2736 x 1824 Detachable 2-in-1 Laptop Windows 10 Home 64-Bit | Price: $1,632.81\n",
      "-----------------------------------\n",
      "34) 2019 HP Pavilion x360 2-in-1 Thin and Light 14\" FHD IPS Touchscreen Laptop, Intel Quad Core i5-8250U upto 3.4GHz, 16GB DDR4 RAM, 512GB SSD, Fingerprint Reader, Backlit Keyboard, Windows 10, Gold | Price: $892.99\n",
      "-----------------------------------\n",
      "35) Microsoft Surface Pro FKJ-00001 Intel Core i7 7th Gen 7660U (2.50 GHz) 16 GB Memory 512 GB SSD Intel Iris Plus Graphics 640 12.3\" Touchscreen 2736 x 1824 Detachable 2-in-1 Laptop Windows 10 Pro 64-bit | Price: $1,499.95\n",
      "-----------------------------------\n",
      "36) 2019 HP Pavilion x360 2-in-1 Thin and Light 14\" FHD IPS Touchscreen Laptop, Intel Quad Core i5-8250U upto 3.4GHz, 16GB DDR4 RAM, 256GB SSD, Fingerprint Reader, Backlit Keyboard, Windows 10, Gold | Price: $854.99\n",
      "-----------------------------------\n",
      "============================================================\n",
      "36 Scrappable Objects on the page. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slept for 4 seconds. \n",
      "1) | SLEEPING FOR 22 SECONDS \n",
      "Slept for 11 seconds. \n"
     ]
    },
    {
     "ename": "ElementNotInteractableException",
     "evalue": "Message: element not interactable\n  (Session info: chrome=80.0.3987.163)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementNotInteractableException\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1def3a34f911>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mninety_pct_a_tag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py\u001b[0m in \u001b[0;36mclick\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mclick\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;34m\"\"\"Clicks the element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLICK_ELEMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mElementNotInteractableException\u001b[0m: Message: element not interactable\n  (Session info: chrome=80.0.3987.163)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mElementNotInteractableException\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1def3a34f911>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mninety_pct_a_tag\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mrdm_slp_4_20\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py\u001b[0m in \u001b[0;36mclick\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mclick\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;34m\"\"\"Clicks the element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLICK_ELEMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mElementNotInteractableException\u001b[0m: Message: element not interactable\n  (Session info: chrome=80.0.3987.163)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Welcome to the program message!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== NewEgg.Com WebScraper Beta ===\")\n",
    "print(\"==\"*30)\n",
    "print('Scope: This project is a beta and is only built to scrape the laptop section of NewEgg.com due to limited time. ')\n",
    "print(\"\")\n",
    "print(\"Instructions:\")\n",
    "print(\"\")\n",
    "return_dt()\n",
    "print(f'Current Date And Time: {current_date}')\n",
    "print(\"\")\n",
    "print(\"(1) Go to www.newegg.com, go to the laptop section, select your requirements (e.g. brand, screensize, and specifications - SSD size, processor brand and etc...) \")\n",
    "print(\"(2) Copy and paste the url from your exact search when prompted \")\n",
    "print('(3) Activate or Disable the \"Head View\", webscraper bots point of view ')\n",
    "print('(4) After the webscraping is successful, you will have an option to concatenate all of the pages you scraped together into one csv file')\n",
    "print('(5) Lastly, you will have an option to clear out the processing folder (data scraped by each page)')\n",
    "print('(6) If you have any issues or errors, \"PRESS CTRL + C\" to quit the program in the terminal ')\n",
    "print('(7) Disclaimer: Newegg may ban you for a 24 - 48 hours for webscraping their data, then you may resume ')\n",
    "print(\"\")\n",
    "\n",
    "# Set up Splinter requirements\n",
    "executable_path = {'executable_path': './chromedriver.exe'}\n",
    "\n",
    "# Ask user to input in the laptop query link they would like to scrape\n",
    "url = input(\"Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: \")\n",
    "head = ''\n",
    "browser =''\n",
    "head_on_off(executable_path)\n",
    "# Sometimes it will ask if the user is a bot. After the user answers the questions, then they may enter \"y\" to proceed\n",
    "safe_proceed_y_n = input(f'The Break Pedal: Answer any robot queries by NewEgg. Enter \"y\" when you are ready to proceed. ')\n",
    "if safe_proceed_y_n == 'y':\n",
    "    print(f'Proceeding with webscrape... ')\n",
    "    current_url = browser.url\n",
    "else:\n",
    "    print(\"Quitting browser. You will need to press CTRL + C to QUIT the program, and then restart it to try scraping the link again. \")\n",
    "    browser.quit()\n",
    "\n",
    "# head_on_off will finish with browse.visit(url) - and returns \"current_url\" as an output that passed into response\n",
    "response = requests.get(current_url)\n",
    "current_page_soup = soup(response.text, 'html.parser')\n",
    "\n",
    "# might need to delete - unnecessary\n",
    "##current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "\n",
    "# Preview some of the data that will be scraped\n",
    "scrappable_y_n(current_page_soup)\n",
    "\n",
    "# Run the results_pages function to gather the total pages to be scraped\n",
    "results_pages(current_page_soup)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "product_catalog = []\n",
    "# This is the loop that sets up the page by page scraping of data / results of the user's query\n",
    "for turn_page in range(1, total_results_pages):\n",
    "    \n",
    "    # check if \n",
    "    \n",
    "    # Goal here is to emulate human behavior by clicking on the screen\n",
    "    browser.find_by_tag('h1')[0].click()\n",
    "    # want to mimic human behavior by clicking on at least one link and fiewing for 6 seconds\n",
    "    number_of_a_tags = len(browser.find_by_tag('a'))\n",
    "    ninety_pct_a_tag = int(round((number_of_a_tags * .905)))\n",
    "    ninety_98th_a_tag = int(round((number_of_a_tags * .98)))\n",
    "    \n",
    "    try:\n",
    "        rdm_slp_4_20()\n",
    "        browser.find_by_tag('h1')[0].click()\n",
    "        # create a random generator loop for multiple mouse overs on the screen\n",
    "            \n",
    "        browser.find_by_tag('a')[ninety_pct_a_tag].click()\n",
    "        browser.back()\n",
    "    except:\n",
    "        browser.find_by_tag('h1')[0].click()\n",
    "        browser.find_by_tag('a')[ninety_pct_a_tag + 1].click()\n",
    "        rdm_slp_4_20()\n",
    "        browser.back()\n",
    "\n",
    "# Might need to delete these below:\n",
    "#     # set the current url as the target page (aiming the boomerang)\n",
    "#     target_url = browser.url\n",
    "\n",
    "#     # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "#     response_target = requests.get(target_url)\n",
    "#     #response\n",
    "\n",
    "#     # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "#     target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "##############################\n",
    "\n",
    "    # if \"reCAPTCHA\", pauses the program, and allows the user to continue to scrape after they're done completing the quizz - move to beginning of the loop and also include an OR\n",
    "    \n",
    "    # package up the \n",
    "    if browser.is_element_present_by_id('g-recaptcha') == True:\n",
    "        continue_scraper = input(\"Noticed Newegg asked if you were robot. When you're done proving you are not a robot and completing all picture tests, enter in any key and press ENTER to continue the webscrape. \")\n",
    "        \n",
    "        # target the new page's url for scraping\n",
    "        target_url = browser.url\n",
    "\n",
    "        # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "        response_target = requests.get(target_url)\n",
    "        #response\n",
    "\n",
    "        # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "        target_page_soup = soup(response_target.text, 'html.parser')\n",
    "        containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "        newegg_page_scraper(containers, turn_page)\n",
    "    \n",
    "        # After scraping a page of data, create instances of objects of the laptops/notebooks using a list comprehension\n",
    "        objects = [Laptops(**prod_obj) for prod_obj in scraped_dict]\n",
    "\n",
    "        # Append all of the objects off the page to the main product_catalog list (List of List of Objects by page scraped)\n",
    "        product_catalog.append(objects)\n",
    "        # Click to the next page\n",
    "        print(\"Will scrape pages, but will need to randomly sleep for max 35 seconds to emulate human behavior. \")\n",
    "        try:\n",
    "            x = random.randint(8, 28)\n",
    "            print(f\"{turn_page}) | SLEEPING FOR {x} SECONDS \")\n",
    "            time.sleep(x)\n",
    "            browser.find_by_tag('h1')[0].click()\n",
    "            time.sleep(3)\n",
    "            browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "\n",
    "        except:\n",
    "            x = random.randint(7, 25)\n",
    "            print(f\"{turn_page}) | SLEEPING FOR {x} SECONDS - EXCEPTION\")\n",
    "            time.sleep(x)\n",
    "            browser.find_by_tag('h1')[0].click()\n",
    "            browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[4]/div/div/div[10]/button').click()\n",
    "            time.sleep(1)\n",
    "        \n",
    "        browser.quit()\n",
    "        \n",
    "#######################################        \n",
    "        \n",
    "    else:\n",
    "        time.sleep(3)\n",
    "        # set the current url as the target page (aiming the boomerang)\n",
    "        target_url = browser.url\n",
    "\n",
    "        # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "        response_target = requests.get(target_url)\n",
    "        #response\n",
    "\n",
    "        # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "        target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "\n",
    "        # Use BeautifulSoup to extract the total results page number\n",
    "        #results_pages = target_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "\n",
    "        # Run the webscraper function by passing in containers and turn_page\n",
    "        containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "        newegg_page_scraper(containers, turn_page)\n",
    "\n",
    "        # After scraping a page of data, create instances of objects of the laptops/notebooks using a list comprehension\n",
    "        objects = [Sub_category(**i) for i in scraped_dict]\n",
    "        \n",
    "        # Append all of the objects off the page to the main product_catalog list (List of List of Objects by page scraped)\n",
    "        product_catalog.append(objects)\n",
    "\n",
    "        # Have the scraper randomly sleep for min of 3 - 19 seconds, then flip the page.\n",
    "\n",
    "        # Click to the next page\n",
    "        try:\n",
    "            x = random.randint(8, 27)\n",
    "            print(f\"{turn_page}) | SLEEPING FOR {x} SECONDS \")\n",
    "            time.sleep(x)\n",
    "            browser.find_by_tag('h1')[0].click()\n",
    "            #browser.find_by_tag('body')[0].click()\n",
    "            time.sleep(4)\n",
    "            browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "\n",
    "        except:\n",
    "            x = random.randint(7, 35)\n",
    "            print(f\"{turn_page}) | SLEEPING FOR {x} SECONDS - EXCEPTION\")\n",
    "            time.sleep(x)\n",
    "            browser.find_by_tag('h1')[0].click()\n",
    "            time.sleep(4)\n",
    "            browser.find_by_tag('h1')[0].click()\n",
    "            time.sleep(1)\n",
    "            browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[4]/div/div/div[10]/button').click()\n",
    "\n",
    "\n",
    "# Exit the broswer once complete webscraping\n",
    "browser.quit()\n",
    "\n",
    "# Prompt the user if they would like to concatenate all of the pages into one csv file\n",
    "concat_y_n = input(f'All {total_results_pages} pages have been saved in the \"processing\" folder (1 page = csv files). Would you like for us concatenate all the files into one? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "\n",
    "if concat_y_n == 'y':\n",
    "    concatenate(total_results_pages)\n",
    "    print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{pdt_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "\n",
    "# Prompt the user to if they would like to clear out processing folder function here - as delete everything to prevent clutter\n",
    "clear_processing_y_n = input(f'The \"processing\" folder has {total_results_pages} csv files of each page that was scraped. Would you like to clear the files? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "if clear_processing_y_n == 'y':\n",
    "    clean_processing_fldr()\n",
    "\n",
    "print('Thank you checking out my project, and hope you found this useful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/html/body/div[5]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_element': <selenium.webdriver.remote.webelement.WebElement (session=\"92cc9c9233e821dc38ef74d6f05ca03d\", element=\"72ac5fdf-5076-4495-b827-c66e93c44d33\")>, 'parent': <splinter.driver.webdriver.chrome.WebDriver object at 0x000001F3B63C64E0>}\n"
     ]
    }
   ],
   "source": [
    "print(browser.find_by_tag('a')[ninety_pct_a_tag].__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'WebDriverElement' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-63d4ed14ade1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mninety_pct_a_tag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'WebDriverElement' has no len()"
     ]
    }
   ],
   "source": [
    "len(browser.find_by_tag('a')[ninety_pct_a_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimipopupsweepstakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser.find_by_tag('button')#[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(current_page_soup.find_all(\"div\", class_=\"item-container\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id=\"rc-imageselect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser.is_element_present_by_id(\"btn_InnerSearch\")\n",
    "#browser.is_element_present_by_id(\"rc-imageselect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(browser.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "browser.find_by_tag('body')[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(browser.find_by_tag('body'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag('a')[1958].click()#['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_a_tags = len(browser.find_by_tag('a'))\n",
    "ninety_pct_a_tag = round((number_of_a_tags * .902))\n",
    "ninety_pct_a_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag('a')[1962].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag('a')[1970].mouse_over()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag('h1')[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*[@id=\"btn_InnerSearch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.is_element_present_by_id('g-recaptcha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.is_element_present_by_id('mimipopupsweepstakes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag('a')[2263].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.is_element_present_by_id('sweepstakespopup')\n",
    "/html/body/div[16]/div/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.is_element_present_by_id('sweepstakespopup_close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_id('sweepstakespopup_close').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[4]/div/div/div[10]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_page_soup.is_element_present_by_xpath(\"/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_element_present_by_xpath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to view objects that were created from the data we collected\n",
    "#product_catalog[1][0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/html/body/div[4]/section/div/div/div[2]/div/div/div[2]/div[2]/div[1]/div[2]/div[4]/div/div/div[11]/button/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "prices = []\n",
    "for con in containers:\n",
    "    counter += 1\n",
    "    try:\n",
    "        #print(str(counter) + \" | \" + con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', ''))\n",
    "        price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "        prices.append(price)\n",
    "    except:\n",
    "        #print(str(counter) + \" | \" + \"null\")\n",
    "        \n",
    "        price = 'null'\n",
    "        prices.append(price)\n",
    "        \n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
