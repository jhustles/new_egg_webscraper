{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping NewEgg.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "#import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "#from selenium import webdriver\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "# conn = 'mongodb://localhost:27017'\n",
    "# client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database\n",
    "# db = client.newegg_laptops_db\n",
    "# # Define collection\n",
    "# collection = db.apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Splinter and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Splinter inputs\n",
    "\n",
    "executable_path = {'executable_path': './chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up url that you want to scrape\n",
    "#url = 'https://www.newegg.com/p/pl?N=100006740&page=1&order=RELEASE'\n",
    "url = 'https://www.newegg.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splinter uses browser to go directly to the url\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Input\n",
    "\n",
    "#query_input = input(\"What products would you like to scrape?\")\n",
    "query_input = \"Laptops\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the search bar, and type in \"laptops\"\n",
    "search_box = browser.find_by_id(\"SearchBox2020\").type(query_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the Search button!\n",
    "\n",
    "submit = browser.find_by_css(\".fa-search\").first.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on the first element which is laptops / notebooks on the side menu far target by class\n",
    "# Optional - can expand this out later for other options\n",
    "\n",
    "browser.find_by_css(\".filter-box-label\").first.click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on \"Sold By: Newegg\" - later add options for each type (later code by xpath for each option)\n",
    "\n",
    "browser.find_by_css(\".form-radiobox-title\").first.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newegg_page_scraper(containers, turn_page):\n",
    "    \n",
    "    images = []\n",
    "    product_brands = []\n",
    "    product_models = []\n",
    "    product_links = []\n",
    "    item_numbers = []\n",
    "    promotions = []\n",
    "    prices = []\n",
    "    shipping_terms = []\n",
    "    \n",
    "    \n",
    "    for con in containers:\n",
    "        page_counter = turn_page\n",
    "        \n",
    "        \n",
    "        image = con.a.img[\"src\"]\n",
    "        images.append(image)\n",
    "        \n",
    "        brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "        product_brands.append(brand_name)\n",
    "        #print(brand_name)\n",
    "\n",
    "        prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "        product_models.append(prd_title)\n",
    "        \n",
    "        product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "        product_links.append(product_link)\n",
    "        \n",
    "        item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('?')[1]\n",
    "        item_numbers.append(item_num)\n",
    "        \n",
    "        current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "        promotions.append(current_promo)\n",
    "\n",
    "        price = con.find_all('li', class_=\"price-current\")[0].text.split()[0]\n",
    "        prices.append(price)\n",
    "\n",
    "        shipping = con.find_all('li', class_='price-ship')[0].text.strip()\n",
    "        shipping_terms.append(shipping)     \n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "    'item_number': item_numbers,\n",
    "    'brand': product_brands,\n",
    "    'model_listing': product_models,\n",
    "    'price': prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': shipping_terms,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "\n",
    "    return df.to_csv(f'./processing/{current_date}_{query_input}_{page_counter}_scraped.csv')\n",
    "    \n",
    "#newegg_page_scraper(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(total_results_pages):\n",
    "    \n",
    "    path = f'./output\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    concatenate_pages = []\n",
    "\n",
    "    for page in scraped_pages:\n",
    "        df = pd.read_csv(page, index_col=0, header=0)\n",
    "        concatenate_pages.append(df)\n",
    "\n",
    "    compiled_data = pd.concat(concatenate_pages, axis=0, ignore_index=True)\n",
    "    compiled_data.to_csv(f\"./finished_outputs/{current_date}_{query_input}_scraped_{total_results_pages}_pages_.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Egg WebScraper Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1e3ef5340075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_results_pages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{query_input}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-82e399e88725>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(total_results_pages)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mconcatenate_pages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mcompiled_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcatenate_pages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mcompiled_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"./finished_outputs/{current_date}_{query_input}_scraped_{total_results_pages}_pages_.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No objects to concatenate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Extract total page numbers from results\n",
    "\n",
    "# Setup current date - for later use\n",
    "current_date = str(datetime.datetime.now()).replace(':','.').replace(' ','_')[:-7]\n",
    "\n",
    "# Use Splinter to grab the current url, to setup request to pull URL\n",
    "current_url = browser.url\n",
    "\n",
    "# Use Request.get() to pull the current url\n",
    "response = requests.get(current_url)\n",
    "#response\n",
    "\n",
    "# Use BeautifulSoup to grab all the HTML using the lxml parser\n",
    "current_page_soup = soup(response.text, 'html.parser')\n",
    "\n",
    "# Use BeautifulSoup to extract the total results page number\n",
    "results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "# Find and extract total pages + and add 1 to ensure proper length of total pages\n",
    "total_results_pages = int(re.split(\"/\", results_pages)[1]) - 33 #+ 2 # need to add 2 b/c 'range(inclusive, exclusive)'\n",
    "\n",
    "# This is \"NEXT PAGE BUTTON CLICK\" - This loops thru the total amount of pages by clicking the next page button\n",
    "\n",
    "for turn_page in range(1, total_results_pages):\n",
    "    \n",
    "    # set the current url as the target page (aiming the boomerang)\n",
    "    target_url = browser.url\n",
    "    \n",
    "    # Use Request.get() - (throw the boomerang at the target, retrieve the info, & return\n",
    "    response_target = requests.get(target_url)\n",
    "    #response\n",
    "    \n",
    "    # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "    target_page_soup = soup(response_target.text, 'html.parser')\n",
    "    \n",
    "    containers = target_page_soup.find_all(\"div\", class_=\"item-container\") \n",
    "    newegg_page_scraper(containers, turn_page)\n",
    "    time.sleep(4)\n",
    "    browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "    \n",
    "browser.quit()\n",
    "concatenate(total_results_pages)\n",
    "\n",
    "print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{query_input}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "print('Thank you and hope you found this useful!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_results_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Scraper And Exports Out to CSV. Later can figure out how to get it out to PyMongo / Mongo Db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct webdriver this is the html page to scrape\n",
    "# splinter .html will set the 'source of the page content' \n",
    "\n",
    "current_url = browser.url\n",
    "# Use Request.get() to pull the current url\n",
    "response = requests.get(current_url)\n",
    "# Use beautiful soup to parse the text using html.parser\n",
    "page_soup = soup(response.text, 'html.parser')\n",
    "\n",
    "# Capture all containers on the page to set up for looping thru later\n",
    "containers = page_soup.find_all(\"div\", class_=\"item-container\") \n",
    "\n",
    "con = containers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.contents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.a.img[\"src\"]\n",
    "con.find_all('a', class_=\"item-title\")[0]['href'].split('?')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the loop to extract: brand, Title of the product, price-dollar, price-cents,\n",
    "# shipping\n",
    "\n",
    "\n",
    "#def newegg_page_scraper(containers, turn_page):\n",
    "def newegg_page_scraper(containers):\n",
    "    item_numbers = []\n",
    "    laptop_brands = []\n",
    "    laptop_models = []\n",
    "    product_links = []\n",
    "    laptop_prices = []\n",
    "    laptop_shipping = []\n",
    "    laptop_images = []\n",
    "    promotions = []\n",
    "    \n",
    "    for con in containers:\n",
    "\n",
    "        page_counter = turn_page\n",
    "        \n",
    "        item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('?')[1]\n",
    "        item_numbers.append(item_num)\n",
    "        \n",
    "        brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "        laptop_brands.append(brand_name)\n",
    "        #print(brand_name)\n",
    "\n",
    "        prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "        laptop_models.append(prd_title)\n",
    "        \n",
    "        product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "        product_links.append(product_link)\n",
    "        \n",
    "        price = con.find_all('li', class_=\"price-current\")[0].text.split()[0]\n",
    "        laptop_prices.append(price)\n",
    "\n",
    "        shipping = con.find_all('li', class_='price-ship')[0].text.strip()\n",
    "        laptop_shipping.append(shipping)\n",
    "        \n",
    "        image = con.a.img[\"src\"]\n",
    "        laptop_image.append(image)\n",
    "        \n",
    "        # add this and the list up top\n",
    "        current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "        promotions.append(current_promo)\n",
    "        \n",
    "        #if statements for the price-was and for sale ending - optional\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "    'brand': laptop_brands,\n",
    "    'model_listing': laptop_models,\n",
    "    'price': laptop_prices,\n",
    "    'shipping': laptop_shipping\n",
    "    })\n",
    "\n",
    "    return df.to_csv(f'./output/newgg_laptops_{page_counter}.csv')\n",
    "    \n",
    "newegg_page_scraper(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
