{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NewEgg.Com WebScraping Program For Laptops - Beta\n",
    "\n",
    "###  - April 2020\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies.\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from re import search\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import random\n",
    "from playsound import playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder to self.\n",
    "#import this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions & Classes Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function to return date throughout the program.\n",
    "\n",
    "def return_dt():\n",
    "    \n",
    "    global current_date\n",
    "    \n",
    "    current_date = str(datetime.datetime.now()).replace(':','.').replace(' ','_')[:-7]\n",
    "    \n",
    "    return current_date\n",
    "\n",
    "#return_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created a function to ask users if they want to watch the Bot (headless = True/False).\n",
    "Lastly, will take you directly to the webpage that was inputted.\n",
    "\"\"\"\n",
    "#head = ''\n",
    "#browser =''\n",
    "def head_on_off(executable_path):\n",
    "    \n",
    "    # Assigning these as global variables enable us to reference these variables outside of the function\n",
    "    global head\n",
    "    global browser\n",
    "    \n",
    "    # Bound possible answers.\n",
    "    \n",
    "    options = [1, 2]\n",
    "    \n",
    "    while head not in options:\n",
    "        \n",
    "        head = int(input('Do you want the desktop watch the bot work? Enter a number: 1 - YES | 2 - NO . Your Answer: '))\n",
    "        \n",
    "        if head not in options:\n",
    "            \n",
    "            print(\"That was not a valid answer. Please try again. \")\n",
    "    # For cases where users enter in valid options:\n",
    "    \n",
    "    if head == options[0]:\n",
    "        \n",
    "        print('Head is activated. Please view only the new automated Google Chrome web browser. ')\n",
    "        \n",
    "        print('Do not make any adjustments to this automated window while the program runs, as it may produce errors or undesired outputs. Only time you should touch the automated window when the recaptcha ask you to prove you are a human. ')\n",
    "        \n",
    "        browser = Browser('chrome', **executable_path, headless=False, incognito=True)\n",
    "    \n",
    "    if head == options[1]:\n",
    "        \n",
    "        print('Headless mode activated. No web browser will pop up. Please proceeed. ')\n",
    "        \n",
    "        browser = Browser('chrome', **executable_path, headless=True, incognito=True)\n",
    "    \n",
    "    # visit the target site\n",
    "    \n",
    "    browser.visit(url)\n",
    "\n",
    "    current_url = browser.url\n",
    "    \n",
    "    return current_url\n",
    "\n",
    "#head_on_off(executable_path)\n",
    "#time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 5) Previews to the user of the scrappable items-contrainers on the page\n",
    "\n",
    "\"\"\"\n",
    "# RENAME AS preview***\n",
    "def preview_y_n(current_page_soup):\n",
    "    \n",
    "    global containers\n",
    "    \n",
    "    preview = input('Would you like to preview some of the scrappable items off the current page? Enter \"y\". ')\n",
    "    \n",
    "    if preview == \"y\":\n",
    "                    \n",
    "        containers = current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "                    \n",
    "        print(\"Preview: expect these scrapped off this page, and for every other total results pages, if there's more than one: \")\n",
    "        \n",
    "        print(\"=\"*35)\n",
    "        \n",
    "        # max items should be 36\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        for con in containers:\n",
    "        \n",
    "            try:\n",
    "                \n",
    "                counter += 1\n",
    "                product_details = con.find_all(\"a\", class_=\"item-title\")[0].text\n",
    "                \n",
    "                product_price = con.find_all(\"li\", class_=\"price-current\")[0].text.split()[0]\n",
    "                \n",
    "                print(f'{counter}) {product_details} | Price: {product_price}')\n",
    "                \n",
    "                print(\"-\"*35)\n",
    "\n",
    "            except (IndexError) as e:\n",
    "                \n",
    "                print(f\"{counter}) This item was not scrappable. Skipped. \")\n",
    "                \n",
    "                print(\"-\"*35)\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if counter == 0:\n",
    "        \n",
    "            print(\"Unable to scrap this link. \")\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            print(f\"{len(containers)} Scrappable Objects on the page. \")\n",
    "    else:\n",
    "    \n",
    "        print(\"Beginning scraping... \")\n",
    "        \n",
    "    return None\n",
    "    \n",
    "#preview_y_n(current_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Originally modeled out parent/child inheritance object structure.\n",
    "\n",
    "After careful research, I found it much easier to export the Pandas Dataframe of the results to a dictionary, and then into a class object, which I will elaborate more down below.\n",
    "\"\"\"\n",
    "# class Product_catalog:\n",
    "    \n",
    "#     all_prod_count = 0\n",
    "    \n",
    "#     def __init__(self, general_category): # computer systems\n",
    "#         self.general_category = general_category\n",
    "        \n",
    "#         Product_catalog.all_prod_count += 1\n",
    "        \n",
    "#     def count_prod(self):\n",
    "#         return int(self.all_prod_count)\n",
    "#         #return '{}'.format(self.general_category)\n",
    "\n",
    "# Sub_category was later changed to Laptops due to the scope of this project.\n",
    "# class Sub_category(Product_catalog): # laptops/notebooks, gaming\n",
    "    \n",
    "#     sub_category_ct = 0\n",
    "    \n",
    "#     def __init__(self, general_category, sub_categ, item_num, brand, price, img_link, prod_link, model_specifications, current_promotions):\n",
    "#         super().__init__(general_category)\n",
    "#         Sub_category.sub_category_ct += 1\n",
    "        \n",
    "#         self.sub_categ = sub_categ\n",
    "#         self.item_num = item_num\n",
    "#         self.brand = brand\n",
    "#         self.price = price\n",
    "#         self.img_link = img_link\n",
    "#         self.prod_link = prod_link\n",
    "#         self.model_specifications = model_specifications\n",
    "#         self.current_promotions = current_promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main NewEgg WebScraper function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def newegg_page_scraper(containers, turn_page):\n",
    "    \n",
    "    page_nums = []\n",
    "    general_category = []\n",
    "    product_categories = []\n",
    "    images = []\n",
    "    product_brands = []\n",
    "    product_models = []\n",
    "    product_links = []\n",
    "    item_numbers = []\n",
    "    promotions = []\n",
    "    prices = []\n",
    "    shipping_terms = []\n",
    "    \n",
    "    # Put this to avoid error that was being generated\n",
    "    global gen_category\n",
    "    \n",
    "    \"\"\" \n",
    "    Loop through all the containers on the HTML, and scrap the following content into the following lists\n",
    "    \n",
    "    \"\"\"\n",
    "    for con in containers:\n",
    "        \n",
    "        try:\n",
    "            page_counter = turn_page\n",
    "            page_nums.append(int(turn_page))\n",
    "            \n",
    "            gen_category = target_page_soup.find_all('div', class_=\"nav-x-body-top-bar fix\")[0].text.split('\\n')[5]\n",
    "            general_category.append(gen_category)\n",
    "            \n",
    "            prod_category = target_page_soup.find_all('h1', class_=\"page-title-text\")[0].text\n",
    "            product_categories.append(prod_category)\n",
    "            \n",
    "            image = con.a.img[\"src\"]\n",
    "            #print(image)\n",
    "            images.append(image)\n",
    "\n",
    "            prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "            product_models.append(prd_title)\n",
    "\n",
    "            product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "            product_links.append(product_link)\n",
    "            \n",
    "            shipping = con.find_all('li', class_='price-ship')[0].text.strip().split()[0]\n",
    "            \n",
    "            if shipping != \"Free\":\n",
    "                shipping = shipping.replace('$', '')\n",
    "                shipping_terms.append(shipping)\n",
    "            else:\n",
    "                shipping = 0.00\n",
    "                shipping_terms.append(shipping)\n",
    "\n",
    "            brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "            product_brands.append(brand_name)\n",
    "\n",
    "        except (IndexError, ValueError) as e:\n",
    "            \n",
    "            # If there are no item_brand container, take the Brand from product details.\n",
    "            product_brands.append(con.find_all('a', class_=\"item-title\")[0].text.split()[0])\n",
    "            #print(f\"{e} block 1\")\n",
    "\n",
    "        try:\n",
    "            current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "            promotions.append(current_promo)\n",
    "            \n",
    "        except:\n",
    "            promotions.append('null')\n",
    "            #print(f\"{e} block 2\")\n",
    "        try:\n",
    "            price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "            prices.append(price)\n",
    "            \n",
    "        except:\n",
    "            price = 'null / out of stock'\n",
    "            prices.append(price)\n",
    "            #print(f\"{e} block 3\")\n",
    "        \n",
    "        try:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "            item_numbers.append(item_num)\n",
    "        except (IndexError) as e:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]\n",
    "            item_numbers.append(item_num)    \n",
    "    \n",
    "    # Convert all of the lists into a dataframe\n",
    "    df = pd.DataFrame({\n",
    "    'item_number': item_numbers,\n",
    "    'general_category': general_category,\n",
    "    'product_category': product_categories,\n",
    "    'brand': product_brands,\n",
    "    'model_specifications': product_models,\n",
    "    'price': prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': shipping_terms,\n",
    "    'page_number': page_nums,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "    \n",
    "    # Rearrange the dataframe columns into the following order.\n",
    "    df = df[['item_number', 'general_category','product_category', 'page_number' ,'brand','model_specifications' ,'current_promotions' ,'price' ,'shipping' ,'product_links','image_link']]\n",
    "    \n",
    "    # Convert the dataframe into a dictionary.\n",
    "    global scraped_dict\n",
    "    scraped_dict = df.to_dict('records')\n",
    "    \n",
    "    # Grab the subcategory \"Laptop/Notebooks\" and eliminate any special characters that may cause errors.\n",
    "    global pdt_category\n",
    "    pdt_category = df['product_category'].unique()[0]\n",
    "    # Eliminate special characters in a string if it exists.\n",
    "    pdt_category = ''.join(e for e in pdt_category if e.isalnum())\n",
    "    \n",
    "    \"\"\" Count the number of items scraped by getting the length of a all the models for sale.\n",
    "        This parameter is always available for each item-container in the HTML\n",
    "    \"\"\"\n",
    "\n",
    "    global items_scraped\n",
    "    items_scraped = len(df['model_specifications'])\n",
    "\n",
    "    \"\"\"\n",
    "    Save the results into a csv file using Pandas\n",
    "    \"\"\"\n",
    "    df.to_csv(f'./processing/{current_date}_{pdt_category}_{items_scraped}_scraped_page{turn_page}.csv')\n",
    "    \n",
    "    # Return these variables as they will be used.\n",
    "    return scraped_dict, items_scraped, pdt_category\n",
    "    \n",
    "#df.head()\n",
    "#newegg_page_scraper(containers, turn_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the total results pages.\n",
    "\n",
    "def results_pages(current_page_soup):\n",
    "    \n",
    "    # Use BeautifulSoup to extract the total results page number\n",
    "    results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "    #print(results_pages)\n",
    "    \n",
    "    # Find and extract total pages + and add 1 to ensure proper length of total pages.\n",
    "    global total_results_pages\n",
    "    total_results_pages = int(re.split(\"/\", results_pages)[1]) + 1 # need to add 1 b/c 'range(inclusive, exclusive)'\n",
    "    #========================================= need to remember to +2, and remove -30\n",
    "    #print(total_results_pages)\n",
    "    \n",
    "    return total_results_pages\n",
    "#results_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build a function to concatenate all pages that were scraped and saved in the processing folder.\n",
    "Save the final output (1 csv file) all the results\n",
    "    \n",
    "\"\"\"\n",
    "def concatenate(total_results_pages):\n",
    "    \n",
    "    path = f'./processing\\\\'\n",
    "    \n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    concatenate_pages = []\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for page in scraped_pages:\n",
    "        \n",
    "        df = pd.read_csv(page, index_col=0, header=0)\n",
    "        \n",
    "        concatenate_pages.append(df)\n",
    "\n",
    "    compiled_data = pd.concat(concatenate_pages, axis=0, ignore_index=True)\n",
    "    \n",
    "    total_items_scraped = len(compiled_data['brand']) # can replace this counter by creating class objects everytime it scrapes\n",
    "    \n",
    "    concatenated_output = compiled_data.to_csv(f\"./finished_outputs/{current_date}_{total_items_scraped}_scraped_{total_results_pages}_pages_.csv\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Built a function to clear out the entire processing files folder to avoid clutter.\n",
    "Or the user can keep the processing files (page by page) for their own analysis.\n",
    "\n",
    "\"\"\"\n",
    "def clean_processing_fldr():\n",
    "    \n",
    "    path = f'./processing\\\\'\n",
    "    \n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    if len(scraped_pages) < 1:\n",
    "        print(\"There are no files in the folder to clear. \")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Clearing out a total of {len(scraped_pages)} scraped pages in the processing folder... \")\n",
    "        \n",
    "        clear_processing_files = []\n",
    "        \n",
    "        for page in scraped_pages:\n",
    "            \n",
    "            os.remove(page)\n",
    "        \n",
    "    print('Clearing of \"Processing\" folder complete. ')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_a_tag_mouse_over():\n",
    "    \n",
    "#     x = random.randint(2, 5)\n",
    "    \n",
    "#     def rdm_slp_2_5(x):\n",
    "        \n",
    "#         time.sleep(x)\n",
    "\n",
    "#         print(f\"Slept for {x} seconds. \")\n",
    "\n",
    "#         return x\n",
    "    \n",
    "#     # Check to see if there are clickable <a>\n",
    "    \n",
    "#     if (browser.is_element_present_by_tag('h1')) == True:\n",
    "        \n",
    "#         print(\"(Check 1) Header is present and hoverable on page. \")\n",
    "    \n",
    "#     else:\n",
    "        \n",
    "#         print(\"(Check 1 - ERROR) Header is NOT present on page. \")\n",
    "        \n",
    "#         # If there isn't, pause the program. Have user click somewhere on the screen.\n",
    "#         playsound('./sounds/user_alert.wav')\n",
    "        \n",
    "#         red_light = input(\"Program could not detect a clickable links to hover over, and click. Please use your mouse to click on the background of the screen, and enter 'y' to continue the scrape. \")\n",
    "        \n",
    "#     if (browser.is_element_present_by_tag(\"a\")) == True:\n",
    "        \n",
    "#         print(\"(Check 2) <a> tags are present on page. Will begin mouse-over thru the page. \")\n",
    "        \n",
    "#     else:\n",
    "#         # If there isn't, pause the program. Have user click somewhere on the screen.\n",
    "#         playsound('./sounds/user_alert.wav')\n",
    "        \n",
    "#         red_light = input(\"Program could not detect a clickable links to hover over, and click. Please use your mouse to click on the background of the screen, and enter 'y' to continue the scrape. \")\n",
    "    \n",
    "\n",
    "#     # Get the total number of <a> tags off the page.\n",
    "    \n",
    "#     number_of_a_tags = len(browser.find_by_tag(\"a\"))\n",
    "    \n",
    "#     print(f\"Total number of <a>: {number_of_a_tags} \")\n",
    "\n",
    "#     \"\"\"\n",
    "#     My observation found that most of the clickable on screen links to each\n",
    "#     product was in the 90th to 99th percentile of <a> links.\n",
    "#     \"\"\"\n",
    "#     ninety_pct_a_tag = int(round((number_of_a_tags * .905)))\n",
    "    \n",
    "#     ninety_96th_a_tag = int(round((number_of_a_tags * .96)))\n",
    "    \n",
    "#     # We know <a> tags are *available* at this point so mouse over through links on the screen.\n",
    "#     for a_tag_num in range(ninety_pct_a_tag, ninety_96th_a_tag):\n",
    "        \n",
    "#         try:\n",
    "#             # Mouse over odd number <a> on the screen (each page 36 items max). Faster for many result items.\n",
    "#             if (a_tag_num % 3) != 0:\n",
    "                \n",
    "#                 #browser.find_by_tag('h1')[0].click()\n",
    "                \n",
    "#                 browser.find_by_tag(\"a\")[a_tag_num].mouse_over()\n",
    "                \n",
    "#                 time.sleep(1)\n",
    "                \n",
    "#         except:\n",
    "#             # Mouse over even number <a> on the screen. Faster for less results items.\n",
    "            \n",
    "#             print(f\"EXEPTION 1 - From random_a_tag_MOUSE_OVER func (Modulo 3 odd) \\n Sleeping for 2 seconds and will try again with Modulo 2 - even. \")\n",
    "            \n",
    "#             browser.find_by_tag(\"a\")[a_tag_num].mouse_over()\n",
    "            \n",
    "#             if (a_tag_num % 2) == 0:\n",
    "            \n",
    "#                 browser.find_by_tag(\"a\")[a_tag_num].mouse_over()\n",
    "#                 time.sleep(1)\n",
    "                \n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_a_tag_mouse_over2():\n",
    "    \n",
    "    x = random.randint(2, 5)\n",
    "    \n",
    "    def rdm_slp_2_5(x):\n",
    "\n",
    "        time.sleep(x)\n",
    "\n",
    "        print(f\"Slept for {x} seconds. \")\n",
    "\n",
    "        return\n",
    "    \n",
    "    # Check to see if there are clickable <a>\n",
    "    # Look for for all combinations of text in the laptop section for h1 header\n",
    "#     if (browser.is_element_present_by_tag('h1')) and \\\n",
    "#     (browser.is_element_present_by_text('Are you a human?') or  \n",
    "#     )== True:\n",
    "    \n",
    "    if (browser.is_element_present_by_tag('h1') and browser.is_element_present_by_text('Laptops')) == True:\n",
    "        print(\"(Check 1) Header is present and hoverable on page. \")\n",
    "    \n",
    "    else:\n",
    "        playsound('./sounds/user_alert.wav')\n",
    "        print(\"(Check 1 - ERROR) Header is NOT present on page. \")\n",
    "        \n",
    "        # If there isn't, pause the program. Have user click somewhere on the screen.\n",
    "        \n",
    "        red_light = input(\"Program could not detect a clickable links to hover over, and click. Please use your mouse to click on the background of the screen, and enter 'y' to continue the scrape. \")\n",
    "        \n",
    "    if (browser.is_element_present_by_tag(\"a\")) == True:\n",
    "        \n",
    "        print(\"(Check 2) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \")\n",
    "        \n",
    "    else:\n",
    "        # If there isn't, pause the program. Have user click somewhere on the screen.\n",
    "        playsound('./sounds/user_alert.wav')\n",
    "        red_light = input(\"Program could not detect a clickable links to hover over, and click. Please use your mouse to click on the background of the screen, and enter 'y' to continue the scrape. \")\n",
    "    \n",
    "\n",
    "    # Get the total number of <a> tags off the page.\n",
    "    \n",
    "    number_of_a_tags = len(browser.find_by_tag(\"a\"))\n",
    "    \n",
    "    print(f\"Total number of <a>: {number_of_a_tags} . \\n Next will collect working / not working <a> tags\")\n",
    "\n",
    "    \"\"\"\n",
    "    My observation found that most of the clickable on screen links to each\n",
    "    product was in the 90th to 99th percentile of <a> links.\n",
    "    \"\"\"\n",
    "    ninety_pct_a_tag = int(round((number_of_a_tags * .905)))\n",
    "    \n",
    "    ninety_96th_a_tag = int(round((number_of_a_tags * .96)))\n",
    "    \n",
    "    atag_selected_working = []\n",
    "    atag_selected_notworking = []\n",
    "    \n",
    "    atag_num_list = []\n",
    "    # list of atags true and false\n",
    "    atagnum_tf = []\n",
    "    \n",
    "    rdm_slp_2_5(x)\n",
    "    \n",
    "    # Loop thru all <a> tags between 90.5% - 96%, and find all present <a> tags\n",
    "    for a_tag_num in range(ninety_pct_a_tag, ninety_96th_a_tag):\n",
    "    \n",
    "        # loop thru whole range and make 2 lists: one for atags and one appending results (T/F)\n",
    "        \n",
    "        atag_num_list.append(a_tag_num)\n",
    "        \n",
    "        result = browser.is_element_present_by_tag(\"a\")[a_tag_num]\n",
    "        \n",
    "        atagnum_tf.append(result)\n",
    "    \n",
    "    total_atags = len(atagnum_tf)\n",
    "    \n",
    "    for a in atagnum_tf:\n",
    "        \n",
    "        if a == True:\n",
    "            # if the <a> tag is present(true), grab the index and store in a list comprehension\n",
    "            atag_index_true = [i for i, x in enumerate(atagnum_tf) if x]\n",
    "        \n",
    "        else:\n",
    "            # if the <a> tag is NOT present, grab the index and store in a list comprehension\n",
    "            atag_index_false = [i for i, x in enumerate(atagnum_tf) if x]\n",
    "\n",
    "    if total_atags == (len(atag_index_true) + len(atag_index_false)):\n",
    "        \n",
    "        print(f\"{total_atags} <a> tags reconcile. \")\n",
    "        print(f\"Total True: {atag_index_true} | Total False: {atag_index_false} \")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"Total <a> tags didn't match up to the sum of the true and false - need to investigate. \")\n",
    "        print(f\"{total_atags} Total <a>'s' | Total True: {atag_index_true} | Total False: {atag_index_false} \")\n",
    "    \n",
    "    \n",
    "    atags_mouse_over_success = []\n",
    "    atags_present_but_errors = []\n",
    "    \n",
    "    for a in atag_index_true:\n",
    "        \n",
    "        try:\n",
    "            working_atag_number = atag_num_list[a]\n",
    "            browser.find_by_tag(\"a\")[working_atag_number].mouse_over()\n",
    "            time.sleep(1)\n",
    "        \n",
    "        except:\n",
    "            working_atag_number = atag_num_list[a]\n",
    "            atags_present_but_errors.append(working_atag_number)\n",
    "            print(f\"{working_atag_number} this tag isn't working. Adding to the 'atags_present_but_errors' list. \")\n",
    "    \n",
    "    print('Attempting mouse over all present links... ')\n",
    "        \n",
    "    return atags_mouse_over_success, atags_present_but_errors\n",
    "        \n",
    "    \n",
    "    ##################################################\n",
    "    # We know <a> tags are *available* at this point so mouse over through links on the screen.\n",
    "#     for a_tag_num in range(ninety_pct_a_tag, ninety_96th_a_tag, 3):\n",
    "\n",
    "#         if (browser.is_element_present_by_tag(\"a\")[a_tag_num]) == True:\n",
    "        \n",
    "#             print(\"(Check 2) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \")\n",
    "            \n",
    "#             print(f\"The current a_tag_num is: {a_tag_num} - reference. \")\n",
    "            \n",
    "#             atag_selected_working.append(a_tag_num)\n",
    "            \n",
    "#         try:\n",
    "#             # Mouse over odd number <a> on the screen (each page 36 items max). Faster for many result items.\n",
    "#             #if (a_tag_num % 3) != 0:\n",
    "                \n",
    "#                 #browser.find_by_tag('h1')[0].click()\n",
    "                \n",
    "#                 browser.find_by_tag(\"a\")[a_tag_num].mouse_over()\n",
    "                \n",
    "#                 print(\"MouseOver&Click Func - Try ran \")\n",
    "                \n",
    "#                 time.sleep(1)\n",
    "                \n",
    "#         except:\n",
    "#             # Mouse over even number <a> on the screen. Faster for less results items.\n",
    "            \n",
    "#             print(f\" EXCEPTION 2 - From random_a_tag_mouse_over_ANDCLICK func (Modulo 3 odd) \\n Sleeping for 2 seconds and will try again with Modulo 2 - even. \")\n",
    "            \n",
    "#             browser.find_by_tag(\"a\")[a_tag_num].mouse_over()\n",
    "            \n",
    "#             print(\"EXCEPTION 2 MouseOver&Click Func - Mouse over ran, next is click... \")\n",
    "            \n",
    "#             if (a_tag_num % 2) == 0:\n",
    "            \n",
    "#                 browser.find_by_tag(\"a\")[a_tag_num].mouse_over()\n",
    "#                 time.sleep(2)\n",
    "    \n",
    "#     # After mousing over, thru the page, *click* a random item.\n",
    "    \n",
    "#     for a_tag_num in randint(ninety_pct_a_tag, ninety_96th_a_tag):\n",
    "        \n",
    "#         if (browser.is_element_present_by_tag(\"a\")[a_tag_num]) == True:\n",
    "            \n",
    "#             browser.find_by_tag(\"a\")[a_tag_num].mouse_over()\n",
    "            \n",
    "#             browser.find_by_tag(\"a\")[a_tag_num].click()\n",
    "            \n",
    "#             return a_tag_num\n",
    "            \n",
    "#             rdm_slp_2_5()\n",
    "            \n",
    "#             break\n",
    "            \n",
    "#         else:\n",
    "#             # Try again if first time didn't work - could be due to unclickable link.\n",
    "#             a_tag_num = randint(ninety_pct_a_tag, ninety_96th_a_tag)\n",
    "            \n",
    "#             if (browser.is_element_present_by_tag(\"a\")[a_tag_num]) == True:\n",
    "\n",
    "#                 browser.find_by_tag(\"a\")[a_tag_num].mouse_over()\n",
    "                \n",
    "#                 browser.find_by_tag(\"a\")[a_tag_num].click()\n",
    "                \n",
    "#                 return a_tag_num\n",
    "            \n",
    "#                 rdm_slp_2_5()\n",
    "                \n",
    "#                 break\n",
    "            \n",
    "#             else:\n",
    "#                 atag_selected_notworking.append(a_tag_num)\n",
    "#                 print(\"Able to mouse over thru page, but was not able to click a link 2 times. Need to review this step. \")\n",
    "    \n",
    "#             return atag_selected_working, atag_selected_notworking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_a_tag_mouse_over3():\n",
    "    \n",
    "    x = random.randint(2, 5)\n",
    "    \n",
    "    def rdm_slp_2_5(x):\n",
    "\n",
    "        time.sleep(x)\n",
    "\n",
    "        print(f\"Slept for {x} seconds. \")\n",
    "\n",
    "        return\n",
    "    \n",
    "    if (browser.is_element_present_by_tag('h1') and browser.is_element_present_by_text('a')) == True:\n",
    "        print(\"(Check 1) Header is present and hoverable on page. \")\n",
    "    \n",
    "    else:\n",
    "        playsound('./sounds/user_alert.wav')\n",
    "        print(\"(Check 1 - ERROR) Header is NOT present on page. \")\n",
    "        \n",
    "        # If there isn't, pause the program. Have user click somewhere on the screen.\n",
    "\n",
    "        red_light = input(\"Program could not detect a clickable links to hover over, and click. Please use your mouse to click on the background of the screen, and enter 'y' to continue the scrape. \")\n",
    "        \n",
    "\n",
    "    # Get the total number of <a> tags off the page.\n",
    "    \n",
    "    number_of_a_tags = len(browser.find_by_tag(\"a\"))\n",
    "    \n",
    "    print(f\"Total number of <a>: {number_of_a_tags} . \\n Next will collect working / not working <a> tags\")\n",
    "\n",
    "    \"\"\"\n",
    "    My observation found that most of the clickable on screen links to each\n",
    "    product was in the 90th to 99th percentile of <a> links.\n",
    "    \"\"\"\n",
    "    ninety_pct_a_tag = int(round((number_of_a_tags * .905)))\n",
    "    \n",
    "    ninety_96th_a_tag = int(round((number_of_a_tags * .96)))\n",
    "    \n",
    "    atag_selected_working = []\n",
    "    atag_selected_notworking = []\n",
    "    \n",
    "    atag_num_list = []\n",
    "    # list of atags true and false\n",
    "    atagnum_tf = []\n",
    "    \n",
    "    rdm_slp_2_5(x)\n",
    "    \n",
    "    atags_mouse_over_success = []\n",
    "    atags_present_but_errors = []\n",
    "    # Loop thru all <a> tags between 90.5% - 96%, and find all present <a> tags\n",
    "    for a_tag_num in range(ninety_pct_a_tag, ninety_96th_a_tag, 2):\n",
    "    \n",
    "        # loop thru whole range and make 2 lists: one for atags and one appending results (T/F)\n",
    "        \n",
    "        atag_num_list.append(a_tag_num)\n",
    "\n",
    "        try:\n",
    "\n",
    "            browser.find_by_tag(\"a\")[a_tag_num].mouse_over()\n",
    "            atags_mouse_over_success.append(a_tag_num)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        except:\n",
    "\n",
    "            atags_present_but_errors.append(a_tag_num)\n",
    "            print(f\"{a_tag_num} this tag isn't working. Adding to the 'atags_present_but_errors' list. \")\n",
    "    \n",
    "    print('Attempting mouse over all present links... ')\n",
    "        \n",
    "    return atags_mouse_over_success, atags_present_but_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "1972 last working <a> tag\n",
      "2 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "1977 last working <a> tag\n",
      "3 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "1982 last working <a> tag\n",
      "4 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "1987 last working <a> tag\n",
      "5 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "1992 last working <a> tag\n",
      "6 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "1997 last working <a> tag\n",
      "7 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "2002 last working <a> tag\n",
      "8 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "2007 last working <a> tag\n",
      "9 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "2012 last working <a> tag\n",
      "10 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "2017 last working <a> tag\n",
      "11 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "2022 last working <a> tag\n",
      "12 - Current Try Count\n",
      "Finally - Last Attempt\n",
      "0 - Current finally Count\n",
      "2027 last working <a> tag\n",
      "13 - Current Try Count\n",
      "Finally - Last Attempt\n"
     ]
    },
    {
     "ename": "JavascriptException",
     "evalue": "Message: javascript error: Failed to execute 'elementsFromPoint' on 'Document': The provided double value is non-finite.\n  (Session info: chrome=80.0.3987.163)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJavascriptException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-d0ccd9366509>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mfinally_atags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mrdm_slp_6_9\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmouse_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mworking_atags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mfinally_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py\u001b[0m in \u001b[0;36mmouse_over\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m         \"\"\"\n\u001b[0;32m    730\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscroll_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m         \u001b[0mActionChains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove_to_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmouse_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\action_chains.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \"\"\"\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw3c\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw3c_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_actions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\actions\\action_builder.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0menc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"actions\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW3C_ACTIONS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclear_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJavascriptException\u001b[0m: Message: javascript error: Failed to execute 'elementsFromPoint' on 'Document': The provided double value is non-finite.\n  (Session info: chrome=80.0.3987.163)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = random.randint(5, 9)\n",
    "    \n",
    "def rdm_slp_6_9(x):\n",
    "\n",
    "    time.sleep(x)\n",
    "\n",
    "    #print(f\"Slept for {x} seconds. \")\n",
    "\n",
    "    return x\n",
    "\n",
    "number_of_a_tags = len(browser.find_by_tag(\"a\"))\n",
    "\n",
    "ninety_pct_a_tag = int(round((number_of_a_tags * .90))) \n",
    "\n",
    "ninety_96th_a_tag = int(round((number_of_a_tags * .95)))\n",
    "\n",
    "working_try_atags = []\n",
    "\n",
    "working_except_atags = []\n",
    "\n",
    "# This will all be working except the last one.\n",
    "finally_atags = []\n",
    "\n",
    "working_atags = []\n",
    "\n",
    "not_working_atags = []\n",
    "\n",
    "try_counter = 0\n",
    "\n",
    "except_counter = 0\n",
    "\n",
    "finally_counter = 0\n",
    "\n",
    "\n",
    "browser.find_by_tag(\"h1\")\n",
    "\n",
    "for i in range(ninety_pct_a_tag, ninety_96th_a_tag, 5):\n",
    "    try:\n",
    "        browser.find_by_tag(\"a\")[i+2].mouse_over()\n",
    "        rdm_slp_6_9(x)\n",
    "        print(f\"Mimic Humans - try - Sleep for {x} seconds. \")\n",
    "        working_try_atags.append(i+2)\n",
    "        working_atags.append(i+2)\n",
    "        try_counter += 1\n",
    "        print(f\"{try_counter} - Current Try Count\")\n",
    "        \n",
    "    except:\n",
    "        print(\"Exception - Trying to hover over another link.\")\n",
    "        not_working_atags.append(i+2)\n",
    "        rdm_slp_6_9(x)\n",
    "        browser.find_by_tag(\"a\")[i+8].mouse_over()\n",
    "        working_except_atags.append(i+8)\n",
    "        working_atags.append(i+8)\n",
    "        except_counter += 1\n",
    "        print(f\"{except_counter} - Current except Count\")\n",
    "        \n",
    "    finally:\n",
    "        not_working_atags.append(i+8)\n",
    "        print(\"Finally - Last Attempt\")\n",
    "        # if there's an error, pull the last item in the list from finally_atags list\n",
    "        finally_atags.append(i+3)\n",
    "        rdm_slp_6_9(x)\n",
    "        browser.find_by_tag(\"a\")[i+3].mouse_over()\n",
    "        working_atags.append(i+3)\n",
    "        finally_counter += 1\n",
    "        print(f\"{except_counter} - Current finally Count\")\n",
    "        \n",
    "    print(f\"{working_atags[-1]} last working <a> tag\")\n",
    "\n",
    "browser.find_by_tag(\"h1\")\n",
    "print(f\"All working <a> tags: {working_atags} \\n\")\n",
    "print(f\"All working <a> tags: {not_working_atags} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What you need to do:\n",
    "\n",
    "\"\"\"\n",
    "- Come up with an algorithm that will <a> hop big (assume) 2k plus <a> tag links\n",
    "- to jump big first, and then except and pause, and then for finally, break the loop after gathering about 5 workable\n",
    "links, and then set up a another random.randint() function to pick one from the list, mouse_over, and click, and back\n",
    "to emulate human behavior OR just mouse over and then scrape. and then next page\n",
    "\n",
    "- Post are you human test <a> links drop down significantly - come up with an algorithm too\n",
    "\n",
    "- if <a> length is greater than 2k then use 90% method\n",
    "\n",
    "if <a> is less than 200 use 35% method\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2005, 2030, 2035]\n"
     ]
    }
   ],
   "source": [
    "print(working_except_atags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(working_atags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2061"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finally_atags[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_a_tags\n",
    "\n",
    "ninety_pct_a_tag\n",
    "\n",
    "ninety_96th_a_tag\n",
    "\n",
    "working_try_atags\n",
    "\n",
    "working_except_atags\n",
    "\n",
    "finally_atags\n",
    "working_atags = []\n",
    "\n",
    "not_working_atags = []\n",
    "\n",
    "try_counter = 0\n",
    "\n",
    "except_counter = 0\n",
    "\n",
    "finally_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2032\n"
     ]
    }
   ],
   "source": [
    "print(finally_atags[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1977, 1982, 1987, 1992, 1997, 2002, 2007, 2012, 2017, 2022, 2027, 2032, 2037]\n"
     ]
    }
   ],
   "source": [
    "print(not_working_atags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1971, 1972, 1976, 1977, 1981, 1982, 1986, 1987, 1991, 1992, 1996, 1997, 2001, 2002, 2006, 2007, 2011, 2012, 2016, 2017, 2021, 2022, 2026, 2027, 2031]\n"
     ]
    }
   ],
   "source": [
    "print(working_atags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-d43ded6432ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_element_present_by_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1989\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "browser.is_element_present_by_tag(\"a\")[1989]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "browser.find_link_by_href(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "browser.is_element_present_by_tag(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag(\"a\")[2100].mouse_over()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "t = [True, False, True, False, False, True]\n",
    "\n",
    "bool_conversion = []\n",
    "for a in t:\n",
    "    bool_val = int(a == True)\n",
    "    bool_conversion.append(bool_val)\n",
    "    \n",
    "print(bool_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "atag_index_true = [i for i, x in enumerate(t) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-db0bf397fa8a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-db0bf397fa8a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    [i for i, v enumerate(t) if v]\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[i for i, v enumerate(t) if v]\n",
    "[i for i, x in enumerate(t) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_conversion.index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False]\n",
    "[i for i, x in enumerate(t) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(atag_num_list_T_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_atags = []\n",
    "\n",
    "for t in bool_conversion:\n",
    "    #print(t)\n",
    "     #print(bool_conversion.index(t))\n",
    "    \n",
    "    if t == True:\n",
    "        \n",
    "        index = atag_num_list_T_F.index(t)\n",
    "        \n",
    "        present_atags.append(index,)\n",
    "            \n",
    "print(present_atags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = ['e', 'e', 'i', 'o', 'i', 'e']\n",
    "\n",
    "for v in vowels:\n",
    "    \n",
    "    a = vowels.index(v)\n",
    "    print(a)\n",
    "\n",
    "# index of 'e'\n",
    "index = vowels.index('e')\n",
    "\n",
    "print('The index of e:', index)\n",
    "\n",
    "# index of the first 'i'\n",
    "index = vowels.index('i')\n",
    "print('The index of i:', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_recaptcha_check():\n",
    "    \n",
    "    if browser.is_element_present_by_id('g-recaptcha') == True:\n",
    "        \n",
    "        playsound('./sounds/user_alert.wav')\n",
    "        \n",
    "        continue_scrape = input(\"Noticed Newegg asked if you were robot. \\n When you're done proving you are not a robot and completing all picture tests, enter in any key and press ENTER to continue the webscrape. \")\n",
    "        \n",
    "        print(\"Continuing with scrape... \")\n",
    "        \n",
    "        return\n",
    "\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_you_human_check():\n",
    "    \n",
    "    global current_url\n",
    "    \n",
    "    current_url=''\n",
    "    \n",
    "    if browser.is_element_present_by_tag('h2') and browser.is_element_present_by_text('Are you a human?')  == True:\n",
    "        \n",
    "        playsound('./sounds/user_alert.wav')\n",
    "        \n",
    "        continue_scrape = input(\"Noticed Newegg suspects we are a bot. \\n Manually click on a link, act as if you're looking at the item, then go back on the browser. Then enter in any key and press ENTER to continue. \")\n",
    "        \n",
    "        # After passes the \"Are you human?\" test\n",
    "        if (browser.is_element_present_by_tag('h1') and browser.is_element_present_by_text('Laptops')) == True:\n",
    "        \n",
    "            print(\"(Check 1 - PASSED) Header (Laptops) is present and hoverable on page. - (AFTER ARE YOU HUMAN CHECK) \")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(\"(Check 1 - ERROR) Header is NOT present on page. - (AFTER ARE YOU HUMAN CHECK) \")\n",
    "        \n",
    "            # If there isn't, pause the program. Have user click somewhere on the screen.\n",
    "            \n",
    "            playsound('./sounds/user_alert.wav')\n",
    "            \n",
    "            print(\"Program could not detect a clickable links to hover over, and click. \")\n",
    "            \n",
    "            continue_scrape = input(\"Noticed Newegg suspects we are a bot. \\n Manually click on a link, act as if you're looking at the item, then go back on the browser. Then enter in any key and press ENTER to continue. \")\n",
    "        \n",
    "        current_url = browser.url\n",
    "        \n",
    "        return current_url\n",
    "        \n",
    "        print('MANUALLY BY-PASSED the \"Are you Human?\" check by the site. Continuing with scrape... \\n')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        current_url = browser.url\n",
    "        \n",
    "        response = requests.get(current_url)\n",
    "        \n",
    "        print(response)\n",
    "        \n",
    "        print('Locked in and returning: \"current_url\" - After the \"Are you human?\"\" check. Proceeding... \\n ')\n",
    "        \n",
    "        return current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_xpath_top_bottom():\n",
    "    \n",
    "    # Check if there are working links on the screen, otherwise alert the user.\n",
    "    \n",
    "    if (browser.is_element_present_by_tag('h1')) == True:\n",
    "        \n",
    "        print(\"(Check 1 - Random Xpath Top Bottom) Header is present and hoverable on page. \")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"(Check 1 - ERROR - Random Xpath Top Bottom) Header is NOT present on page. \")\n",
    "        \n",
    "        # If there isn't, pause the program. Have user click somewhere on the screen.\n",
    "        \n",
    "        playsound('./sounds/user_alert.wav')\n",
    "        \n",
    "        red_light = input(\"Program could not detect a clickable links to hover over, and click. Please use your mouse to click on the background of the screen, and enter 'y' to continue the scrape. \")\n",
    "        \n",
    "    if (browser.is_element_present_by_tag(\"a\")) == True:\n",
    "        \n",
    "        print(\"(Check 2- Random Xpath Top Bottom) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \")\n",
    "        \n",
    "    else:\n",
    "        # If there isn't, pause the program. Have user click somewhere on the screen.\n",
    "        \n",
    "        playsound('./sounds/user_alert.wav')\n",
    "        \n",
    "        red_light = input(\"Program could not detect a clickable links to hover over, and click. Please use your mouse to click on the background of the screen, and enter 'y' to continue the scrape. \")\n",
    "    \n",
    "    # There are clickable links, then 'flip the coin' to choose top or bottom button\n",
    "    coin_toss_top_bottom = random.randint(0,1)\n",
    "    \n",
    "    next_page_button_results = []\n",
    "    \n",
    "    # If the coin toss is even, mouse_over and click the top page link.\n",
    "    if (coin_toss_top_bottom == 0):\n",
    "        \n",
    "        print('Heads - Clicking \"Next Page\" Top Button. ')\n",
    "        \n",
    "        browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').mouse_over()\n",
    "        \n",
    "        browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "        \n",
    "        next_page_button_results.append(coin_toss_top_bottom)\n",
    "        \n",
    "        print('Heads - SUCCESSFUL \"Next Page\" Top Button. ')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('Tails - Clicking \"Next Page\" Bottom Button. ')\n",
    "        \n",
    "        browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[4]/div/div/div[10]/button').mouse_over()\n",
    "        \n",
    "        browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[4]/div/div/div[10]/button').click()\n",
    "        \n",
    "        next_page_button_results.append(coin_toss_top_bottom)\n",
    "        \n",
    "        print('Tails - SUCCESSFUL \"Next Page\" Bottom Button. ')\n",
    "        \n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class takes in the dictionary from the webscraper function,\n",
    "and will be used in a list comprehension to produce class \"objects\"\n",
    "in the \"product_catalog\"\n",
    "\"\"\"\n",
    "\n",
    "class Laptops:\n",
    "    \n",
    "    def __init__(self, **entries):\n",
    "        \n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask if you're in another country\n",
    "# id=\"popup_overlay\"\n",
    "# /html/body/div[13]/div/div[2]/div[2]/button[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser.is_element_present_by_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program Logic\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NewEgg.Com WebScraper Beta ===\n",
      "============================================================\n",
      "Scope: This project is a beta and is only built to scrape the laptop section of NewEgg.com due to limited time. \n",
      "\n",
      "Instructions:\n",
      "\n",
      "Current Date And Time: 2020-04-20_21.50.15\n",
      "\n",
      "(1) Go to www.newegg.com, go to the laptop section, select your requirements (e.g. brand, screensize, and specifications - SSD size, processor brand and etc...) \n",
      "(2) Copy and paste the url from your exact search when prompted \n",
      "(3) Activate or Disable the \"Head View\", webscraper bots point of view \n",
      "(4) After the webscraping is successful, you will have an option to concatenate all of the pages you scraped together into one csv file\n",
      "(5) Lastly, you will have an option to clear out the processing folder (data scraped by each page)\n",
      "(6) If you have any issues or errors, \"PRESS CTRL + C\" to quit the program in the terminal \n",
      "(7) Disclaimer: Newegg may ban you for a 24 - 48 hours for webscraping their data, then you may resume. \n",
      " Also, please consider executing during the day, with tons of web traffic to their site in your respective area. \n",
      "\n",
      "Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: https://www.newegg.com/p/pl?N=100167748%20600004804\n",
      "Do you want the desktop watch the bot work? Enter a number: 1 - YES | 2 - NO . Your Answer: 1\n",
      "Head is activated. Please view only the new automated Google Chrome web browser. \n",
      "Do not make any adjustments to this automated window while the program runs, as it may produce errors or undesired outputs. Only time you should touch the automated window when the recaptcha ask you to prove you are a human. \n",
      "The Break Pedal: Answer any robot queries by NewEgg. \n",
      " Enter \"y\" when you are ready to proceed. y\n",
      "Proceeding with webscrape... \n",
      "<Response [200]>\n",
      "Locked in and returning: \"current_url\" - After the \"Are you human?\"\" check. Proceeding... \n",
      " \n",
      "Would you like to preview some of the scrappable items off the current page? Enter \"y\". n\n",
      "Beginning scraping... \n",
      "Initial click on screen and product, and then back. \n",
      "(Check 1 - ERROR) Header is NOT present on page. \n",
      "Program could not detect a clickable links to hover over, and click. Please use your mouse to click on the background of the screen, and enter 'y' to continue the scrape. y\n",
      "(Check 2) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "Total number of <a>: 2198 . \n",
      " Next will collect working / not working <a> tags\n",
      "Slept for 4 seconds. \n",
      "EXCEPTION - Couldn't click an <a> tag link...trying another random present <a> link. \n",
      "(Check 1 - ERROR) Header is NOT present on page. \n",
      "Program could not detect a clickable links to hover over, and click. Please use your mouse to click on the background of the screen, and enter 'y' to continue the scrape. y\n",
      "(Check 2) <a> tags are present on page. Will begin mouse-over thru the page, and click a link. \n",
      "Total number of <a>: 2198 . \n",
      " Next will collect working / not working <a> tags\n",
      "Slept for 5 seconds. \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-ea0146a6300e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Initial click on screen and product, and then back. \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mrandom_a_tag_mouse_over2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[0matags_mouse_over_success\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matags_mouse_over_success\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-2949d866b411>\u001b[0m in \u001b[0;36mrandom_a_tag_mouse_over2\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_element_present_by_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma_tag_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-ea0146a6300e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"EXCEPTION - Couldn't click an <a> tag link...trying another random present <a> link. \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[0mrandom_a_tag_mouse_over2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0matags_mouse_over_success\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matags_mouse_over_success\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mfirst_atag_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matags_mouse_over_success\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-2949d866b411>\u001b[0m in \u001b[0;36mrandom_a_tag_mouse_over2\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0matag_num_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_tag_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_element_present_by_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma_tag_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0matagnum_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\"\"\" Welcome to the program message!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== NewEgg.Com WebScraper Beta ===\")\n",
    "print(\"==\"*30)\n",
    "print('Scope: This project is a beta and is only built to scrape the laptop section of NewEgg.com due to limited time. ')\n",
    "print(\"\")\n",
    "print(\"Instructions:\")\n",
    "print(\"\")\n",
    "return_dt()\n",
    "print(f'Current Date And Time: {current_date}')\n",
    "print(\"\")\n",
    "print(\"(1) Go to www.newegg.com, go to the laptop section, select your requirements (e.g. brand, screensize, and specifications - SSD size, processor brand and etc...) \")\n",
    "print(\"(2) Copy and paste the url from your exact search when prompted \")\n",
    "print('(3) Activate or Disable the \"Head View\", webscraper bots point of view ')\n",
    "print('(4) After the webscraping is successful, you will have an option to concatenate all of the pages you scraped together into one csv file')\n",
    "print('(5) Lastly, you will have an option to clear out the processing folder (data scraped by each page)')\n",
    "print('(6) If you have any issues or errors, \"PRESS CTRL + C\" to quit the program in the terminal ')\n",
    "print('(7) Disclaimer: Newegg may ban you for a 24 - 48 hours for webscraping their data, then you may resume. \\n Also, please consider executing during the day, with tons of web traffic to their site in your respective area. ')\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Set up Splinter requirements.\n",
    "executable_path = {'executable_path': './chromedriver.exe'}\n",
    "\n",
    "# Ask user to input in the laptop query link they would like to scrape.\n",
    "url = input(\"Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: \")\n",
    "\n",
    "# Setting up global variables for head and browser that will used in a function in the program.\n",
    "head = ''\n",
    "browser =''\n",
    "\n",
    "\"\"\"\n",
    "Run the head_on_off function to ask the user if they want to watch the program work.\n",
    "# head_on_off will finish with browse.visit(url) - and returns \"current_url\" as an output\n",
    "that passed into response in a bit below.\n",
    "\"\"\"\n",
    "head_on_off(executable_path)\n",
    "\n",
    "\"\"\"\n",
    "Think of safe_proceed_y_n function as a break pedal.\n",
    "Sometimes it will ask if the user is a bot after multiple scrapes.\n",
    "\"\"\"\n",
    "\n",
    "safe_proceed_y_n = input(f'The Break Pedal: Answer any robot queries by NewEgg. \\n Enter \"y\" when you are ready to proceed. ')\n",
    "if safe_proceed_y_n == 'y':\n",
    "    print(f'Proceeding with webscrape... ')\n",
    "    current_url = browser.url\n",
    "    \n",
    "else:\n",
    "    print(\"Quitting browser. \\n You will need to press CTRL + C to QUIT the program, and then restart it to try scraping the link again. \")\n",
    "    browser.quit()\n",
    "\n",
    "    #AREA WHERE IT BREAKS SOMETIMES - ARE YOU HUMAN\n",
    "# Gather regquests and gets the data, and parses the text for BeautifulSoup to setup scrape.\n",
    "\n",
    "\"\"\"\n",
    "Checks to see if Newegg throws a head fake, in the html \"Are you human?\" to prevent scraping. Returns current_url.\n",
    "If yes, then we will need to manually bypass, and resume the scrape.\n",
    "\"\"\"\n",
    "\n",
    "are_you_human_check()\n",
    "\n",
    "response = requests.get(current_url)\n",
    "#print(response)\n",
    "\n",
    "# It breaks here bc of an index error - not the same HTML\n",
    "current_page_soup = soup(response.text, 'html.parser')\n",
    "\n",
    "# might need to delete - unnecessary\n",
    "##current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "\n",
    "# Option to preview some of the data that will be scraped.\n",
    "preview_y_n(current_page_soup)\n",
    "\n",
    "# Run the results_pages function to gather the total pages to be scraped.\n",
    "results_pages(current_page_soup)\n",
    "\n",
    "time.sleep(2)\n",
    "\"\"\"\n",
    "This is the loop that performs the page by page scraping of data / results\n",
    "of the user's query.\n",
    "\"\"\"\n",
    "# List set up for where class Laptop objects will be stored.\n",
    "product_catalog = []\n",
    "\n",
    "for turn_page in range(1, total_results_pages):\n",
    "    \n",
    "    \"\"\"\n",
    "    If \"reCAPTCHA\" pops up, pause the program using an input. This allows the user to continue\n",
    "    to scrape after they're done completing the quiz by inputting any value.\n",
    "    \n",
    "    \"\"\"\n",
    "    g_recaptcha_check()\n",
    "        \n",
    "    try:\n",
    "        #rdm_slp_4_20()\n",
    "        print(f\"Initial click on screen and product, and then back. \")\n",
    "        random_a_tag_mouse_over3() # have this return last working index and call it again to continue or just scrape\n",
    "        # the page\n",
    "        atags_mouse_over_success = map(int, atags_mouse_over_success)\n",
    "#         first_atag_num = min(atags_mouse_over_success)\n",
    "#         last_atag_num = max(atags_mouse_over_success)\n",
    "        \n",
    "#         click_atag = random.randint(first_atag_num, last_atag_num)\n",
    "#         browser.find_by_tag(\"a\")[click_atag].mouse_over()\n",
    "#         time.sleep(1)\n",
    "#         browser.find_by_tag(\"a\")[click_atag].click()\n",
    "#         browser.back()\n",
    "\n",
    "    # If there's an error, random sleep, try clicking a nonreactive link on the screen, and then click a tag.\n",
    "    except:\n",
    "        print(f\"EXCEPTION - Couldn't click an <a> tag link...trying another random present <a> link. \")\n",
    "        random_a_tag_mouse_over3()\n",
    "        atags_mouse_over_success = map(int, atags_mouse_over_success)\n",
    "#         first_atag_num = min(atags_mouse_over_success)\n",
    "#         last_atag_num = max(atags_mouse_over_success)\n",
    "        \n",
    "#         click_atag = random.randint(first_atag_num, last_atag_num)\n",
    "        browser.find_by_tag(\"a\")[click_atag].mouse_over()\n",
    "        time.sleep(1)\n",
    "        browser.find_by_tag(\"a\")[click_atag].click()\n",
    "        browser.back()\n",
    "        #browser.back()\n",
    "        \n",
    "    print(\"STOPPED PROGRAM. \")\n",
    "    \n",
    "    break\n",
    "    \n",
    "    # After you press back, run checks again to alert users if we are intercepted.\n",
    "    g_recaptcha_check()\n",
    "    # Passing \"are_youare_you_human_check\" returns \"current_url\", but will not need it because we have \"target_url\"\n",
    "    are_you_human_check()\n",
    "    \n",
    "    # target the new page's url for scraping\n",
    "    target_url = browser.url\n",
    "\n",
    "    # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "    response_target = requests.get(target_url)\n",
    "    #response\n",
    "\n",
    "    # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "    target_page_soup = soup(response_target.text, 'html.parser')\n",
    "    \n",
    "    containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "\n",
    "    # Execute webscraper function. Output is a csv file in the processing folder and dictionary.\n",
    "    newegg_page_scraper(containers, turn_page)\n",
    "\n",
    "    # Create instances of class objects of the laptops/notebooks using a list comprehension.\n",
    "    objects = [Laptops(**prod_obj) for prod_obj in scraped_dict]\n",
    "\n",
    "    # Append all of the objects to the main product_catalog list (List of List of Objects).\n",
    "    product_catalog.append(objects)\n",
    "\n",
    "    print(\"Will scrape pages, but will need to randomly sleep for max 35 seconds to emulate human behavior. \")\n",
    "    try:\n",
    "        y = random.randint(1, 5)\n",
    "        print(f\"{turn_page}) | SLEEPING FOR {y} SECONDS\")\n",
    "        time.sleep(y)\n",
    "        random_a_tag_mouse_over()\n",
    "        random_xpath_top_bottom()\n",
    "        \n",
    "    except:\n",
    "        y = random.randint(3, 5)\n",
    "        print(f\"{turn_page}) | SLEEPING FOR {y} SECONDS - EXCEPTION\")\n",
    "        time.sleep(y)\n",
    "        random_a_tag_mouse_over()\n",
    "        random_xpath_top_bottom()\n",
    "        time.sleep(1)\n",
    "\n",
    "# Exit the broswer once complete webscraping\n",
    "browser.quit()\n",
    "\n",
    "# Prompt the user if they would like to concatenate all of the pages into one csv file\n",
    "concat_y_n = input(f'All {total_results_pages} pages have been saved in the \"processing\" folder (1 page = csv files). Would you like for us concatenate all the files into one? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "\n",
    "if concat_y_n == 'y':\n",
    "    concatenate(total_results_pages)\n",
    "    print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{pdt_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "\n",
    "# Prompt the user to if they would like to clear out processing folder function here - as delete everything to prevent clutter\n",
    "clear_processing_y_n = input(f'The \"processing\" folder has {total_results_pages} csv files of each page that was scraped. Would you like to clear the files? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "if clear_processing_y_n == 'y':\n",
    "    clean_processing_fldr()\n",
    "\n",
    "print('Thank you checking out my project, and hope you found this useful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/html/body/div[5]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(browser.find_by_tag('a')[ninety_pct_a_tag].__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(browser.find_by_tag('a')[ninety_pct_a_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimipopupsweepstakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser.find_by_tag('button')#[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(current_page_soup.find_all(\"div\", class_=\"item-container\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id=\"rc-imageselect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser.is_element_present_by_id(\"btn_InnerSearch\")\n",
    "#browser.is_element_present_by_id(\"rc-imageselect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(browser.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "browser.find_by_tag('body')[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(browser.find_by_tag('body'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag('a')[1958].click()#['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_a_tags = len(browser.find_by_tag('a'))\n",
    "ninety_pct_a_tag = round((number_of_a_tags * .902))\n",
    "ninety_pct_a_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag('a')[1962].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag('a')[1970].mouse_over()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag('h1')[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*[@id=\"btn_InnerSearch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.is_element_present_by_id('g-recaptcha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.is_element_present_by_id('mimipopupsweepstakes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_tag('a')[2263].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.is_element_present_by_id('sweepstakespopup')\n",
    "/html/body/div[16]/div/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.is_element_present_by_id('sweepstakespopup_close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_id('sweepstakespopup_close').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser.find_by_xpath('/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[4]/div/div/div[10]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_page_soup.is_element_present_by_xpath(\"/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_element_present_by_xpath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to view objects that were created from the data we collected\n",
    "#product_catalog[1][0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/html/body/div[4]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/html/body/div[4]/section/div/div/div[2]/div/div/div[2]/div[2]/div[1]/div[2]/div[4]/div/div/div[11]/button/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "prices = []\n",
    "for con in containers:\n",
    "    counter += 1\n",
    "    try:\n",
    "        #print(str(counter) + \" | \" + con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', ''))\n",
    "        price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "        prices.append(price)\n",
    "    except:\n",
    "        #print(str(counter) + \" | \" + \"null\")\n",
    "        \n",
    "        price = 'null'\n",
    "        prices.append(price)\n",
    "        \n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
