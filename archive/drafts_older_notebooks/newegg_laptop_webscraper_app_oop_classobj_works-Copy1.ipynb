{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from re import search\n",
    "import copy # might not need this\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import random\n",
    "#import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dt():\n",
    "    global current_date\n",
    "    current_date = str(datetime.datetime.now()).replace(':','.').replace(' ','_')[:-7]\n",
    "    return current_date\n",
    "#return_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of class objects:\n",
    "\n",
    "product_catalog = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this below to pull single items and test parts of the scraper and the head on off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " executable_path = {'executable_path': './chromedriver.exe'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " url = input(\"Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2)\n",
    "\n",
    "# Function to ask users if they want to watch the Bot (headless = False) work OR not (headless = True)\n",
    "# Lastly, will take you directly to the webpage that was inputted\n",
    "head = ''\n",
    "browser =''\n",
    "def head_on_off(executable_path):\n",
    "    # Have moved two preset variables, head and browser that are both \" = '' \"\n",
    "    # assigning these as global variables enable us to reference them outside and inside the function\n",
    "    global head\n",
    "    global browser\n",
    "    # options creates a bound to an answer\n",
    "    options = [1, 2]\n",
    "    #executable_path = {'executable_path': './chromedriver.exe'}\n",
    "    # for all cases where users input in a value that is not valid\n",
    "    while head not in options:\n",
    "        head = int(input('Do you want the desktop watch the bot work? Enter a number: 1 - YES | 2 - NO . Your Answer: '))\n",
    "        if head not in options:\n",
    "            print(\"That was not a valid answer. Please try again. \")\n",
    "    # For cases where users enter in valid options:\n",
    "    if head == options[0]:\n",
    "        print('Head is activated. Please view only the new automated Google Chrome web browser. ')\n",
    "        print('Do not make any adjustments to this automated window while the program runs, as it may produce errors or undesired outputs. ')\n",
    "        browser = Browser('chrome', **executable_path, headless=False)\n",
    "    if head == options[1]:\n",
    "        print('Headless mode activated. No web browser will pop up. Please proceeed. ')\n",
    "        browser = Browser('chrome', **executable_path, headless=True)\n",
    "    # visit the target site\n",
    "    browser.visit(url)\n",
    "    global current_url\n",
    "    current_url = browser.url\n",
    "    #print(current_url)\n",
    "    return current_url\n",
    "\n",
    "#head_on_off(executable_path)\n",
    "#time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3)\n",
    "\n",
    "# Use Splinter to grab the current url, to setup request to pull URL\n",
    "#current_url = browser.url #+ '&Page=' #+ str(turn_page)\n",
    "\n",
    "# Use Request.get() to pull the current url\n",
    "current_url = browser.url\n",
    "print(current_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(current_url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4)\n",
    "\n",
    "# Use BeautifulSoup to grab all the HTML using the htmlparser\n",
    "current_page_soup = soup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_page_soup.find_all('a', class_=\"item-title\")[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_soup.find_all('div', class_=\"nav-x-body-top-bar fix\")[0].text.split('\\n')[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_soup.find_all('h1', class_=\"page-title-text\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_page_soup.find_all(\"a\", class_=\"item-title\")[0].text\n",
    "#current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "\n",
    "#example = current_page_soup.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "\n",
    "try:\n",
    "    example1 = current_page_soup.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "    print(\"example 1\")\n",
    "    print(example1)\n",
    "except (IndexError) as e:\n",
    "    example2 = current_page_soup.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]\n",
    "    print(\"example 2\")\n",
    "    print(example2)\n",
    "#bool(example.split('?'))\n",
    "\n",
    "# if bool(example.split('?')) == True:\n",
    "#     #example.split('?')\n",
    "    \n",
    "# if re.search('?', example) == True:\n",
    "#     print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.split('?')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_soup.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 5) Are there scrappable items-contrainers on the page? List first, last and count, also how many pages\n",
    "\n",
    "def scrappable_y_n(current_page_soup):\n",
    "    global containers\n",
    "    containers = current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "    \n",
    "    # print first and last objects so users can understand what the output will be\n",
    "    print(\"Preview: expect these scrapped off this page, and for every other total results pages, if there's more than one: \")\n",
    "    print(\"=\"*35)\n",
    "    # max items should be 36\n",
    "    counter = 0\n",
    "    for con in containers:\n",
    "        try:\n",
    "            counter += 1\n",
    "            product_details = con.find_all(\"a\", class_=\"item-title\")[0].text\n",
    "            product_price = con.find_all(\"li\", class_=\"price-current\")[0].text.split()[0]\n",
    "            print(f'{counter}) {product_details} | Price: {product_price}')\n",
    "            print(\"-\"*35)\n",
    "            \n",
    "        except (IndexError) as e:\n",
    "            print(f\"{counter}) This item was not scrappable. Skipped. \")\n",
    "            print(\"-\"*35)\n",
    "            \n",
    "    print(\"=\"*60)\n",
    "    if counter == 0:\n",
    "        print(\"Unable to scrap this link. \")\n",
    "    else:\n",
    "        print(f\"{len(containers)} Scrappable Objects on the page. \")\n",
    "    #return current_page_soup\n",
    "#scrappable_y_n(current_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic classes here and then have the function create product objects AND export out to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product_catalog:\n",
    "    \n",
    "    all_prod_count = 0\n",
    "    \n",
    "    def __init__(self, general_category): # computer systems\n",
    "        self.general_category = general_category\n",
    "        \n",
    "        Product_catalog.all_prod_count += 1\n",
    "        \n",
    "    def count_prod(self):\n",
    "        return int(self.all_prod_count)\n",
    "        #return '{}'.format(self.general_category)\n",
    "        \n",
    "class Sub_category(Product_catalog): # laptops/notebooks, gaming\n",
    "    \n",
    "    sub_category_ct = 0\n",
    "    \n",
    "    def __init__(self, general_category, sub_categ, item_num, brand, price, img_link, prod_link, model_specifications, current_promotions):\n",
    "        super().__init__(general_category)\n",
    "        Sub_category.sub_category_ct += 1\n",
    "        \n",
    "        self.sub_categ = sub_categ\n",
    "        self.item_num = item_num\n",
    "        self.brand = brand\n",
    "        self.price = price\n",
    "        self.img_link = img_link\n",
    "        self.prod_link = prod_link\n",
    "        self.model_specifications = model_specifications\n",
    "        self.current_promotions = current_promotions\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CREATING OBJECTS AND IT WORKS\n",
    "\n",
    "Sub_category(\n",
    "\"Computer_Systems\",\n",
    "\"Laptops/Notebooks\",\n",
    "'Item=9SIA7AB8D73120',\n",
    "\"HP\",\n",
    "1449.00, \"//c1.neweggimages.com/ProductImageCompressAll300/A7AB_1_201811092013621813.jpg\",\n",
    "'https://www.newegg.com/p/1TS-000D-032G0?Item=9SIA7AB8D73120',\n",
    "                      \n",
    "'HP EliteBook 840 G5 Premium School and Business Laptop (Intel 8th Gen i7-8550U Quad-Core, 16GB RAM, 256GB PCIe SSD, 14\" FHD 1920x1080 Sure View Display, Thunderbolt3, NFC, Fingerprint, Win 10 Pro)',   \n",
    "'Free Expedited Shipping')\n",
    "\n",
    "\n",
    "Sub_category(\n",
    "\"Computer_Systems\",\n",
    "\"Laptops/Notebooks\",\n",
    "'Item=0000000000000',\n",
    "\"HP\",\n",
    "1449.00, \"//c1.neweggimages.com/ProductImageCompressAll300/A7AB_1_201811092013621813.jpg\",\n",
    "'https://www.newegg.com/p/1TS-000D-032G0?Item=AKAKAKAKAKA',\n",
    "                      \n",
    "'HP EliteBook 840 G5 Premium School and Business Laptop (Intel 8th Gen i7-8550U Quad-Core, 16GB RAM, 256GB PCIe SSD, 14\" FHD 1920x1080 Sure View Display, Thunderbolt3, NFC, Fingerprint, Win 10 Pro)',   \n",
    "'Free Expedited Shipping')\n",
    "\n",
    "#Product_catalog.all_prod_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning purposes\n",
    "Sub_category.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Product_catalog.__dict__\n",
    "print(Product_catalog.count_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to pass in the Sub_category for it to know what to count\n",
    "Product_catalog.count_prod(Sub_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lptp_1.count_prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub_category.sub_category_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keys_list = list(lptp_1.__dict__.keys())\n",
    "keys_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lptp_1.__dict__.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values_list = list(lptp_1.__dict__.values())\n",
    "dict_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    'keys': keys_list,\n",
    "    'values': dict_values_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# handy trick - to see all data from the object.\n",
    "#print(prod_1.__dict__)\n",
    "#prod_1\n",
    "\n",
    "lptp_1 = sub_category(\"Computer_Systems\", Product_catalog.all_prod_count,\"Laptops/Notebooks\", sub_category.sub_cat_ct, \"HP\", 1449.00, \"//c1.neweggimages.com/ProductImageCompressAll300/A7AB_1_201811092013621813.jpg\", 'https://www.newegg.com/p/1TS-000D-032G0?Item=9SIA7AB8D73120',\n",
    "'HP EliteBook 840 G5 Premium School and Business Laptop (Intel 8th Gen i7-8550U Quad-Core, 16GB RAM, 256GB PCIe SSD, 14\" FHD 1920x1080 Sure View Display, Thunderbolt3, NFC, Fingerprint, Win 10 Pro)', 'Free Expedited Shipping')\n",
    "lptp_1.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_2.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKIP THIS ONE - NEXT ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reenabled item_number\n",
    "def newegg_page_scraper(containers, turn_page): #before: (containers, turn_page)\n",
    "    images = []\n",
    "    product_brands = []\n",
    "    product_models = []\n",
    "    product_links = []\n",
    "    item_numbers = []\n",
    "    product_categories = []\n",
    "    promotions = []\n",
    "    prices = []\n",
    "    shipping_terms = []\n",
    "    page_nums = []\n",
    "\n",
    "\n",
    "    for con in containers:\n",
    "        try:\n",
    "            page_counter = turn_page\n",
    "            page_nums.append(int(turn_page))\n",
    "            \n",
    "            image = con.a.img[\"src\"]\n",
    "            #print(image)\n",
    "            images.append(image)\n",
    "\n",
    "            prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "            product_models.append(prd_title)\n",
    "\n",
    "            product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "            product_links.append(product_link)\n",
    "            \n",
    "            shipping = con.find_all('li', class_='price-ship')[0].text.strip().split()[0]\n",
    "            if shipping != \"Free\":\n",
    "                shipping = shipping.replace('$', '')\n",
    "                shipping_terms.append(shipping)\n",
    "            else:\n",
    "                shipping = 0.00\n",
    "                shipping_terms.append(shipping)\n",
    "\n",
    "            brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "            product_brands.append(brand_name)\n",
    "\n",
    "        except (IndexError, ValueError) as e:\n",
    "            # if there's no item_brand container, take the Brand from product details\n",
    "            product_brands.append(con.find_all('a', class_=\"item-title\")[0].text.split()[0])\n",
    "            #print(f\"{e} block 1\")\n",
    "\n",
    "        try:\n",
    "            current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "            promotions.append(current_promo)\n",
    "        except:\n",
    "            promotions.append('null')\n",
    "            #print(f\"{e} block 2\")\n",
    "        try:\n",
    "            price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "            prices.append(price)\n",
    "        except (IndexError, ValueError) as e:\n",
    "            prices.append('null')\n",
    "            #print(f\"{e} block 3\")\n",
    "        \n",
    "        try:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "            item_numbers.append(item_num)\n",
    "        except (IndexError) as e:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]\n",
    "            item_numbers.append(item_num)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "    'item_number': item_numbers,\n",
    "    'brand': product_brands,\n",
    "    'model_specifications': product_models,\n",
    "    'price': prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': shipping_terms,\n",
    "    'page_number': page_nums,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "    df['general_category'] = current_page_soup.find_all('div', class_=\"nav-x-body-top-bar fix\")[0].text.split('\\n')[5]\n",
    "    df['product_category'] = current_page_soup.find_all('h1', class_=\"page-title-text\")[0].text\n",
    "    # rearrange columns\n",
    "    df = df[['item_number', 'general_category','product_category', 'page_number' ,'brand','model_specifications' ,'current_promotions' ,'price' ,'shipping' ,'product_links','image_link']]\n",
    "    global product_category\n",
    "    product_category = df['product_category'].unique()[0]\n",
    "    # eliminate special characters in a string if it exists\n",
    "    product_category = ''.join(e for e in product_category if e.isalnum())\n",
    "    \n",
    "    #return_list.append(product_category)\n",
    "    global items_scraped\n",
    "    items_scraped = len(df['model_specifications'])\n",
    "\n",
    "    \n",
    "    df.to_csv(f'./processing/{current_date}_{product_category}_{items_scraped}_scraped_page{turn_page}.csv')\n",
    "    return items_scraped, product_category\n",
    "    \n",
    "#df.head()\n",
    "#newegg_page_scraper(containers, turn_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(containers[1].find_all('a', class_=\"item-brand\")[0].img[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST VERSION FOR CLASSES #############################\n",
    "def newegg_page_scraper(containers, turn_page): #before: (containers, turn_page)\n",
    "    page_nums = []\n",
    "    general_category = []\n",
    "    product_categories = []\n",
    "    images = []\n",
    "    product_brands = []\n",
    "    product_models = []\n",
    "    product_links = []\n",
    "    item_numbers = []\n",
    "    promotions = []\n",
    "    prices = []\n",
    "    shipping_terms = []\n",
    "    \n",
    "    global gen_category\n",
    "    \n",
    "    for con in containers:\n",
    "        try:\n",
    "            page_counter = turn_page\n",
    "            page_nums.append(int(turn_page))\n",
    "            print(page_counter)\n",
    "            \n",
    "            gen_category = target_page_soup.find_all('div', class_=\"nav-x-body-top-bar fix\")[0].text.split('\\n')[5]\n",
    "            general_category.append(gen_category)\n",
    "            print(gen_category)\n",
    "            \n",
    "            prod_category = target_page_soup.find_all('h1', class_=\"page-title-text\")[0].text\n",
    "            product_categories.append(prod_category)\n",
    "            print(prod_category)\n",
    "            \n",
    "            prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "            product_models.append(prd_title)\n",
    "            print(prd_title)\n",
    "\n",
    "            product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "            product_links.append(product_link)\n",
    "            print(product_link)\n",
    "            \n",
    "            shipping = con.find_all('li', class_='price-ship')[0].text.strip().split()[0]\n",
    "            if shipping != \"Free\":\n",
    "                shipping = shipping.replace('$', '')\n",
    "                shipping_terms.append(shipping)\n",
    "                print(shipping)\n",
    "            else:\n",
    "                shipping = 0.00\n",
    "                shipping_terms.append(shipping)\n",
    "                print(shipping)\n",
    "            brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "            product_brands.append(brand_name)\n",
    "            print(brand_name)\n",
    "\n",
    "        except (IndexError, TypeError, ValueError) as e:\n",
    "            # if there's no item_brand container, take the Brand from product details\n",
    "            brand_name = con.find_all('a', class_=\"item-title\")[0].text.split()[0]\n",
    "            product_brands.append(brand_name)\n",
    "            print(brand_name)\n",
    "        finally:\n",
    "            brand_name = 'no brand data available / unable to scrape - refer to screenshots'\n",
    "            product_brands.append(brand_name)\n",
    "            \n",
    "        try:\n",
    "            current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "            promotions.append(current_promo)\n",
    "            print(current_promo)\n",
    "        except (IndexError, TypeError, ValueError) as e:\n",
    "            current_promo = 'no promotion / unable to scrape - refer to screenshots'\n",
    "            promotions.append(current_promo)\n",
    "            print(current_promo)\n",
    "            \n",
    "        try:\n",
    "            image = con.a.img[\"src\"]\n",
    "            images.append(image)\n",
    "            print(f\"{image} -- TRY IMAGE\")\n",
    "\n",
    "        except (IndexError, TypeError, ValueError) as e:\n",
    "            image = \"unable to scrape image / not available - refer to screenshots\"\n",
    "            images.append(image)\n",
    "            print(f\"{image} -- FINALLY IMAGE\")\n",
    "\n",
    "        try:\n",
    "            price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "            prices.append(price)\n",
    "            print(f\"{price}\")\n",
    "        except (IndexError, TypeError, ValueError) as e:\n",
    "            price = 'no promotion / unable to scrape - refer to screenshots'\n",
    "            prices.append(price)\n",
    "            print(f\"{price}\")\n",
    "        try:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1].split('?')[0]\n",
    "            item_numbers.append(item_num)\n",
    "            print(f\"{item_num}\")\n",
    "        except (IndexError, TypeError, ValueError) as e:\n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('p/')[1]\n",
    "            item_numbers.append(item_num)\n",
    "            print(f\"{item_num}\")\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "    'item_number': item_numbers,\n",
    "    'general_category': general_category,\n",
    "    'product_category': product_categories,\n",
    "    'brand': product_brands,\n",
    "    'model_specifications': product_models,\n",
    "    'price': prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': shipping_terms,\n",
    "    'page_number': page_nums,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "    \n",
    "    df = df[['item_number', 'general_category','product_category', 'page_number' ,'brand','model_specifications' ,'current_promotions' ,'price' ,'shipping' ,'product_links','image_link']]\n",
    "    \n",
    "    global scraped_dict\n",
    "    scraped_dict = df.to_dict('records')\n",
    "    \n",
    "    global pdt_category\n",
    "    pdt_category = df['product_category'].unique()[0]\n",
    "    # eliminate special characters in a string if it exists\n",
    "    pdt_category = ''.join(e for e in pdt_category if e.isalnum())\n",
    "    \n",
    "    global items_scraped\n",
    "    items_scraped = len(df['model_specifications'])\n",
    "    \n",
    "    df.to_csv(f'./processing/{current_date}_{pdt_category}_{items_scraped}_scraped_page{turn_page}.csv')\n",
    "    return scraped_dict, items_scraped, pdt_category\n",
    "    \n",
    "#df.head()\n",
    "#newegg_page_scraper(containers, turn_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "\n",
    "# create a function to return results pages, if exists, otherwise just scrape one page\n",
    "def results_pages(current_page_soup):\n",
    "    # Use BeautifulSoup to extract the total results page number\n",
    "    results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "    #print(results_pages)\n",
    "    # Find and extract total pages + and add 1 to ensure proper length of total pages\n",
    "    global total_results_pages\n",
    "    total_results_pages = int(re.split(\"/\", results_pages)[1]) + 2 # need to add 2 b/c 'range(inclusive, exclusive)'\n",
    "    #========================================= need to remember to +2, and remove -30\n",
    "    #print(total_results_pages)\n",
    "    \n",
    "    return total_results_pages\n",
    "#results_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working\n",
    "def concatenate(total_results_pages):\n",
    "    \n",
    "    path = f'./processing\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    concatenate_pages = []\n",
    "    counter = 0\n",
    "    for page in scraped_pages:\n",
    "        df = pd.read_csv(page, index_col=0, header=0)\n",
    "        concatenate_pages.append(df)\n",
    "\n",
    "    compiled_data = pd.concat(concatenate_pages, axis=0, ignore_index=True)\n",
    "    total_items_scraped = len(compiled_data['brand']) # can replace this counter by creating class objects everytime it scrapes\n",
    "    concatenated_output = compiled_data.to_csv(f\"./finished_outputs/{current_date}_{total_items_scraped}_scraped_{total_results_pages}_pages_.csv\")\n",
    "    return concatenated_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_results_pages = 4\n",
    "# concatenate(total_results_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTRUCT CLASSES HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis is working\n",
    "def clean_processing_fldr():\n",
    "    # delete all files in the 'processing folder'\n",
    "    path = f'./processing\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    if len(scraped_pages) < 1:\n",
    "        print(\"There are no files in the folder to clear. \")\n",
    "    else:\n",
    "        print(f\"Clearing out a total of {len(scraped_pages)} scraped pages in the processing folder... \")\n",
    "        clear_processing_files = []\n",
    "        for page in scraped_pages:\n",
    "            os.remove(page)\n",
    "        \n",
    "    print('Clearing of \"Processing\" folder complete. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# probably won't use these but good practice of fundamentals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# webscrape first page, then run page turner, then scraper for every page thereafter\n",
    "# \n",
    "\n",
    "path = f'./finished_outputs\\\\'\n",
    "finished_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "dict = {}\n",
    "counter = 0\n",
    "for file in finished_files:\n",
    "    counter += 1\n",
    "    key = str(counter)\n",
    "    file = file[19:]\n",
    "    dict[key] = file\n",
    "    print(str(counter) + \") \" + file )\n",
    "    \n",
    "select = input(\"Which file would you like to read? Enter in the number. \")\n",
    "\n",
    "\n",
    "\n",
    "#with open(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not plan on using this... Save for later\n",
    "\n",
    "results = []\n",
    "for num in range(counter):\n",
    "    results.append(num)\n",
    "\n",
    "correct_selection = False\n",
    "while correct_selection == False:\n",
    "    \n",
    "    select = input(\"Which file would you like to read? Enter in the number. \")\n",
    "    \n",
    "    if int(select) not in results:\n",
    "        print(\"That was not a valid selection\")\n",
    "        correct_selection = False\n",
    "    else:\n",
    "        print(\"Processing now. \")\n",
    "        correct_selection = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold off on using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning lesson is you can't call a function within a function\n",
    "\n",
    "def page_turner(total_results_pages):\n",
    "    # This is \"NEXT PAGE BUTTON CLICK\" - This loops thru the total amount of pages by clicking the next page button\n",
    "    for turn_page in range(1, total_results_pages):\n",
    "        # set the current url as the target page (aiming the boomerang)\n",
    "        target_url = browser.url\n",
    "\n",
    "        # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "        response_target = requests.get(target_url)\n",
    "        #response\n",
    "\n",
    "        # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "        target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "        # Use BeautifulSoup to extract the total results page number\n",
    "        #results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "        \n",
    "        results_pages = target_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "        #=========================================================\n",
    "        containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "        \n",
    "        newegg_page_scraper(containers, turn_page)\n",
    "        \n",
    "        #for i in range(total_results_pages):\n",
    "        x = random.randint(3, 25)\n",
    "        print(f\"{turn_page}) | SLEEPING FOR SECONDS: {x} \")\n",
    "        time.sleep(x)\n",
    "            \n",
    "        browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "\n",
    "    browser.quit()\n",
    "# concatenate(total_results_pages)\n",
    "# clean_processing_fldr()\n",
    "# # clear out processing folder function here - as delete everything to prevent clutter\n",
    "\n",
    "\n",
    "# print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{product_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "# print('Thank you and hope you found this useful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume below - just skip the top function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sub_category:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_again = True\n",
    "while scrape_again == True:\n",
    "    return_dt()\n",
    "    print(\"=== NewEgg.Com WebScraper Beta ===\")\n",
    "    print(\"==\"*30)\n",
    "    print(f\"Date: {current_date}\")\n",
    "    print(\"\")\n",
    "    print(\"Instructions:\")\n",
    "    print(\"(1) Go to www.newegg.com, search for your laptop requirements (e.g. brand and specifications) \")\n",
    "    print(\"(2) Copy and paste the url from your exact search \")\n",
    "    print('(3) Activate or Disable the \"Head View\", webscraper bots point of view ')\n",
    "    print('(4) Check the \"final_output folder when the webscraper bot is done scraping \"')\n",
    "    print(\"\")\n",
    "\n",
    "    executable_path = {'executable_path': './chromedriver.exe'}\n",
    "    url = input(\"Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: \")\n",
    "    head = ''\n",
    "    browser =''\n",
    "    head_on_off(executable_path)\n",
    "    response = requests.get(current_url)\n",
    "    #response\n",
    "\n",
    "    current_page_soup = soup(response.text, 'html.parser')\n",
    "    current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "    scrappable_y_n(current_page_soup)\n",
    "    ######################################\n",
    "    # # Are there any pop ups / safe to proceed?\n",
    "    safe_proceed_y_n = input(f'The Break Pedal: Answer any robot queries by NewEgg. Enter \"y\" when you are ready to proceed. ')\n",
    "    if safe_proceed_y_n == 'y':\n",
    "        print(f'Proceeding with webscrape... ')\n",
    "    else:\n",
    "        print(\"Quitting browser. You will need to press ctrl + c to quit, and then restart the program to try again. \")\n",
    "        browser.quit()\n",
    "    # ################\n",
    "\n",
    "\n",
    "    #newegg_page_scraper(containers)\n",
    "\n",
    "    # will need to UNCOMMENT AFTER\n",
    "    results_pages(current_page_soup)\n",
    "\n",
    "    #page_turner(total_results_pages)\n",
    "\n",
    "    #total_results_pages = 5\n",
    "    for turn_page in range(1, total_results_pages):\n",
    "        # set the current url as the target page (aiming the boomerang)\n",
    "        target_url = browser.url\n",
    "\n",
    "        # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "        response_target = requests.get(target_url)\n",
    "        #response\n",
    "\n",
    "        # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "        target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "        # Use BeautifulSoup to extract the total results page number\n",
    "        #results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "\n",
    "        results_pages = target_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "        #=========================================================\n",
    "        containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "\n",
    "        newegg_page_scraper(containers, turn_page)\n",
    "        screenshot(f\"./screenshots/{current_date}_{items_scraped}_items_{pdt_category}_page_{turn_page}\", suffix=\".png\", full=True)\n",
    "        objects = [Sub_category(**i) for i in scraped_dict]\n",
    "        product_catalog.append(objects)\n",
    "\n",
    "        x = random.randint(3, 25)\n",
    "        print(\"Emulating Human Behavior\")\n",
    "        print(f\"{turn_page}) | SLEEPING FOR {x} SECONDS \")\n",
    "        time.sleep(x)\n",
    "        \n",
    "        browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "\n",
    "    ##################################################################\n",
    "    concat_y_n = input(f'All {total_results_pages} pages have been saved in the \"processing\" folder (1 page = csv files). Would you like for us concatenate all the files into one? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "    if concat_y_n == 'y':\n",
    "        concatenate(total_results_pages)\n",
    "        print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{pdt_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "    # temporarily changed product_category to \"pdt_category\"\n",
    "\n",
    "\n",
    "    # clear out processing folder function here - as delete everything to prevent clutter\n",
    "    clear_processing_y_n = input(f'The \"processing\" folder has {total_results_pages} csv files of each page that was scraped. Would you like to clear the files? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "    if clear_processing_y_n == 'y':\n",
    "        clean_processing_fldr()\n",
    "\n",
    "\n",
    "    scrape_again = input(\"Would you like to scrape again? Enter in 'y' or 'n'. \")\n",
    "    if scrape_again == 'y':\n",
    "        scrape_again = True\n",
    "    scrape_again = False\n",
    "    print('Thank you! Hope you found this useful. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_catalog[1][0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_page_soup.find_all(\"div\", class_=\"item-container\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_page_soup.find_all('a', class_=\"item-title\")[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_category = target_page_soup.find_all('div', class_=\"nav-x-body-top-bar fix\")[0].text.split('\\n')[5]\n",
    "            general_category.append(gen_category)\n",
    "            \n",
    "            prod_category = target_page_soup.find_all('h1', class_=\"page-title-text\")[0].text\n",
    "            product_categories.append(prod_category)\n",
    "            \n",
    "            prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "            product_models.append(prd_title)\n",
    "\n",
    "            product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "            product_links.append(product_link)\n",
    "            \n",
    "            shipping = con.find_all('li', class_='price-ship')[0].text.strip().split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_page_soup.img.a.[\"src\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kon = current_page_soup.find_all(\"div\", class_=\"item-container\")[0]\n",
    "kon.img['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
