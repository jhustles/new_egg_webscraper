{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from re import search\n",
    "import copy # might not need this\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import random\n",
    "#import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dt():\n",
    "    global current_date\n",
    "    current_date = str(datetime.datetime.now()).replace(':','.').replace(' ','_')[:-7]\n",
    "    return current_date\n",
    "#return_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executable_path = {'executable_path': './chromedriver.exe'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = input(\"Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2)\n",
    "\n",
    "# Function to ask users if they want to watch the Bot (headless = False) work OR not (headless = True)\n",
    "# Lastly, will take you directly to the webpage that was inputted\n",
    "head = ''\n",
    "browser =''\n",
    "def head_on_off(executable_path):\n",
    "    # Have moved two preset variables, head and browser that are both \" = '' \"\n",
    "    # assigning these as global variables enable us to reference them outside and inside the function\n",
    "    global head\n",
    "    global browser\n",
    "    # options creates a bound to an answer\n",
    "    options = [1, 2]\n",
    "    #executable_path = {'executable_path': './chromedriver.exe'}\n",
    "    # for all cases where users input in a value that is not valid\n",
    "    while head not in options:\n",
    "        head = int(input('Do you want the desktop watch the bot work? Enter a number: 1 - YES | 2 - NO . Your Answer: '))\n",
    "        if head not in options:\n",
    "            print(\"That was not a valid answer. Please try again. \")\n",
    "    # For cases where users enter in valid options:\n",
    "    if head == options[0]:\n",
    "        print('Head is activated. Please view only the new automated Google Chrome web browser. ')\n",
    "        print('Do not make any adjustments to this automated window while the program runs, as it may produce errors or undesired outputs. ')\n",
    "        browser = Browser('chrome', **executable_path, headless=False)\n",
    "    if head == options[1]:\n",
    "        print('Headless mode activated. No web browser will pop up. Please proceeed. ')\n",
    "        browser = Browser('chrome', **executable_path, headless=True)\n",
    "    # visit the target site\n",
    "    browser.visit(url)\n",
    "    global current_url\n",
    "    current_url = browser.url\n",
    "    #print(current_url)\n",
    "    return current_url\n",
    "\n",
    "#head_on_off(executable_path)\n",
    "#time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3)\n",
    "\n",
    "# Use Splinter to grab the current url, to setup request to pull URL\n",
    "#current_url = browser.url #+ '&Page=' #+ str(turn_page)\n",
    "\n",
    "# Use Request.get() to pull the current url\n",
    "response = requests.get(current_url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4)\n",
    "\n",
    "# Use BeautifulSoup to grab all the HTML using the htmlparser\n",
    "current_page_soup = soup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_page_soup.find_all(\"a\", class_=\"item-title\")[0].text\n",
    "current_page_soup.find_all(\"div\", class_=\"item-container\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 5) Are there scrappable items-contrainers on the page? List first, last and count, also how many pages\n",
    "\n",
    "def scrappable_y_n(current_page_soup):\n",
    "    global containers\n",
    "    containers = current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "    \n",
    "    # print first and last objects so users can understand what the output will be\n",
    "    print(\"Preview: expect these scrapped off this page, and for every other total results pages, if there's more than one: \")\n",
    "    print(\"=\"*35)\n",
    "    # max items should be 36\n",
    "    counter = 0\n",
    "    for con in containers:\n",
    "        try:\n",
    "            counter += 1\n",
    "            product_details = con.find_all(\"a\", class_=\"item-title\")[0].text\n",
    "            product_price = con.find_all(\"li\", class_=\"price-current\")[0].text.split()[0]\n",
    "            print(f'{counter}) {product_details} | Price: {product_price}')\n",
    "            print(\"-\"*35)\n",
    "            \n",
    "        except (IndexError) as e:\n",
    "            print(f\"{counter}) This item was not scrappable. Skipped. \")\n",
    "            print(\"-\"*35)\n",
    "            \n",
    "    print(\"=\"*60)\n",
    "    if counter == 0:\n",
    "        print(\"Unable to scrap this link. \")\n",
    "    else:\n",
    "        print(f\"{len(containers)} Scrappable Objects on the page. \")\n",
    "    #return containers\n",
    "#scrappable_y_n(current_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic classes here and then have the function create product objects AND export out to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product_catalog:\n",
    "    \n",
    "    all_prod_count = 0\n",
    "    \n",
    "    def __init__(self, general_category): # computer systems\n",
    "        self.general_category = general_category\n",
    "        \n",
    "        Product_catalog.all_prod_count += 1\n",
    "        \n",
    "    def count_prod(self):\n",
    "        return int(self.all_prod_count)\n",
    "        #return '{}'.format(self.general_category)\n",
    "        \n",
    "class Sub_category(Product_catalog): # laptops/notebooks, gaming\n",
    "    \n",
    "    def __init__(self, general_category, sub_categ, item_num, brand, price, img_link, prod_link, model_specifications, current_promotions):\n",
    "        super().__init__(general_category)\n",
    "        \n",
    "        self.sub_categ = sub_categ\n",
    "        self.item_num = item_num\n",
    "        self.brand = brand\n",
    "        self.price = price\n",
    "        self.img_link = img_link\n",
    "        self.prod_link = prod_link\n",
    "        self.model_specifications = model_specifications\n",
    "        self.current_promotions = current_promotions\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lptp_1 = Sub_category(\n",
    "\"Computer_Systems\",\n",
    "\"Laptops/Notebooks\",\n",
    "'Item=9SIA7AB8D73120',\n",
    "\"HP\",\n",
    "1449.00, \"//c1.neweggimages.com/ProductImageCompressAll300/A7AB_1_201811092013621813.jpg\",\n",
    "'https://www.newegg.com/p/1TS-000D-032G0?Item=9SIA7AB8D73120',\n",
    "                      \n",
    "'HP EliteBook 840 G5 Premium School and Business Laptop (Intel 8th Gen i7-8550U Quad-Core, 16GB RAM, 256GB PCIe SSD, 14\" FHD 1920x1080 Sure View Display, Thunderbolt3, NFC, Fingerprint, Win 10 Pro)',   \n",
    "'Free Expedited Shipping')\n",
    "\n",
    "\n",
    "#Product_catalog.all_prod_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning purposes\n",
    "Sub_category.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Product_catalog.__dict__\n",
    "print(Product_catalog.count_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Product_catalog.count_prod(lptp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lptp_1.count_prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you must put in \"Product_catalog.prod_count\" if you want to use the counter in the class\n",
    "# prod_1 = Product_catalog(\"Computer_Systems\",\"Product_catalog.all_prod_count\",\"Laptops/Notebooks\", sub_category.sub_cat_ct, \"HP\", 1449.00, \"//c1.neweggimages.com/ProductImageCompressAll300/A7AB_1_201811092013621813.jpg\", 'https://www.newegg.com/p/1TS-000D-032G0?Item=9SIA7AB8D73120',\n",
    "# 'HP EliteBook 840 G5 Premium School and Business Laptop (Intel 8th Gen i7-8550U Quad-Core, 16GB RAM, 256GB PCIe SSD, 14\" FHD 1920x1080 Sure View Display, Thunderbolt3, NFC, Fingerprint, Win 10 Pro)', 'Free Expedited Shipping')\n",
    "\n",
    "#prod_2 = Product_catalog(\"Product_catalog.prod_count\",\"Laptops/Notebooks\", \"Lenovo\", 999.99, \"//c1.neweggimages.com/ProductImageCompressAll300/AA0S_1_20200212323015877.jpg\", \"https://www.newegg.com/silver-lenovo-thinkbook-14s/p/1TS-000E-0HM31?Item=9SIAA0SB0R0608\")\n",
    "\n",
    "# handy trick - to see all data from the object.\n",
    "#print(prod_1.__dict__)\n",
    "#prod_1\n",
    "\n",
    "lptp_1 = sub_category(\"Computer_Systems\", Product_catalog.all_prod_count,\"Laptops/Notebooks\", sub_category.sub_cat_ct, \"HP\", 1449.00, \"//c1.neweggimages.com/ProductImageCompressAll300/A7AB_1_201811092013621813.jpg\", 'https://www.newegg.com/p/1TS-000D-032G0?Item=9SIA7AB8D73120',\n",
    "'HP EliteBook 840 G5 Premium School and Business Laptop (Intel 8th Gen i7-8550U Quad-Core, 16GB RAM, 256GB PCIe SSD, 14\" FHD 1920x1080 Sure View Display, Thunderbolt3, NFC, Fingerprint, Win 10 Pro)', 'Free Expedited Shipping')\n",
    "lptp_1.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_2.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reenabled item_number\n",
    "def newegg_page_scraper(containers, turn_page): #before: (containers, turn_page)\n",
    "    \n",
    "    images = []\n",
    "    product_brands = []\n",
    "    product_models = []\n",
    "    product_links = []\n",
    "    item_numbers = []\n",
    "    product_categories = []\n",
    "    promotions = []\n",
    "    prices = []\n",
    "    shipping_terms = []\n",
    "    page_nums = []\n",
    "\n",
    "\n",
    "    for con in containers:\n",
    "        try:\n",
    "            page_counter = turn_page\n",
    "            page_nums.append(int(turn_page))\n",
    "\n",
    "            image = con.a.img[\"src\"]\n",
    "            #print(image)\n",
    "            images.append(image)\n",
    "\n",
    "            prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "            product_models.append(prd_title)\n",
    "\n",
    "            product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "            product_links.append(product_link)\n",
    "            \n",
    "            item_num = con.find_all('a', class_=\"item-title\")[0]['href'].split('?')[1].split('=')[1]\n",
    "            item_numbers.append(item_num)\n",
    "\n",
    "            shipping = con.find_all('li', class_='price-ship')[0].text.strip().split()[0]\n",
    "            if shipping != \"Free\":\n",
    "                shipping = shipping.replace('$', '')\n",
    "                shipping_terms.append(shipping)\n",
    "            else:\n",
    "                shipping = 0.00\n",
    "                shipping_terms.append(shipping)\n",
    "\n",
    "            brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "            product_brands.append(brand_name)\n",
    "\n",
    "        except (IndexError, ValueError) as e:\n",
    "            # if there's no item_brand container, take the Brand from product details\n",
    "            product_brands.append(con.find_all('a', class_=\"item-title\")[0].text.split()[0])\n",
    "            #print(f\"{e} block 1\")\n",
    "\n",
    "        try:\n",
    "            current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "            promotions.append(current_promo)\n",
    "        except:\n",
    "            promotions.append('null')\n",
    "            #print(f\"{e} block 2\")\n",
    "        try:\n",
    "            price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "            prices.append(price)\n",
    "        except (IndexError, ValueError) as e:\n",
    "            prices.append('null')\n",
    "            #print(f\"{e} block 3\")\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "    'item_number': item_numbers,\n",
    "    'brand': product_brands,\n",
    "    'model_specifications': product_models,\n",
    "    'price': prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': shipping_terms,\n",
    "    'page_number': page_nums,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "\n",
    "    df['product_category'] = current_page_soup.find_all('h1', class_=\"page-title-text\")[0].text\n",
    "    # rearrange columns\n",
    "    df = df[['item_number', 'product_category', 'page_number' ,'brand','model_specifications' ,'current_promotions' ,'price' ,'shipping' ,'product_links','image_link']]\n",
    "    global product_category\n",
    "    product_category = df['product_category'].unique()[0]\n",
    "    # eliminate special characters in a string if it exists\n",
    "    product_category = ''.join(e for e in product_category if e.isalnum())\n",
    "    \n",
    "    #return_list.append(product_category)\n",
    "    global items_scraped\n",
    "    items_scraped = len(df['model_specifications'])\n",
    "    \n",
    "    df.to_csv(f'./processing/{current_date}_{product_category}_{items_scraped}_scraped_page{turn_page}.csv')\n",
    "    return items_scraped, product_category\n",
    "    \n",
    "#df.head()\n",
    "#newegg_page_scraper(containers, turn_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(containers[1].find_all('a', class_=\"item-brand\")[0].img[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create function to create class objects\n",
    "\n",
    "    # use pd.read_csv\n",
    "    # loop thru df usint iterrows():\n",
    "    # assign each column as pieces that get assigned to each piece of the class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "\n",
    "# create a function to return results pages, if exists, otherwise just scrape one page\n",
    "def results_pages():\n",
    "    # Use BeautifulSoup to extract the total results page number\n",
    "    results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "    #print(results_pages)\n",
    "    # Find and extract total pages + and add 1 to ensure proper length of total pages\n",
    "    global total_results_pages\n",
    "    total_results_pages = int(re.split(\"/\", results_pages)[1]) #+ 2 # need to add 2 b/c 'range(inclusive, exclusive)'\n",
    "    #========================================= need to remember to +2, and remove -30\n",
    "    #print(total_results_pages)\n",
    "    total_results_pages = total_results_pages ## ### ## ###\n",
    "    \n",
    "    return total_results_pages\n",
    "#results_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working\n",
    "def concatenate(total_results_pages):\n",
    "    \n",
    "    path = f'./processing\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    concatenate_pages = []\n",
    "    counter = 0\n",
    "    for page in scraped_pages:\n",
    "        df = pd.read_csv(page, index_col=0, header=0)\n",
    "        concatenate_pages.append(df)\n",
    "\n",
    "    compiled_data = pd.concat(concatenate_pages, axis=0, ignore_index=True)\n",
    "    total_items_scraped = len(compiled_data['brand']) # can replace this counter by creating class objects everytime it scrapes\n",
    "    concatenated_output = compiled_data.to_csv(f\"./finished_outputs/{current_date}_{total_items_scraped}_scraped_{total_results_pages}_pages_.csv\")\n",
    "    return concatenated_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_results_pages = 4\n",
    "# concatenate(total_results_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis is working\n",
    "def clean_processing_fldr():\n",
    "    # delete all files in the 'processing folder'\n",
    "    path = f'./processing\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    if len(scraped_pages) < 1:\n",
    "        print(\"There are no files in the folder to clear. \")\n",
    "    else:\n",
    "        print(f\"Clearing out a total of {len(scraped_pages)} scraped pages in the processing folder... \")\n",
    "        clear_processing_files = []\n",
    "        for page in scraped_pages:\n",
    "            os.remove(page)\n",
    "        \n",
    "    print('Clearing of \"Processing\" folder complete. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webscrape first page, then run page turner, then scraper for every page thereafter\n",
    "# \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning lesson is you can't call a function within a function\n",
    "\n",
    "def page_turner(total_results_pages):\n",
    "    # This is \"NEXT PAGE BUTTON CLICK\" - This loops thru the total amount of pages by clicking the next page button\n",
    "    for turn_page in range(1, total_results_pages):\n",
    "        # set the current url as the target page (aiming the boomerang)\n",
    "        target_url = browser.url\n",
    "\n",
    "        # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "        response_target = requests.get(target_url)\n",
    "        #response\n",
    "\n",
    "        # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "        target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "        # Use BeautifulSoup to extract the total results page number\n",
    "        #results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "        \n",
    "        results_pages = target_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "        #=========================================================\n",
    "        containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "        \n",
    "        newegg_page_scraper(containers, turn_page)\n",
    "        \n",
    "        #for i in range(total_results_pages):\n",
    "        x = random.randint(3, 25)\n",
    "        print(f\"{turn_page}) | SLEEPING FOR SECONDS: {x} \")\n",
    "        time.sleep(x)\n",
    "            \n",
    "        browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "\n",
    "    browser.quit()\n",
    "# concatenate(total_results_pages)\n",
    "# clean_processing_fldr()\n",
    "# # clear out processing folder function here - as delete everything to prevent clutter\n",
    "\n",
    "\n",
    "# print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{product_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "# print('Thank you and hope you found this useful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_again = 'y'\n",
    "# while scrape_again =='y':\n",
    "#     print(\"=== NewEgg.Com WebScraper Beta ===\")\n",
    "#     print(\"==\"*30)\n",
    "#     print(\"Instructions:\")\n",
    "#     print(\"(1) Go to www.newegg.com, search for your laptop requirements (e.g. brand and specifications) \")\n",
    "#     print(\"(2) Copy and paste the url from your exact search \")\n",
    "#     print('(3) Activate or Disable the \"Head View\", webscraper bots point of view ')\n",
    "#     print('(4) Check the \"final_output folder when the webscraper bot is done scraping \"')\n",
    "#     print(\"\")\n",
    "# # NEED TO CREATE LAST INPUT TO LOOP THIS WHOLE THING AGAIN.\n",
    "# #Maybe use pandas to provide summary of data\n",
    "# #Export images of box and whisker plots using statistics by brand\n",
    "\n",
    "return_dt()\n",
    "\n",
    "executable_path = {'executable_path': './chromedriver.exe'}\n",
    "\n",
    "url = input(\"Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: \")\n",
    "head = ''\n",
    "browser =''\n",
    "head_on_off(executable_path)\n",
    "response = requests.get(current_url)\n",
    "#response\n",
    "\n",
    "current_page_soup = soup(response.text, 'html.parser')\n",
    "current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "scrappable_y_n(current_page_soup)\n",
    "\n",
    "# Are there any pop ups / safe to proceed?\n",
    "safe_proceed_y_n = input(f'The Break Pedal: Answer any robot queries by NewEgg. Enter \"y\" when you are ready to proceed. ')\n",
    "if safe_proceed_y_n == 'y':\n",
    "    print(f'Proceeding with webscrape... ')\n",
    "else:\n",
    "    print(\"Quitting browser. You will need to press ctrl + c to quit, and then restart the program to try again. \")\n",
    "    browser.quit()\n",
    "#newegg_page_scraper(containers)\n",
    "\n",
    "# will need to UNCOMMENT AFTER\n",
    "results_pages()\n",
    "\n",
    "#page_turner(total_results_pages)\n",
    "\n",
    "#total_results_pages = 5\n",
    "for turn_page in range(1, total_results_pages):\n",
    "    # set the current url as the target page (aiming the boomerang)\n",
    "    target_url = browser.url\n",
    "\n",
    "    # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "    response_target = requests.get(target_url)\n",
    "    #response\n",
    "\n",
    "    # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "    target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "    # Use BeautifulSoup to extract the total results page number\n",
    "    #results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "\n",
    "    results_pages = target_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "    #=========================================================\n",
    "    containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "\n",
    "    newegg_page_scraper(containers, turn_page)\n",
    "\n",
    "    #for i in range(total_results_pages):\n",
    "    x = random.randint(3, 25)\n",
    "    print(f\"{turn_page}) | SLEEPING FOR {x} SECONDS \")\n",
    "    time.sleep(x)\n",
    "\n",
    "    browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "##################################################################\n",
    "concat_y_n = input(f'All {total_results_pages} pages have been saved in the \"processing\" folder (1 page = csv files). Would you like for us concatenate all the files into one? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "if concat_y_n == 'y':\n",
    "    concatenate(total_results_pages)\n",
    "    print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{product_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "\n",
    "# clear out processing folder function here - as delete everything to prevent clutter\n",
    "clear_processing_y_n = input(f'The \"processing\" folder has {total_results_pages} csv files of each page that was scraped. Would you like to clear the files? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "if clear_processing_y_n == 'y':\n",
    "    clean_processing_fldr()\n",
    "\n",
    "\n",
    "print('Thank you and hope you found this useful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
