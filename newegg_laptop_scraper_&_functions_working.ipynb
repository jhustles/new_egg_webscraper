{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from re import search\n",
    "import copy # might not need this\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import random\n",
    "#import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dt():\n",
    "    global current_date\n",
    "    current_date = str(datetime.datetime.now()).replace(':','.').replace(' ','_')[:-7]\n",
    "    return current_date\n",
    "#return_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executable_path = {'executable_path': './chromedriver.exe'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = input(\"Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2)\n",
    "\n",
    "# Function to ask users if they want to watch the Bot (headless = False) work OR not (headless = True)\n",
    "# Lastly, will take you directly to the webpage that was inputted\n",
    "head = ''\n",
    "browser =''\n",
    "def head_on_off(executable_path):\n",
    "    # Have moved two preset variables, head and browser that are both \" = '' \"\n",
    "    # assigning these as global variables enable us to reference them outside and inside the function\n",
    "    global head\n",
    "    global browser\n",
    "    # options creates a bound to an answer\n",
    "    options = [1, 2]\n",
    "    #executable_path = {'executable_path': './chromedriver.exe'}\n",
    "    # for all cases where users input in a value that is not valid\n",
    "    while head not in options:\n",
    "        head = int(input('Do you want the desktop watch the bot work? Enter a number: 1 - YES | 2 - NO . Your Answer: '))\n",
    "        if head not in options:\n",
    "            print(\"That was not a valid answer. Please try again. \")\n",
    "    # For cases where users enter in valid options:\n",
    "    if head == options[0]:\n",
    "        print('Head is activated. Please view only the new automated Google Chrome web browser. ')\n",
    "        print('Do not make any adjustments to this automated window while the program runs, as it may produce errors or undesired outputs. ')\n",
    "        browser = Browser('chrome', **executable_path, headless=False)\n",
    "    if head == options[1]:\n",
    "        print('Headless mode activated. No web browser will pop up. Please proceeed. ')\n",
    "        browser = Browser('chrome', **executable_path, headless=True)\n",
    "    # visit the target site\n",
    "    browser.visit(url)\n",
    "    global current_url\n",
    "    current_url = browser.url\n",
    "    #print(current_url)\n",
    "    return current_url\n",
    "\n",
    "#head_on_off(executable_path)\n",
    "#time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3)\n",
    "\n",
    "# Use Splinter to grab the current url, to setup request to pull URL\n",
    "#current_url = browser.url #+ '&Page=' #+ str(turn_page)\n",
    "\n",
    "# Use Request.get() to pull the current url\n",
    "response = requests.get(current_url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4)\n",
    "\n",
    "# Use BeautifulSoup to grab all the HTML using the htmlparser\n",
    "current_page_soup = soup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_page_soup.find_all(\"a\", class_=\"item-title\")[0].text\n",
    "current_page_soup.find_all(\"div\", class_=\"item-container\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 5) Are there scrappable items-contrainers on the page? List first, last and count, also how many pages\n",
    "\n",
    "def scrappable_y_n(current_page_soup):\n",
    "    global containers\n",
    "    containers = current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "    \n",
    "    # print first and last objects so users can understand what the output will be\n",
    "    print(\"Here are the items that you can expect to be scrapped off this page, and for every other total results pages, if there's more than one: \")\n",
    "    print(\"=\"*35)\n",
    "    # max items should be 36\n",
    "    counter = 0\n",
    "    for con in containers:\n",
    "        try:\n",
    "            counter += 1\n",
    "            product_details = con.find_all(\"a\", class_=\"item-title\")[0].text\n",
    "            product_price = con.find_all(\"li\", class_=\"price-current\")[0].text.split()[0]\n",
    "            print(f'{counter}) {product_details} | Price: {product_price}')\n",
    "            print(\"-\"*35)\n",
    "            \n",
    "        except (IndexError) as e:\n",
    "            print(f\"{counter}) This item was not scrappable. Skipped. \")\n",
    "            print(\"-\"*35)\n",
    "            \n",
    "    print(\"=\"*60)\n",
    "    if counter == 0:\n",
    "        print(\"Unable to scrap this link. \")\n",
    "    else:\n",
    "        print(f\"{len(containers)} Scrappable Objects on the page. \")\n",
    "    #return containers\n",
    "#scrappable_y_n(current_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic classes here and then have the function create product objects AND export out to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newegg_page_scraper(containers, turn_page): #before: (containers, turn_page)\n",
    "    \n",
    "    images = []\n",
    "    product_brands = []\n",
    "    product_models = []\n",
    "    product_links = []\n",
    "    #item_numbers = []\n",
    "    product_categories = []\n",
    "    promotions = []\n",
    "    prices = []\n",
    "    shipping_terms = []\n",
    "    page_nums = []\n",
    "\n",
    "\n",
    "    for con in containers:\n",
    "        try:\n",
    "            page_counter = turn_page\n",
    "            page_nums.append(int(turn_page))\n",
    "\n",
    "            image = con.a.img[\"src\"]\n",
    "            #print(image)\n",
    "            images.append(image)\n",
    "\n",
    "            prd_title = con.find_all('a', class_=\"item-title\")[0].text\n",
    "            product_models.append(prd_title)\n",
    "\n",
    "            product_link = con.find_all('a', class_=\"item-title\")[0]['href']\n",
    "            product_links.append(product_link)\n",
    "\n",
    "            shipping = con.find_all('li', class_='price-ship')[0].text.strip().split()[0]\n",
    "            if shipping != \"Free\":\n",
    "                shipping = shipping.replace('$', '')\n",
    "                shipping_terms.append(shipping)\n",
    "            else:\n",
    "                shipping = 0.00\n",
    "                shipping_terms.append(shipping)\n",
    "\n",
    "            brand_name = con.find_all('a', class_=\"item-brand\")[0].img[\"title\"]\n",
    "            product_brands.append(brand_name)\n",
    "\n",
    "        except (IndexError, ValueError) as e:\n",
    "            # if there's no item_brand container, take the Brand from product details\n",
    "            product_brands.append(con.find_all('a', class_=\"item-title\")[0].text.split()[0])\n",
    "            #print(f\"{e} block 1\")\n",
    "\n",
    "        try:\n",
    "            current_promo = con.find_all(\"p\", class_=\"item-promo\")[0].text\n",
    "            promotions.append(current_promo)\n",
    "        except:\n",
    "            promotions.append('null')\n",
    "            #print(f\"{e} block 2\")\n",
    "        try:\n",
    "            price = con.find_all('li', class_=\"price-current\")[0].text.split()[0].replace('$','').replace(',', '')\n",
    "            prices.append(price)\n",
    "        except (IndexError, ValueError) as e:\n",
    "            prices.append('null')\n",
    "            #print(f\"{e} block 3\")\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "    #'item_number': item_numbers,\n",
    "    'brand': product_brands,\n",
    "    'model_specifications': product_models,\n",
    "    'price': prices,\n",
    "    'current_promotions': promotions,\n",
    "    'shipping': shipping_terms,\n",
    "    'page_number': page_nums,\n",
    "    'product_links': product_links,\n",
    "    'image_link': images\n",
    "    })\n",
    "\n",
    "    df['product_category'] = current_page_soup.find_all('h1', class_=\"page-title-text\")[0].text\n",
    "    # rearrange columns\n",
    "    df = df[['product_category' ,'brand','model_specifications' ,'current_promotions' ,'price' ,'shipping' ,'product_links','image_link']]\n",
    "    global product_category\n",
    "    product_category = df['product_category'].unique()[0]\n",
    "    # eliminate special characters in a string if it exists\n",
    "    product_category = ''.join(e for e in product_category if e.isalnum())\n",
    "    \n",
    "    #return_list.append(product_category)\n",
    "    global items_scraped\n",
    "    items_scraped = len(df['model_specifications'])\n",
    "    \n",
    "    df.to_csv(f'./processing/{current_date}_{product_category}_{items_scraped}_scraped_page{turn_page}.csv')\n",
    "    return items_scraped, product_category\n",
    "    \n",
    "#df.head()\n",
    "#newegg_page_scraper(containers, turn_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(containers[1].find_all('a', class_=\"item-brand\")[0].img[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "\n",
    "# create a function to return results pages, if exists, otherwise just scrape one page\n",
    "def results_pages():\n",
    "    # Use BeautifulSoup to extract the total results page number\n",
    "    results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "    #print(results_pages)\n",
    "    # Find and extract total pages + and add 1 to ensure proper length of total pages\n",
    "    global total_results_pages\n",
    "    total_results_pages = int(re.split(\"/\", results_pages)[1]) #+ 2 # need to add 2 b/c 'range(inclusive, exclusive)'\n",
    "    #========================================= need to remember to +2, and remove -30\n",
    "    #print(total_results_pages)\n",
    "    total_results_pages = total_results_pages ## ### ## ###\n",
    "    \n",
    "    return total_results_pages\n",
    "#results_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working\n",
    "def concatenate(total_results_pages):\n",
    "    \n",
    "    path = f'./processing\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    concatenate_pages = []\n",
    "    counter = 0\n",
    "    for page in scraped_pages:\n",
    "        df = pd.read_csv(page, index_col=0, header=0)\n",
    "        concatenate_pages.append(df)\n",
    "\n",
    "    compiled_data = pd.concat(concatenate_pages, axis=0, ignore_index=True)\n",
    "    total_items_scraped = len(compiled_data['brand']) # can replace this counter by creating class objects everytime it scrapes\n",
    "    concatenated_output = compiled_data.to_csv(f\"./finished_outputs/{current_date}_{total_items_scraped}_scraped_{total_results_pages}_pages_.csv\")\n",
    "    return concatenated_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_results_pages = 4\n",
    "# concatenate(total_results_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis is working\n",
    "def clean_processing_fldr():\n",
    "    # delete all files in the 'processing folder'\n",
    "    path = f'./processing\\\\'\n",
    "    scraped_pages = glob.glob(path + \"/*.csv\")\n",
    "    if len(scraped_pages) < 1:\n",
    "        print(\"There are no files in the folder to clear. \")\n",
    "    else:\n",
    "        print(f\"Clearing out a total of {len(scraped_pages)} scraped pages in the processing folder... \")\n",
    "        clear_processing_files = []\n",
    "        for page in scraped_pages:\n",
    "            os.remove(page)\n",
    "        \n",
    "    print('Clearing of \"Processing\" folder complete. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webscrape first page, then run page turner, then scraper for every page thereafter\n",
    "# \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning lesson is you can't call a function within a function\n",
    "\n",
    "def page_turner(total_results_pages):\n",
    "    # This is \"NEXT PAGE BUTTON CLICK\" - This loops thru the total amount of pages by clicking the next page button\n",
    "    for turn_page in range(1, total_results_pages):\n",
    "        # set the current url as the target page (aiming the boomerang)\n",
    "        target_url = browser.url\n",
    "\n",
    "        # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "        response_target = requests.get(target_url)\n",
    "        #response\n",
    "\n",
    "        # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "        target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "        # Use BeautifulSoup to extract the total results page number\n",
    "        #results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "        \n",
    "        results_pages = target_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "        #=========================================================\n",
    "        containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "        \n",
    "        newegg_page_scraper(containers, turn_page)\n",
    "        \n",
    "        #for i in range(total_results_pages):\n",
    "        x = random.randint(3, 25)\n",
    "        print(f\"{turn_page}) | SLEEPING FOR SECONDS: {x} \")\n",
    "        time.sleep(x)\n",
    "            \n",
    "        browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "\n",
    "    browser.quit()\n",
    "# concatenate(total_results_pages)\n",
    "# clean_processing_fldr()\n",
    "# # clear out processing folder function here - as delete everything to prevent clutter\n",
    "\n",
    "\n",
    "# print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{product_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "# print('Thank you and hope you found this useful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: https://www.newegg.com/Laptops-Notebooks/SubCategory/ID-32?Tid=6740\n",
      "Do you want the desktop watch the bot work? Enter a number: 1 - YES | 2 - NO . Your Answer: 1\n",
      "Head is activated. Please view only the new automated Google Chrome web browser. \n",
      "Do not make any adjustments to this automated window while the program runs, as it may produce errors or undesired outputs. \n",
      "Here are the items that you can expect to be scrapped off this page, and for every other total results pages, if there's more than one: \n",
      "===================================\n",
      "1) GIGABYTE AERO 15 Classic-SA-U74ADP Gaming Laptop - 15.6\" 4K/UHD IPS - Intel Core i7-9750H 2.60 GHz - NVIDIA GeForce GTX 1660 Ti - 16 GB Memory 512 GB SSD - Windows 10 Pro 64-bit | Price: $1,379.00\n",
      "-----------------------------------\n",
      "2) ASUS VivoBook S15 S532 Thin & Light 15.6\" FHD, Intel Core i7-8565U CPU, 8 GB DDR4 RAM, PCIe NVMe 512 GB SSD, Windows 10 Home, S532FA-SB77, Transparent Silver | Price: $749.99\n",
      "-----------------------------------\n",
      "3) Asus TUF Gaming Laptop, 15.6\" Full HD IPS-Type, Intel Core i7-9750H, GeForce GTX 1650, 8 GB DDR4, 512 GB PCIe SSD, Gigabit Wi-Fi 5, Windows 10 Home, TUF505GT-AH73 Notebook PC Computer | Price: $799.99\n",
      "-----------------------------------\n",
      "4) ASUS VivoBook S15 S532 Thin & Light 15.6\" FHD, Intel Core i7-8565U CPU, GeForce MX250, 8 GB DDR4 RAM, 256 GB PCIe NVMe SSD, IR Camera, Windows 10 Home, S532FL-EB71, Transparent Silver | Price: $799.99\n",
      "-----------------------------------\n",
      "5) Lenovo Laptop 100e 81CYX503US Intel Celeron N3450 (1.10 GHz) 4 GB LPDDR4 Memory 128 GB eMMC Intel HD Graphics 500 11.6\" Windows 10 Pro 64-bit | Price: $249.99\n",
      "-----------------------------------\n",
      "6) ASUS ZenBook S13 Ultra Thin & Light Laptop 13.9\" FHD, Intel Core i7-8565U CPU, GeForce MX150, 8 GB RAM, 512 GB PCIe SSD, Windows 10 Pro, Silver Blue, UX392FN-XS71 | Price: $999.99\n",
      "-----------------------------------\n",
      "7) ASUS VivoBook S14 S432 Thin and Light 14\" FHD, Intel Core i7-8565U CPU, 8 GB RAM, 512 GB PCIe NVMe SSD, IR Camera, Windows 10 Home, S432FA-AB74, Transparent Silver | Price: $749.99\n",
      "-----------------------------------\n",
      "8) HP Laptop ProBook 450 G6 (5VC00UT#ABA) Intel Core i5 8th Gen 8265U (1.60 GHz) 8 GB Memory 256 GB SSD Intel UHD Graphics 620 15.6\" 1920 x 1080 Windows 10 Pro 64-bit | Price: $849.00\n",
      "-----------------------------------\n",
      "9) ASUS ZenBook 14 Ultra-Slim Laptop 14\" Full HD 4-Way NanoEdge Bezel, AMD R7 3700U CPU, 8 GB RAM, 512 GB PCIe SSD, NumberPad, Windows 10 - UM433DA-DH75, Icicle Silver | Price: $799.99\n",
      "-----------------------------------\n",
      "10) ASUS ZenBook 14 Ultra-Slim Laptop 14\" Full HD 4-Way NanoEdge Bezel, AMD R7 3700U CPU, 16 GB DDR4, 1 TB PCIe SSD, Windows 10 Pro - UM433DA-NH74, Icicle Silver | Price: $899.99\n",
      "-----------------------------------\n",
      "11) Acer Aspire 3 15.6\" - AMD Ryzen 7 2700U Quad-core (4 Core) 2.20 GHz - 8 GB DDR4 SDRAM - 256 GB SSD - Windows 10 Home 64-bit - 1920 x 1080 - A315-41-R14K - NX.GY9AA.014 - Obsidian Black | Price: $529.99\n",
      "-----------------------------------\n",
      "12) DELL Laptop XPS XPS9370-7040SLV Intel Core i7 8th Gen 8550U (1.80 GHz) 16 GB Memory 1 TB PCIe SSD Intel UHD Graphics 620 13.3\" Touchscreen Windows 10 Home 64-Bit | Price: $1,889.99\n",
      "-----------------------------------\n",
      "13) ASUS ZenBook Pro Duo UX581 15.6\" 4K UHD NanoEdge Bezel Touch, Intel Core i7-9750H, 16 GB RAM, 1 TB PCIe SSD, GeForce RTX 2060, Innovative ScreenPad Plus, Windows 10 Pro - UX581GV-XB74T, Celestial Blue | Price: $2,450.99\n",
      "-----------------------------------\n",
      "14) ASUS ZenBook 14 Ultra Thin and Light Laptop, 4-Way NanoEdge 14\" FHD, Intel Core i7-10510U, 8 GB RAM, 512 GB PCIe SSD, NVIDIA GeForce MX250, Windows 10 Home, Utopia Blue, UX431FL-EH74 | Price: $882.99\n",
      "-----------------------------------\n",
      "15) ASUS ZenBook S, 13.3” UHD 4K Touch, 8th Gen Whiskey Lake Intel Core i7-8565U Processor, 16GB RAM, 512GB PCIe SSD, FP Sensor, Thunderbolt, Windows 10 Professional - UX391FA-XH74T Deep Dive Blue laptop | Price: $1,149.99\n",
      "-----------------------------------\n",
      "16) ASUS Laptop ZenBook UX434FLC-XH77 Intel Core i7 10th Gen 10510U (1.80 GHz) 16 GB LPDDR3 Memory 512 GB SSD NVIDIA GeForce MX250 14.0\" Windows 10 Pro 64-bit | Price: $1,187.99\n",
      "-----------------------------------\n",
      "17) HP Mobile Thin Client Notebook Mt44 4JA74UA#ABA AMD Ryzen 3 1st Gen 2300U (2.00 GHz) 8 GB Memory 128 GB SSD AMD Radeon Vega 6 14.0\" Windows 10 Pro 64-bit | Price: $349.99\n",
      "-----------------------------------\n",
      "18) Lenovo IdeaPad 130 Laptop, 15.6\" HD Display, AMD A9-9425 Upto 3.70GHz, 8GB RAM, 512GB SSD, DVDRW, HDMI, Card Reader, Wi-Fi, Bluetooth, Windows 10 Pro | Price: $549.00\n",
      "-----------------------------------\n",
      "19) Apple 13\" MacBook Air,Intel Core i7, 8GB RAM, 128GB SSD,Backlit Keyboard,Thunderbolt 2,SDXC card slot,Bluetooth,Webcam,Mac OS,Silver | Price: $899.99\n",
      "-----------------------------------\n",
      "20) Apple 16\" MacBook Pro (2.6 GHz Intel Core i7 6-Core | 512GB SSD) (Late 2019, Space Gray) | Price: $2,231.50\n",
      "-----------------------------------\n",
      "21) 2019 Newest Lenovo IdeaPad S145 81N3005LUS 15.6\" High Performance Laptop AMD A6-9225 Dual-Core 2.60 GHz/ 4GB/500GB HDD/Bluetooth/No DVD/W10 | Price: $319.99\n",
      "-----------------------------------\n",
      "22) HP Laptop ProBook 645 G4 (4LB42UT#ABA) AMD Ryzen 7 1st Gen 2700U (2.20 GHz) 8 GB Memory 256 GB PCIe NVMe SSD AMD Radeon RX Vega 10 14.0\" Windows 10 Pro 64-Bit | Price: $799.00\n",
      "-----------------------------------\n",
      "23) Lenovo Laptop IdeaPad 130S-11IGM 81KT0000US Intel Pentium Silver N5000 (1.10 GHz) 4 GB LPDDR4 Memory 64 GB eMMC Intel UHD Graphics 605 11.6\" Windows 10 S | Price: $299.99\n",
      "-----------------------------------\n",
      "24) ASUS VivoBook S15 15.6\" Whiskey Lake Intel Core i5-8265U Processor, 8 GB DDR4, 256 GB SSD, Windows 10 - S530FA-DB51-IG, Portable IPS Full HD NanoEdge Bezel Laptop, Backlit Keyboard | Price: $699.00\n",
      "-----------------------------------\n",
      "25) ASUS ZenBook 14, Intel Core Whiskey Lake i5-8265U, 8 GB RAM, 256 GB NVMe PCIe SSD, NumberPad, Wi-Fi 5, Windows 10, Ultra Thin and Light Laptop, 4-Way NanoEdge 14\" FHD, Silver Blue, UX431FA-ES51 | Price: $796.99\n",
      "-----------------------------------\n",
      "26) HP 15 Notebook, 15.6\" HD Touch Display, AMD Ryzen 7 3700U Upto 4.0GHz, 16GB RAM, 512GB NVMe SSD, Vega 10, HDMI, Card Reader, Wi-Fi, Bluetooth, Windows 10 Pro | Price: $829.99\n",
      "-----------------------------------\n",
      "27) Lenovo IdeaPad S150 Notebook, 14\" HD Display, AMD A6-9220e Upto 2.4GHz, 4GB RAM, 64GB eMMC, HDMI, Card Reader, Wi-Fi, Bluetooth, Windows 10 Home S (81VS0001US) | Price: $311.99\n",
      "-----------------------------------\n",
      "28) THOMSON NEO14 Laptop / Intel CPU /  2/32 GB / 14.1 inch HD 1366x768 / Webcam / Wi-Fi & Bluetooth 4.0 / US Keyboard with Multi-touch Pad /Battery up to 7 hours / Windows 10 Home / Black, Slim and Light | Price: $129.99\n",
      "-----------------------------------\n",
      "29) Dell Inspiron 14 3000 Notebook, 14\" HD Display, Intel Core i5-1035G4 Upto 3.7GHz, 8GB RAM, 512GB NVMe SSD, HDMI, Card Reader, Wi-Fi, Bluetooth, Windows 10 Pro S | Price: $649.99\n",
      "-----------------------------------\n",
      "30) Acer Laptop Aspire 5 Intel Core i7 8th Gen 8565U (1.80 GHz) 12 GB Memory 512 GB SSD NVIDIA GeForce MX250 15.6\" Windows 10 Home 64-bit | Price: $889.99\n",
      "-----------------------------------\n",
      "31) ASUS ZenBook 14, Intel Core Whiskey Lake i7-8565U, 8 GB RAM, 512 GB NVMe PCIe SSD, Wi-Fi 5, Windows 10, Silver Blue, Ultra Thin and Light Laptop, 4-Way NanoEdge 14\" FHD UX431FA-ES74 | Price: $872.99\n",
      "-----------------------------------\n",
      "32) Dell Inspiron 15 5000 (5575) Laptop, 15.6”, AMD Ryzen™ 5 2500U with Radeon™ Vega8 Graphics, 1TB HDD, 4GB RAM, i5575-A410BLU-PUS Notebook PC Computer Blue | Price: $478.80\n",
      "-----------------------------------\n",
      "33) Acer Aspire 3 A315-32-C3KK 15.6\" LCD Notebook - Intel Celeron N4100 Quad-core (4 Core) 1.10 GHz - 4 GB DDR4 SDRAM - 1 TB HDD - Windows 10 Home 64-bit - 1920 x 1080 - ComfyView - Oxidant Red | Price: $349.99\n",
      "-----------------------------------\n",
      "34) HP 15 Notebook, 15.6\" HD Touch Display, Intel Core i5-1035G4 Up to 3.70 GHz, 8 GB RAM, 128 GB SSD, HDMI, Card Reader, Wi-Fi, Bluetooth, Windows 10 Home | Price: $629.99\n",
      "-----------------------------------\n",
      "35) HP 17.3\" HD+ TouchScreen Notebook,8th Gen Intel Quad Core i7-8565U Processor,16GB DDR4 RAM,512GB SSD,Intel UHD Graphics 620,DVD-RW,Wifi,Bluetooth 4.2,USB,HDMI, Windows 10 Pro | Price: $899.00\n",
      "-----------------------------------\n",
      "36) GIGABYTE AERO 17 WA-7US1130SO - 17.3\" 144 Hz IPS - Intel Core i7 9th Gen 9750H (2.60 GHz) - NVIDIA GeForce RTX 2060 - 16 GB Memory 512 GB SSD - Windows 10 Home 64-bit - Gaming Laptop | Price: $1,899.00\n",
      "-----------------------------------\n",
      "============================================================\n",
      "36 Scrappable Objects on the page. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Break Pedal: Answer any robot queries by NewEgg. Enter \"y\" when you are ready to proceed. y\n",
      "Proceeding with webscrape... \n",
      "1) | SLEEPING FOR SECONDS: 10 \n",
      "2) | SLEEPING FOR SECONDS: 17 \n",
      "3) | SLEEPING FOR SECONDS: 17 \n",
      "4) | SLEEPING FOR SECONDS: 5 \n",
      "All 5 pages have been saved in the \"processing\" folder (1 page = csv files). Would you like for us concatenate all the files into one? Enter \"y\", if so. Otherwise, enter anykey to exit the program. y\n",
      "WebScraping Complete! All 5 have been scraped and saved as 2020-04-12_19.25.11_LaptopsNotebooks_scraped_5_pages_.csv in the \"finished_outputs\" folder\n",
      "The \"processing\" folder has 5 csv files of each page that was scraped. Would you like to clear the files? Enter \"y\", if so. Otherwise, enter anykey to exit the program. n\n",
      "Thank you and hope you found this useful!\n"
     ]
    }
   ],
   "source": [
    "# scrape_again = 'y'\n",
    "# while scrape_again =='y':\n",
    "#     print(\"=== NewEgg.Com WebScraper Beta ===\")\n",
    "#     print(\"==\"*30)\n",
    "#     print(\"Instructions:\")\n",
    "#     print(\"(1) Go to www.newegg.com, search for the product and filter for your requirements (e.g. brand and specifications) \")\n",
    "#     print(\"(2) Copy and paste the url from your exact search \")\n",
    "#     print('(3) Activate or Disable the \"Head View\", webscraper bots point of view ')\n",
    "#     print('(4) Check the \"final_output folder when the webscraper bot is done scraping \"')\n",
    "#     print(\"\")\n",
    "# # NEED TO CREATE LAST INPUT TO LOOP THIS WHOLE THING AGAIN.\n",
    "# Maybe use pandas to provide summary of data\n",
    "# Export images of box and whisker plots using statistics by brand\n",
    "\n",
    "return_dt()\n",
    "\n",
    "executable_path = {'executable_path': './chromedriver.exe'}\n",
    "\n",
    "url = input(\"Step 1) Please copy and paste your laptop query that you want to webscrape, and press enter: \")\n",
    "head = ''\n",
    "browser =''\n",
    "head_on_off(executable_path)\n",
    "response = requests.get(current_url)\n",
    "#response\n",
    "\n",
    "current_page_soup = soup(response.text, 'html.parser')\n",
    "current_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "scrappable_y_n(current_page_soup)\n",
    "\n",
    "# Are there any pop ups / safe to proceed?\n",
    "safe_proceed_y_n = input(f'The Break Pedal: Answer any robot queries by NewEgg. Enter \"y\" when you are ready to proceed. ')\n",
    "if safe_proceed_y_n == 'y':\n",
    "    print(f'Proceeding with webscrape... ')\n",
    "else:\n",
    "    print(\"Quitting browser. You will need to press ctrl + c to quit, and then restart the program to try again. \")\n",
    "    browser.quit()\n",
    "#newegg_page_scraper(containers)\n",
    "\n",
    "# will need to UNCOMMENT AFTER\n",
    "#results_pages()\n",
    "\n",
    "#page_turner(total_results_pages)\n",
    "\n",
    "total_results_pages = 5\n",
    "for turn_page in range(1, total_results_pages):\n",
    "    # set the current url as the target page (aiming the boomerang)\n",
    "    target_url = browser.url\n",
    "\n",
    "    # Use Request.get() - throw the boomerang at the target, retrieve the info, & return back to requestor\n",
    "    response_target = requests.get(target_url)\n",
    "    #response\n",
    "\n",
    "    # Use BeautifulSoup to read grab all the HTML using the lxml parser\n",
    "    target_page_soup = soup(response_target.text, 'html.parser')\n",
    "\n",
    "    # Use BeautifulSoup to extract the total results page number\n",
    "    #results_pages = current_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "\n",
    "    results_pages = target_page_soup.find_all('span', class_=\"list-tool-pagination-text\")[0].text.strip()\n",
    "    #=========================================================\n",
    "    containers = target_page_soup.find_all(\"div\", class_=\"item-container\")\n",
    "\n",
    "    newegg_page_scraper(containers, turn_page)\n",
    "\n",
    "    #for i in range(total_results_pages):\n",
    "    x = random.randint(3, 25)\n",
    "    print(f\"{turn_page}) | SLEEPING FOR {x} SECONDS \")\n",
    "    time.sleep(x)\n",
    "\n",
    "    browser.find_by_xpath('//*[@id=\"bodyArea\"]/section/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div[1]/div[2]/div/div[2]/button').click()\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "##################################################################\n",
    "concat_y_n = input(f'All {total_results_pages} pages have been saved in the \"processing\" folder (1 page = csv files). Would you like for us concatenate all the files into one? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "if concat_y_n == 'y':\n",
    "    concatenate(total_results_pages)\n",
    "    print(f'WebScraping Complete! All {total_results_pages} have been scraped and saved as {current_date}_{product_category}_scraped_{total_results_pages}_pages_.csv in the \"finished_outputs\" folder')\n",
    "\n",
    "# clear out processing folder function here - as delete everything to prevent clutter\n",
    "clear_processing_y_n = input(f'The \"processing\" folder has {total_results_pages} csv files of each page that was scraped. Would you like to clear the files? Enter \"y\", if so. Otherwise, enter anykey to exit the program. ')\n",
    "if clear_processing_y_n == 'y':\n",
    "    clean_processing_fldr()\n",
    "\n",
    "    \n",
    "print('Thank you and hope you found this useful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
